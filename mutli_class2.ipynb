{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd213681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3826783299446106, Accuracy: 88.6814193725586, Test Loss: 0.23311454057693481, Test Accuracy: 92.4338150024414\n",
      "Epoch 2, Loss: 0.2317880243062973, Accuracy: 92.44519805908203, Test Loss: 0.20232480764389038, Test Accuracy: 93.585693359375\n",
      "Epoch 3, Loss: 0.19117125868797302, Accuracy: 94.28759002685547, Test Loss: 0.19409720599651337, Test Accuracy: 93.17318725585938\n",
      "Epoch 4, Loss: 0.16969864070415497, Accuracy: 94.92601013183594, Test Loss: 0.1666862666606903, Test Accuracy: 95.06137084960938\n",
      "Epoch 5, Loss: 0.15610621869564056, Accuracy: 95.42857360839844, Test Loss: 0.14713887870311737, Test Accuracy: 95.7340087890625\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "import os\n",
    "\n",
    "#traindata = pd.read_csv('preprocessed_train_multiclass(a)_new.csv', header=None)\n",
    "#testdata = pd.read_csv('preprocessed_test_multiclass(a)_new.csv', header=None)\n",
    "X=pd.read_csv (r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli121.csv\", header=None )\n",
    "T=pd.read_csv (r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli11.csv\", header=None )\n",
    "C=pd.read_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli1_label.csv\",header=None)\n",
    "Y=pd.read_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli2_label.csv\",header=None)\n",
    "'''X = pd.read_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain_1.csv\", header=None)\n",
    "Y = pd.read_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\train_label.csv\", header=None)\n",
    "\n",
    "T = pd.read_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\pva1\\pva_1.csv\", header=None)\n",
    "C = pd.read_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\pva1\\pva_label.csv\", header=None)'''\n",
    "\n",
    "X1 = X.iloc[0:,0:76]# 0 to 73 columns of data frame with all rows\n",
    "T1 = T.iloc[0:,0:76]# \n",
    "\n",
    "scaler = Normalizer().fit(X1)\n",
    "trainX = scaler.transform(X1)\n",
    "\n",
    "\n",
    "scaler = Normalizer().fit(T1)\n",
    "testT = scaler.transform(T1)\n",
    "\n",
    "y_train1 = np.array(Y)\n",
    "y_test1 = np.array(C)\n",
    "\n",
    "y_train = to_categorical(y_train1)\n",
    "y_test = to_categorical(y_test1)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "X_train = trainX\n",
    "#X_train = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "#X_test = np.reshape(testT, (testT.shape[0], 1, testT.shape[1]))\n",
    "X_test = testT\n",
    "\n",
    "# Add a channels dimension\n",
    "X_train = X_train[..., tf.newaxis].astype(\"float64\")\n",
    "X_test = X_test[..., tf.newaxis].astype(\"float64\")\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train1)).shuffle(1000000).batch(50)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test1)).batch(50)\n",
    "\n",
    "\n",
    "class MyModel(Model):\n",
    "    #\n",
    "    def __init__(self):\n",
    "        #\n",
    "        super(MyModel, self).__init__()\n",
    "        self.lstm1=LSTM(50,input_shape=(1, 76), return_sequences= True )\n",
    "        self.drop1=Dropout(0.01)\n",
    "        self.lstm2=LSTM(50,return_sequences=True)\n",
    "        self.drop2=Dropout(0.01)\n",
    "        self.lstm3=LSTM(50, return_sequences=False)\n",
    "        self.drop3=Dropout(0.01)\n",
    "        self.d1=Dense(14)\n",
    "        self.activation1=Activation('softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.lstm1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.lstm3(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.d1(x)\n",
    "        return self.activation1(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "'''\n",
    "model.add(LSTM(50,input_shape=(1, 76), return_sequences= True ))  \n",
    "model.add(Dropout(0.01))\n",
    "model.add(LSTM(50,return_sequences=True)) \n",
    "model.add(Dropout(0.01))\n",
    "#model.add(LSTM(40,return_sequences=True))   \n",
    "#model.add(Dropout(0.01))\n",
    "#model.add(LSTM(40, return_sequences=True))  \n",
    "#model.add(Dropout(0.01))\n",
    "#model.add(LSTM(40, return_sequences=True)) \n",
    "#model.add(Dropout(0.01))\n",
    "#model.add(LSTM(80, return_sequences=True)) \n",
    "#model.add(Dropout(0.01))\n",
    "model.add(LSTM(50, return_sequences=False)) \n",
    "model.add(Dropout(0.01))\n",
    "model.add(Dense(14))# the no. of output classes\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(traind, labels):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(traind, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    \n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    \n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8001b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ana\\envs\\tensorflowb\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222654, 80)\n",
      "(1174778, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import OneClassSVM\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df=pd.read_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1.csv\")\n",
    "df2=df.drop_duplicates( keep=False)\n",
    "print ( df.shape)\n",
    "print ( df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc206f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ana\\envs\\tensorflowb\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222654, 80)\n",
      "(1174778, 77)\n"
     ]
    }
   ],
   "source": [
    "df2.drop('Protocol',axis=1,inplace=True)\n",
    "df2.drop('Dst Port',axis=1,inplace=True)\n",
    "df2.drop('Timestamp',axis=1,inplace=True)\n",
    "print ( df.shape)\n",
    "print ( df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03b1c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1174778, 77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44742\\AppData\\Local\\Temp/ipykernel_9564/4146149062.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2[index]=df2[index].fillna(0)\n",
      "C:\\Users\\44742\\AppData\\Local\\Temp/ipykernel_9564/4146149062.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2[index]=df2[index].replace(np.Infinity,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1174778, 77)\n"
     ]
    }
   ],
   "source": [
    "print ( df2.shape )\n",
    "\n",
    "for index in df2.columns:\n",
    "    df2[index]=df2[index].fillna(0)\n",
    "    df2[index]=df2[index].replace(np.Infinity,1)\n",
    "\n",
    "#X.drop ( columus=[77],axis=1,inplace=True)\n",
    "#X.to_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\pva1\\pva_1.csv\", header=None,index=0)\n",
    "print (df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c4e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2.csv\", header=None,index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e9ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt1=df2.groupby('Label').apply(pd.DataFrame.sample, frac=0.5).reset_index(level='Label', drop=True)\n",
    "pt2=df2.drop(pt1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c40df1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign                      450202\n",
      "DDOS attack-HOIC             50045\n",
      "DoS attacks-Hulk             33242\n",
      "Bot                          20944\n",
      "Infilteration                12498\n",
      "SSH-Bruteforce               10042\n",
      "FTP-BruteForce                4242\n",
      "DoS attacks-GoldenEye         3050\n",
      "DoS attacks-SlowHTTPTest      2156\n",
      "DoS attacks-Slowloris          774\n",
      "DDOS attack-LOIC-UDP           127\n",
      "Brute Force -Web                45\n",
      "Brute Force -XSS                17\n",
      "SQL Injection                    6\n",
      "Name: Label, dtype: int64\n",
      "['Benign' 'Benign' 'Benign' ... 'SSH-Bruteforce' 'SSH-Bruteforce'\n",
      " 'SSH-Bruteforce']\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "pt1.to_csv (r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli1.csv\", header=None,index=0 )\n",
    "pt2.to_csv (r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli2.csv\", header=None,index=0 )\n",
    "labelt=pt1.Label.copy()\n",
    "print ( labelt.value_counts())\n",
    "arrs=labelt.values\n",
    "d1=np.array(arrs)\n",
    "values = array(d1)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "data3 = pd.DataFrame(integer_encoded) # header:原第一行的索引，index:原第一列的索引\n",
    "data3.to_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli1_labeln.csv\",header=None,index=0)\n",
    "# # binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "data4 = pd.DataFrame(onehot_encoded) # header:原第一行的索引，index:原第一列的索引\n",
    "data4.to_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli1_label.csv\",header=None,index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2d8361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign                      450203\n",
      "DDOS attack-HOIC             50045\n",
      "DoS attacks-Hulk             33242\n",
      "Bot                          20943\n",
      "Infilteration                12498\n",
      "SSH-Bruteforce               10043\n",
      "FTP-BruteForce                4241\n",
      "DoS attacks-GoldenEye         3049\n",
      "DoS attacks-SlowHTTPTest      2155\n",
      "DoS attacks-Slowloris          773\n",
      "DDOS attack-LOIC-UDP           127\n",
      "Brute Force -Web                45\n",
      "Brute Force -XSS                17\n",
      "SQL Injection                    7\n",
      "Name: Label, dtype: int64\n",
      "['Benign' 'Benign' 'Benign' ... 'Infilteration' 'Infilteration'\n",
      " 'Infilteration']\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "labelt=pt2.Label.copy()\n",
    "print ( labelt.value_counts())\n",
    "arrs=labelt.values\n",
    "d1=np.array(arrs)\n",
    "values = array(d1)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "data3 = pd.DataFrame(integer_encoded) # header:原第一行的索引，index:原第一列的索引\n",
    "data3.to_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli2_labeln.csv\",header=None,index=0)\n",
    "# # binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "data4 = pd.DataFrame(onehot_encoded) # header:原第一行的索引，index:原第一列的索引\n",
    "data4.to_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli2_label.csv\",header=None,index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13cc54a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Flow Pkts/s'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\ana\\envs\\tensorflowb\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ana\\envs\\tensorflowb\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Flow Pkts/s'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1872/117709699.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Flow Pkts/s'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Flow Pkts/s'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# removing NAN and INF values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Flow Byts/s'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Flow Byts/s'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Flow Pkts/s'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Flow Pkts/s'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInfinity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ana\\envs\\tensorflowb\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ana\\envs\\tensorflowb\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Flow Pkts/s'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "df2 = pd.read_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli1.csv\", header=None)\n",
    "\n",
    "\n",
    "for index in df2.columns:\n",
    "    df2[index]=df2[index].fillna(0)\n",
    "    df2[index]=df2[index].replace(np.Infinity,1)\n",
    "    \n",
    "df2.to_csv (r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli11.csv\", header=None,index=0 )\n",
    "print ( df2.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4719cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587388, 77)\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli2.csv\", header=None)\n",
    "for index in df2.columns:\n",
    "    df2[index]=df2[index].fillna(0)\n",
    "    df2[index]=df2[index].replace(np.Infinity,1)\n",
    "    \n",
    "df2.to_csv (r\"E:\\学习心得\\data\\cic-ids2018\\02-03\\ptrain1\\ptrain2_sli121.csv\", header=None,index=0 )\n",
    "print ( df2.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a56f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cf82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
