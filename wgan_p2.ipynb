{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad8923e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_212 (Conv2D)         (None, 16, 17, 16)        160       \n",
      "                                                                 \n",
      " leaky_re_lu_108 (LeakyReLU)  (None, 16, 17, 16)       0         \n",
      "                                                                 \n",
      " dropout_108 (Dropout)       (None, 16, 17, 16)        0         \n",
      "                                                                 \n",
      " conv2d_213 (Conv2D)         (None, 8, 9, 32)          4640      \n",
      "                                                                 \n",
      " zero_padding2d_27 (ZeroPadd  (None, 9, 10, 32)        0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " batch_normalization_133 (Ba  (None, 9, 10, 32)        128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_109 (LeakyReLU)  (None, 9, 10, 32)        0         \n",
      "                                                                 \n",
      " dropout_109 (Dropout)       (None, 9, 10, 32)         0         \n",
      "                                                                 \n",
      " conv2d_214 (Conv2D)         (None, 5, 5, 64)          18496     \n",
      "                                                                 \n",
      " batch_normalization_134 (Ba  (None, 5, 5, 64)         256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_110 (LeakyReLU)  (None, 5, 5, 64)         0         \n",
      "                                                                 \n",
      " dropout_110 (Dropout)       (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " conv2d_215 (Conv2D)         (None, 5, 5, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_135 (Ba  (None, 5, 5, 128)        512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_111 (LeakyReLU)  (None, 5, 5, 128)        0         \n",
      "                                                                 \n",
      " dropout_111 (Dropout)       (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " flatten_27 (Flatten)        (None, 3200)              0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1)                 3201      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,249\n",
      "Trainable params: 100,801\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.999894] [G loss: 1.000068]\n",
      "(14461, 768)\n",
      "1 [D loss: 0.999908] [G loss: 1.000182]\n",
      "2 [D loss: 0.999905] [G loss: 1.000221]\n",
      "3 [D loss: 0.999907] [G loss: 1.000217]\n",
      "4 [D loss: 0.999907] [G loss: 1.000212]\n",
      "5 [D loss: 0.999910] [G loss: 1.000200]\n",
      "6 [D loss: 0.999907] [G loss: 1.000192]\n",
      "7 [D loss: 0.999913] [G loss: 1.000200]\n",
      "8 [D loss: 0.999914] [G loss: 1.000196]\n",
      "9 [D loss: 0.999920] [G loss: 1.000181]\n",
      "10 [D loss: 0.999915] [G loss: 1.000188]\n",
      "11 [D loss: 0.999911] [G loss: 1.000172]\n",
      "12 [D loss: 0.999910] [G loss: 1.000168]\n",
      "13 [D loss: 0.999922] [G loss: 1.000168]\n",
      "14 [D loss: 0.999918] [G loss: 1.000159]\n",
      "15 [D loss: 0.999927] [G loss: 1.000146]\n",
      "16 [D loss: 0.999926] [G loss: 1.000130]\n",
      "17 [D loss: 0.999930] [G loss: 1.000141]\n",
      "18 [D loss: 0.999934] [G loss: 1.000131]\n",
      "19 [D loss: 0.999933] [G loss: 1.000126]\n",
      "20 [D loss: 0.999942] [G loss: 1.000121]\n",
      "21 [D loss: 0.999940] [G loss: 1.000102]\n",
      "22 [D loss: 0.999943] [G loss: 1.000104]\n",
      "23 [D loss: 0.999949] [G loss: 1.000103]\n",
      "24 [D loss: 0.999952] [G loss: 1.000094]\n",
      "25 [D loss: 0.999950] [G loss: 1.000092]\n",
      "26 [D loss: 0.999957] [G loss: 1.000079]\n",
      "27 [D loss: 0.999955] [G loss: 1.000081]\n",
      "28 [D loss: 0.999958] [G loss: 1.000081]\n",
      "29 [D loss: 0.999962] [G loss: 1.000078]\n",
      "30 [D loss: 0.999960] [G loss: 1.000073]\n",
      "31 [D loss: 0.999962] [G loss: 1.000067]\n",
      "32 [D loss: 0.999961] [G loss: 1.000069]\n",
      "33 [D loss: 0.999967] [G loss: 1.000063]\n",
      "34 [D loss: 0.999967] [G loss: 1.000068]\n",
      "35 [D loss: 0.999972] [G loss: 1.000064]\n",
      "36 [D loss: 0.999969] [G loss: 1.000062]\n",
      "37 [D loss: 0.999970] [G loss: 1.000064]\n",
      "38 [D loss: 0.999970] [G loss: 1.000062]\n",
      "39 [D loss: 0.999979] [G loss: 1.000060]\n",
      "40 [D loss: 0.999976] [G loss: 1.000053]\n",
      "41 [D loss: 0.999984] [G loss: 1.000049]\n",
      "42 [D loss: 0.999981] [G loss: 1.000051]\n",
      "43 [D loss: 0.999984] [G loss: 1.000052]\n",
      "44 [D loss: 0.999990] [G loss: 1.000037]\n",
      "45 [D loss: 0.999993] [G loss: 1.000051]\n",
      "46 [D loss: 1.000003] [G loss: 1.000042]\n",
      "47 [D loss: 1.000001] [G loss: 1.000048]\n",
      "48 [D loss: 1.000007] [G loss: 1.000040]\n",
      "49 [D loss: 1.000024] [G loss: 1.000043]\n",
      "50 [D loss: 1.000039] [G loss: 1.000033]\n",
      "51 [D loss: 1.000039] [G loss: 1.000034]\n",
      "52 [D loss: 1.000043] [G loss: 1.000032]\n",
      "53 [D loss: 1.000061] [G loss: 1.000022]\n",
      "54 [D loss: 1.000078] [G loss: 1.000036]\n",
      "55 [D loss: 1.000093] [G loss: 1.000020]\n",
      "56 [D loss: 1.000103] [G loss: 1.000024]\n",
      "57 [D loss: 1.000119] [G loss: 1.000013]\n",
      "58 [D loss: 1.000127] [G loss: 1.000020]\n",
      "59 [D loss: 1.000154] [G loss: 0.999992]\n",
      "60 [D loss: 1.000178] [G loss: 1.000002]\n",
      "61 [D loss: 1.000200] [G loss: 1.000008]\n",
      "62 [D loss: 1.000199] [G loss: 0.999995]\n",
      "63 [D loss: 1.000234] [G loss: 1.000022]\n",
      "64 [D loss: 1.000238] [G loss: 1.000007]\n",
      "65 [D loss: 1.000285] [G loss: 0.999986]\n",
      "66 [D loss: 1.000312] [G loss: 0.999966]\n",
      "67 [D loss: 1.000347] [G loss: 0.999996]\n",
      "68 [D loss: 1.000371] [G loss: 0.999974]\n",
      "69 [D loss: 1.000398] [G loss: 0.999964]\n",
      "70 [D loss: 1.000406] [G loss: 0.999975]\n",
      "71 [D loss: 1.000441] [G loss: 0.999948]\n",
      "72 [D loss: 1.000472] [G loss: 0.999940]\n",
      "73 [D loss: 1.000522] [G loss: 0.999969]\n",
      "74 [D loss: 1.000564] [G loss: 0.999941]\n",
      "75 [D loss: 1.000578] [G loss: 0.999911]\n",
      "76 [D loss: 1.000607] [G loss: 0.999931]\n",
      "77 [D loss: 1.000697] [G loss: 0.999912]\n",
      "78 [D loss: 1.000711] [G loss: 0.999956]\n",
      "79 [D loss: 1.000706] [G loss: 0.999911]\n",
      "80 [D loss: 1.000765] [G loss: 0.999939]\n",
      "81 [D loss: 1.000850] [G loss: 0.999902]\n",
      "82 [D loss: 1.000789] [G loss: 0.999949]\n",
      "83 [D loss: 1.000887] [G loss: 0.999944]\n",
      "84 [D loss: 1.000899] [G loss: 0.999961]\n",
      "85 [D loss: 1.000972] [G loss: 0.999959]\n",
      "86 [D loss: 1.000946] [G loss: 0.999940]\n",
      "87 [D loss: 1.001013] [G loss: 0.999938]\n",
      "88 [D loss: 1.001077] [G loss: 0.999882]\n",
      "89 [D loss: 1.001148] [G loss: 0.999910]\n",
      "90 [D loss: 1.001099] [G loss: 0.999943]\n",
      "91 [D loss: 1.001155] [G loss: 0.999846]\n",
      "92 [D loss: 1.001193] [G loss: 0.999953]\n",
      "93 [D loss: 1.001262] [G loss: 1.000023]\n",
      "94 [D loss: 1.001361] [G loss: 0.999978]\n",
      "95 [D loss: 1.001324] [G loss: 0.999877]\n",
      "96 [D loss: 1.001404] [G loss: 0.999845]\n",
      "97 [D loss: 1.001369] [G loss: 0.999956]\n",
      "98 [D loss: 1.001525] [G loss: 0.999990]\n",
      "99 [D loss: 1.001465] [G loss: 0.999911]\n",
      "100 [D loss: 1.001489] [G loss: 1.000006]\n",
      "101 [D loss: 1.001487] [G loss: 1.000035]\n",
      "102 [D loss: 1.001479] [G loss: 0.999981]\n",
      "103 [D loss: 1.001522] [G loss: 1.000001]\n",
      "104 [D loss: 1.001568] [G loss: 1.000021]\n",
      "105 [D loss: 1.001654] [G loss: 1.000069]\n",
      "106 [D loss: 1.001639] [G loss: 1.000044]\n",
      "107 [D loss: 1.001786] [G loss: 1.000135]\n",
      "108 [D loss: 1.001652] [G loss: 0.999974]\n",
      "109 [D loss: 1.001813] [G loss: 1.000012]\n",
      "110 [D loss: 1.001753] [G loss: 1.000092]\n",
      "111 [D loss: 1.001708] [G loss: 0.999987]\n",
      "112 [D loss: 1.001737] [G loss: 1.000151]\n",
      "113 [D loss: 1.001865] [G loss: 1.000048]\n",
      "114 [D loss: 1.001667] [G loss: 1.000055]\n",
      "115 [D loss: 1.001797] [G loss: 1.000005]\n",
      "116 [D loss: 1.001831] [G loss: 1.000060]\n",
      "117 [D loss: 1.001851] [G loss: 1.000145]\n",
      "118 [D loss: 1.001809] [G loss: 0.999923]\n",
      "119 [D loss: 1.001882] [G loss: 1.000185]\n",
      "120 [D loss: 1.001971] [G loss: 1.000103]\n",
      "121 [D loss: 1.001957] [G loss: 1.000078]\n",
      "122 [D loss: 1.001958] [G loss: 1.000099]\n",
      "123 [D loss: 1.002056] [G loss: 1.000039]\n",
      "124 [D loss: 1.001922] [G loss: 1.000111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 [D loss: 1.002012] [G loss: 1.000232]\n",
      "126 [D loss: 1.002054] [G loss: 1.000074]\n",
      "127 [D loss: 1.001895] [G loss: 1.000103]\n",
      "128 [D loss: 1.002025] [G loss: 1.000057]\n",
      "129 [D loss: 1.002099] [G loss: 1.000113]\n",
      "130 [D loss: 1.002015] [G loss: 1.000211]\n",
      "131 [D loss: 1.001900] [G loss: 1.000187]\n",
      "132 [D loss: 1.002147] [G loss: 1.000050]\n",
      "133 [D loss: 1.001989] [G loss: 1.000131]\n",
      "134 [D loss: 1.002161] [G loss: 1.000143]\n",
      "135 [D loss: 1.002154] [G loss: 1.000022]\n",
      "136 [D loss: 1.002042] [G loss: 1.000124]\n",
      "137 [D loss: 1.002069] [G loss: 1.000180]\n",
      "138 [D loss: 1.002095] [G loss: 1.000198]\n",
      "139 [D loss: 1.002072] [G loss: 1.000145]\n",
      "140 [D loss: 1.002171] [G loss: 1.000060]\n",
      "141 [D loss: 1.002268] [G loss: 1.000090]\n",
      "142 [D loss: 1.002081] [G loss: 1.000189]\n",
      "143 [D loss: 1.002219] [G loss: 1.000109]\n",
      "144 [D loss: 1.002178] [G loss: 1.000191]\n",
      "145 [D loss: 1.002272] [G loss: 1.000227]\n",
      "146 [D loss: 1.002253] [G loss: 1.000136]\n",
      "147 [D loss: 1.002292] [G loss: 1.000144]\n",
      "148 [D loss: 1.002376] [G loss: 1.000192]\n",
      "149 [D loss: 1.002285] [G loss: 1.000068]\n",
      "150 [D loss: 1.002435] [G loss: 1.000154]\n",
      "151 [D loss: 1.002238] [G loss: 1.000113]\n",
      "152 [D loss: 1.002417] [G loss: 0.999975]\n",
      "153 [D loss: 1.002370] [G loss: 1.000117]\n",
      "154 [D loss: 1.002434] [G loss: 0.999909]\n",
      "155 [D loss: 1.002339] [G loss: 1.000032]\n",
      "156 [D loss: 1.002323] [G loss: 1.000156]\n",
      "157 [D loss: 1.002330] [G loss: 1.000064]\n",
      "158 [D loss: 1.002274] [G loss: 0.999941]\n",
      "159 [D loss: 1.002311] [G loss: 1.000056]\n",
      "160 [D loss: 1.002351] [G loss: 0.999978]\n",
      "161 [D loss: 1.002361] [G loss: 1.000050]\n",
      "162 [D loss: 1.002334] [G loss: 1.000080]\n",
      "163 [D loss: 1.002457] [G loss: 0.999979]\n",
      "164 [D loss: 1.002277] [G loss: 1.000003]\n",
      "165 [D loss: 1.002392] [G loss: 0.999950]\n",
      "166 [D loss: 1.002377] [G loss: 1.000261]\n",
      "167 [D loss: 1.002359] [G loss: 1.000001]\n",
      "168 [D loss: 1.002530] [G loss: 1.000130]\n",
      "169 [D loss: 1.002473] [G loss: 1.000042]\n",
      "170 [D loss: 1.002525] [G loss: 0.999902]\n",
      "171 [D loss: 1.002435] [G loss: 1.000042]\n",
      "172 [D loss: 1.002498] [G loss: 0.999948]\n",
      "173 [D loss: 1.002445] [G loss: 1.000042]\n",
      "174 [D loss: 1.002642] [G loss: 1.000075]\n",
      "175 [D loss: 1.002555] [G loss: 1.000132]\n",
      "176 [D loss: 1.002528] [G loss: 1.000004]\n",
      "177 [D loss: 1.002635] [G loss: 1.000049]\n",
      "178 [D loss: 1.002519] [G loss: 1.000206]\n",
      "179 [D loss: 1.002589] [G loss: 1.000061]\n",
      "180 [D loss: 1.002569] [G loss: 1.000118]\n",
      "181 [D loss: 1.002559] [G loss: 1.000077]\n",
      "182 [D loss: 1.002495] [G loss: 1.000071]\n",
      "183 [D loss: 1.002360] [G loss: 1.000273]\n",
      "184 [D loss: 1.002584] [G loss: 1.000155]\n",
      "185 [D loss: 1.002394] [G loss: 1.000010]\n",
      "186 [D loss: 1.002532] [G loss: 1.000086]\n",
      "187 [D loss: 1.002614] [G loss: 1.000110]\n",
      "188 [D loss: 1.002443] [G loss: 0.999947]\n",
      "189 [D loss: 1.002605] [G loss: 1.000095]\n",
      "190 [D loss: 1.002513] [G loss: 1.000059]\n",
      "191 [D loss: 1.002499] [G loss: 1.000330]\n",
      "192 [D loss: 1.002561] [G loss: 1.000040]\n",
      "193 [D loss: 1.002680] [G loss: 1.000125]\n",
      "194 [D loss: 1.002727] [G loss: 1.000156]\n",
      "195 [D loss: 1.002544] [G loss: 1.000144]\n",
      "196 [D loss: 1.002585] [G loss: 1.000122]\n",
      "197 [D loss: 1.002734] [G loss: 1.000141]\n",
      "198 [D loss: 1.002464] [G loss: 1.000140]\n",
      "199 [D loss: 1.002602] [G loss: 1.000162]\n",
      "200 [D loss: 1.002641] [G loss: 1.000273]\n",
      "201 [D loss: 1.002633] [G loss: 1.000176]\n",
      "202 [D loss: 1.002655] [G loss: 1.000010]\n",
      "203 [D loss: 1.002522] [G loss: 1.000219]\n",
      "204 [D loss: 1.002539] [G loss: 1.000056]\n",
      "205 [D loss: 1.002581] [G loss: 1.000050]\n",
      "206 [D loss: 1.002665] [G loss: 1.000095]\n",
      "207 [D loss: 1.002738] [G loss: 1.000113]\n",
      "208 [D loss: 1.002733] [G loss: 1.000158]\n",
      "209 [D loss: 1.002762] [G loss: 1.000261]\n",
      "210 [D loss: 1.002805] [G loss: 1.000150]\n",
      "211 [D loss: 1.002766] [G loss: 0.999955]\n",
      "212 [D loss: 1.002760] [G loss: 1.000149]\n",
      "213 [D loss: 1.002667] [G loss: 1.000090]\n",
      "214 [D loss: 1.002685] [G loss: 1.000221]\n",
      "215 [D loss: 1.002680] [G loss: 0.999954]\n",
      "216 [D loss: 1.002759] [G loss: 1.000008]\n",
      "217 [D loss: 1.002718] [G loss: 1.000034]\n",
      "218 [D loss: 1.002761] [G loss: 0.999999]\n",
      "219 [D loss: 1.002725] [G loss: 1.000181]\n",
      "220 [D loss: 1.002684] [G loss: 0.999998]\n",
      "221 [D loss: 1.002623] [G loss: 0.999892]\n",
      "222 [D loss: 1.002807] [G loss: 0.999962]\n",
      "223 [D loss: 1.002644] [G loss: 1.000114]\n",
      "224 [D loss: 1.002827] [G loss: 0.999988]\n",
      "225 [D loss: 1.002702] [G loss: 0.999972]\n",
      "226 [D loss: 1.002956] [G loss: 1.000076]\n",
      "227 [D loss: 1.002682] [G loss: 1.000181]\n",
      "228 [D loss: 1.002682] [G loss: 0.999951]\n",
      "229 [D loss: 1.002932] [G loss: 1.000087]\n",
      "230 [D loss: 1.002818] [G loss: 0.999998]\n",
      "231 [D loss: 1.002997] [G loss: 0.999957]\n",
      "232 [D loss: 1.002837] [G loss: 0.999868]\n",
      "233 [D loss: 1.002840] [G loss: 0.999986]\n",
      "234 [D loss: 1.002879] [G loss: 1.000038]\n",
      "235 [D loss: 1.002795] [G loss: 0.999944]\n",
      "236 [D loss: 1.002699] [G loss: 0.999923]\n",
      "237 [D loss: 1.002884] [G loss: 1.000133]\n",
      "238 [D loss: 1.002710] [G loss: 0.999838]\n",
      "239 [D loss: 1.002812] [G loss: 1.000030]\n",
      "240 [D loss: 1.002752] [G loss: 1.000026]\n",
      "241 [D loss: 1.002767] [G loss: 0.999966]\n",
      "242 [D loss: 1.002843] [G loss: 1.000017]\n",
      "243 [D loss: 1.002881] [G loss: 1.000174]\n",
      "244 [D loss: 1.002891] [G loss: 1.000136]\n",
      "245 [D loss: 1.002757] [G loss: 1.000038]\n",
      "246 [D loss: 1.002770] [G loss: 0.999941]\n",
      "247 [D loss: 1.002869] [G loss: 1.000103]\n",
      "248 [D loss: 1.002782] [G loss: 1.000272]\n",
      "249 [D loss: 1.002897] [G loss: 1.000321]\n",
      "250 [D loss: 1.002614] [G loss: 1.000226]\n",
      "251 [D loss: 1.002796] [G loss: 1.000402]\n",
      "252 [D loss: 1.002742] [G loss: 1.000457]\n",
      "253 [D loss: 1.002764] [G loss: 1.000315]\n",
      "254 [D loss: 1.002691] [G loss: 1.000255]\n",
      "255 [D loss: 1.002531] [G loss: 1.000296]\n",
      "256 [D loss: 1.002701] [G loss: 1.000346]\n",
      "257 [D loss: 1.002645] [G loss: 1.000414]\n",
      "258 [D loss: 1.002402] [G loss: 1.000317]\n",
      "259 [D loss: 1.002560] [G loss: 1.000506]\n",
      "260 [D loss: 1.002588] [G loss: 1.000515]\n",
      "261 [D loss: 1.002590] [G loss: 1.000449]\n",
      "262 [D loss: 1.002491] [G loss: 1.000554]\n",
      "263 [D loss: 1.002601] [G loss: 1.000463]\n",
      "264 [D loss: 1.002811] [G loss: 1.000529]\n",
      "265 [D loss: 1.002613] [G loss: 1.000479]\n",
      "266 [D loss: 1.002714] [G loss: 1.000359]\n",
      "267 [D loss: 1.002711] [G loss: 1.000304]\n",
      "268 [D loss: 1.002665] [G loss: 1.000396]\n",
      "269 [D loss: 1.002656] [G loss: 1.000555]\n",
      "270 [D loss: 1.002733] [G loss: 1.000489]\n",
      "271 [D loss: 1.002616] [G loss: 1.000606]\n",
      "272 [D loss: 1.002602] [G loss: 1.000606]\n",
      "273 [D loss: 1.002748] [G loss: 1.000616]\n",
      "274 [D loss: 1.002657] [G loss: 1.000587]\n",
      "275 [D loss: 1.002651] [G loss: 1.000365]\n",
      "276 [D loss: 1.002606] [G loss: 1.000377]\n",
      "277 [D loss: 1.002611] [G loss: 1.000460]\n",
      "278 [D loss: 1.002758] [G loss: 1.000719]\n",
      "279 [D loss: 1.002781] [G loss: 1.000572]\n",
      "280 [D loss: 1.002694] [G loss: 1.000514]\n",
      "281 [D loss: 1.002609] [G loss: 1.000644]\n",
      "282 [D loss: 1.002812] [G loss: 1.000552]\n",
      "283 [D loss: 1.002716] [G loss: 1.000583]\n",
      "284 [D loss: 1.002685] [G loss: 1.000507]\n",
      "285 [D loss: 1.002823] [G loss: 1.000424]\n",
      "286 [D loss: 1.002936] [G loss: 1.000631]\n",
      "287 [D loss: 1.002570] [G loss: 1.000722]\n",
      "288 [D loss: 1.002794] [G loss: 1.000492]\n",
      "289 [D loss: 1.002708] [G loss: 1.000647]\n",
      "290 [D loss: 1.002814] [G loss: 1.000710]\n",
      "291 [D loss: 1.002723] [G loss: 1.000487]\n",
      "292 [D loss: 1.002879] [G loss: 1.000446]\n",
      "293 [D loss: 1.002635] [G loss: 1.000668]\n",
      "294 [D loss: 1.002676] [G loss: 1.000380]\n",
      "295 [D loss: 1.002643] [G loss: 1.000438]\n",
      "296 [D loss: 1.002503] [G loss: 1.000418]\n",
      "297 [D loss: 1.002845] [G loss: 1.000391]\n",
      "298 [D loss: 1.002965] [G loss: 1.000346]\n",
      "299 [D loss: 1.002717] [G loss: 1.000303]\n",
      "300 [D loss: 1.002763] [G loss: 1.000323]\n",
      "301 [D loss: 1.002880] [G loss: 1.000433]\n",
      "302 [D loss: 1.002614] [G loss: 1.000305]\n",
      "303 [D loss: 1.002871] [G loss: 1.000215]\n",
      "304 [D loss: 1.002989] [G loss: 1.000235]\n",
      "305 [D loss: 1.002881] [G loss: 1.000231]\n",
      "306 [D loss: 1.002974] [G loss: 1.000289]\n",
      "307 [D loss: 1.002952] [G loss: 1.000157]\n",
      "308 [D loss: 1.002900] [G loss: 1.000241]\n",
      "309 [D loss: 1.003021] [G loss: 1.000084]\n",
      "310 [D loss: 1.002865] [G loss: 1.000244]\n",
      "311 [D loss: 1.002988] [G loss: 1.000195]\n",
      "312 [D loss: 1.002882] [G loss: 1.000015]\n",
      "313 [D loss: 1.002930] [G loss: 1.000276]\n",
      "314 [D loss: 1.003031] [G loss: 1.000164]\n",
      "315 [D loss: 1.002964] [G loss: 1.000271]\n",
      "316 [D loss: 1.002889] [G loss: 1.000228]\n",
      "317 [D loss: 1.003035] [G loss: 1.000053]\n",
      "318 [D loss: 1.002840] [G loss: 1.000354]\n",
      "319 [D loss: 1.003028] [G loss: 1.000230]\n",
      "320 [D loss: 1.003088] [G loss: 1.000084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321 [D loss: 1.003060] [G loss: 1.000143]\n",
      "322 [D loss: 1.003066] [G loss: 1.000068]\n",
      "323 [D loss: 1.002916] [G loss: 0.999912]\n",
      "324 [D loss: 1.003137] [G loss: 1.000306]\n",
      "325 [D loss: 1.002807] [G loss: 1.000454]\n",
      "326 [D loss: 1.002746] [G loss: 1.000715]\n",
      "327 [D loss: 1.002730] [G loss: 1.000570]\n",
      "328 [D loss: 1.002808] [G loss: 1.000495]\n",
      "329 [D loss: 1.002797] [G loss: 1.000630]\n",
      "330 [D loss: 1.002974] [G loss: 1.000573]\n",
      "331 [D loss: 1.002751] [G loss: 1.000645]\n",
      "332 [D loss: 1.002924] [G loss: 1.000446]\n",
      "333 [D loss: 1.002937] [G loss: 1.000414]\n",
      "334 [D loss: 1.002739] [G loss: 1.000452]\n",
      "335 [D loss: 1.002960] [G loss: 1.000569]\n",
      "336 [D loss: 1.002902] [G loss: 1.000446]\n",
      "337 [D loss: 1.002963] [G loss: 1.000426]\n",
      "338 [D loss: 1.003019] [G loss: 1.000600]\n",
      "339 [D loss: 1.002892] [G loss: 1.000371]\n",
      "340 [D loss: 1.003070] [G loss: 1.000390]\n",
      "341 [D loss: 1.002867] [G loss: 1.000519]\n",
      "342 [D loss: 1.002711] [G loss: 1.000548]\n",
      "343 [D loss: 1.002908] [G loss: 1.000459]\n",
      "344 [D loss: 1.002691] [G loss: 1.000565]\n",
      "345 [D loss: 1.002786] [G loss: 1.000615]\n",
      "346 [D loss: 1.002573] [G loss: 1.000538]\n",
      "347 [D loss: 1.002755] [G loss: 1.000865]\n",
      "348 [D loss: 1.002868] [G loss: 1.000502]\n",
      "349 [D loss: 1.002823] [G loss: 1.000583]\n",
      "350 [D loss: 1.002839] [G loss: 1.000657]\n",
      "351 [D loss: 1.002813] [G loss: 1.000595]\n",
      "352 [D loss: 1.002797] [G loss: 1.000435]\n",
      "353 [D loss: 1.002721] [G loss: 1.000338]\n",
      "354 [D loss: 1.003023] [G loss: 1.000387]\n",
      "355 [D loss: 1.002793] [G loss: 1.000404]\n",
      "356 [D loss: 1.002649] [G loss: 1.000447]\n",
      "357 [D loss: 1.002929] [G loss: 1.000433]\n",
      "358 [D loss: 1.002804] [G loss: 1.000521]\n",
      "359 [D loss: 1.003024] [G loss: 1.000430]\n",
      "360 [D loss: 1.003008] [G loss: 1.000234]\n",
      "361 [D loss: 1.002899] [G loss: 1.000687]\n",
      "362 [D loss: 1.002797] [G loss: 1.000385]\n",
      "363 [D loss: 1.002777] [G loss: 1.000576]\n",
      "364 [D loss: 1.002927] [G loss: 1.000351]\n",
      "365 [D loss: 1.002842] [G loss: 1.000467]\n",
      "366 [D loss: 1.002694] [G loss: 1.000524]\n",
      "367 [D loss: 1.002838] [G loss: 1.000402]\n",
      "368 [D loss: 1.002898] [G loss: 1.000402]\n",
      "369 [D loss: 1.002810] [G loss: 1.000404]\n",
      "370 [D loss: 1.002782] [G loss: 1.000226]\n",
      "371 [D loss: 1.002667] [G loss: 1.000261]\n",
      "372 [D loss: 1.002887] [G loss: 1.000350]\n",
      "373 [D loss: 1.002781] [G loss: 1.000422]\n",
      "374 [D loss: 1.002808] [G loss: 1.000534]\n",
      "375 [D loss: 1.002742] [G loss: 1.000572]\n",
      "376 [D loss: 1.002700] [G loss: 1.000445]\n",
      "377 [D loss: 1.002806] [G loss: 1.000374]\n",
      "378 [D loss: 1.002852] [G loss: 1.000371]\n",
      "379 [D loss: 1.002878] [G loss: 1.000358]\n",
      "380 [D loss: 1.002765] [G loss: 1.000210]\n",
      "381 [D loss: 1.002823] [G loss: 1.000396]\n",
      "382 [D loss: 1.002959] [G loss: 1.000305]\n",
      "383 [D loss: 1.002696] [G loss: 1.000309]\n",
      "384 [D loss: 1.002992] [G loss: 1.000232]\n",
      "385 [D loss: 1.002822] [G loss: 0.999984]\n",
      "386 [D loss: 1.002997] [G loss: 1.000162]\n",
      "387 [D loss: 1.002789] [G loss: 1.000240]\n",
      "388 [D loss: 1.003024] [G loss: 0.999936]\n",
      "389 [D loss: 1.002868] [G loss: 1.000019]\n",
      "390 [D loss: 1.003015] [G loss: 1.000149]\n",
      "391 [D loss: 1.003037] [G loss: 1.000347]\n",
      "392 [D loss: 1.002789] [G loss: 1.000182]\n",
      "393 [D loss: 1.002810] [G loss: 1.000183]\n",
      "394 [D loss: 1.002889] [G loss: 1.000189]\n",
      "395 [D loss: 1.003009] [G loss: 1.000073]\n",
      "396 [D loss: 1.003042] [G loss: 0.999944]\n",
      "397 [D loss: 1.003035] [G loss: 0.999936]\n",
      "398 [D loss: 1.003095] [G loss: 0.999930]\n",
      "399 [D loss: 1.003162] [G loss: 0.999899]\n",
      "400 [D loss: 1.002842] [G loss: 0.999902]\n",
      "401 [D loss: 1.003065] [G loss: 1.000235]\n",
      "402 [D loss: 1.003029] [G loss: 0.999866]\n",
      "403 [D loss: 1.003022] [G loss: 1.000104]\n",
      "404 [D loss: 1.003012] [G loss: 0.999969]\n",
      "405 [D loss: 1.002907] [G loss: 1.000053]\n",
      "406 [D loss: 1.002879] [G loss: 1.000170]\n",
      "407 [D loss: 1.002991] [G loss: 1.000250]\n",
      "408 [D loss: 1.003008] [G loss: 1.000108]\n",
      "409 [D loss: 1.002802] [G loss: 1.000311]\n",
      "410 [D loss: 1.002644] [G loss: 1.000336]\n",
      "411 [D loss: 1.002744] [G loss: 1.000332]\n",
      "412 [D loss: 1.002673] [G loss: 1.000625]\n",
      "413 [D loss: 1.002994] [G loss: 1.000370]\n",
      "414 [D loss: 1.002853] [G loss: 1.000423]\n",
      "415 [D loss: 1.002793] [G loss: 1.000496]\n",
      "416 [D loss: 1.002795] [G loss: 1.000348]\n",
      "417 [D loss: 1.002937] [G loss: 1.000418]\n",
      "418 [D loss: 1.002779] [G loss: 1.000504]\n",
      "419 [D loss: 1.002766] [G loss: 1.000328]\n",
      "420 [D loss: 1.002968] [G loss: 1.000361]\n",
      "421 [D loss: 1.002743] [G loss: 1.000511]\n",
      "422 [D loss: 1.002822] [G loss: 1.000401]\n",
      "423 [D loss: 1.002926] [G loss: 1.000352]\n",
      "424 [D loss: 1.002881] [G loss: 1.000513]\n",
      "425 [D loss: 1.002934] [G loss: 1.000367]\n",
      "426 [D loss: 1.002992] [G loss: 1.000354]\n",
      "427 [D loss: 1.002831] [G loss: 1.000360]\n",
      "428 [D loss: 1.002712] [G loss: 1.000204]\n",
      "429 [D loss: 1.002927] [G loss: 1.000428]\n",
      "430 [D loss: 1.002910] [G loss: 1.000437]\n",
      "431 [D loss: 1.002812] [G loss: 1.000264]\n",
      "432 [D loss: 1.002780] [G loss: 1.000373]\n",
      "433 [D loss: 1.003092] [G loss: 1.000613]\n",
      "434 [D loss: 1.002763] [G loss: 1.000595]\n",
      "435 [D loss: 1.002924] [G loss: 1.000464]\n",
      "436 [D loss: 1.002831] [G loss: 1.000584]\n",
      "437 [D loss: 1.002991] [G loss: 1.000570]\n",
      "438 [D loss: 1.002893] [G loss: 1.000802]\n",
      "439 [D loss: 1.002768] [G loss: 1.000677]\n",
      "440 [D loss: 1.002861] [G loss: 1.000644]\n",
      "441 [D loss: 1.002890] [G loss: 1.000848]\n",
      "442 [D loss: 1.002607] [G loss: 1.000854]\n",
      "443 [D loss: 1.002752] [G loss: 1.000849]\n",
      "444 [D loss: 1.002660] [G loss: 1.000939]\n",
      "445 [D loss: 1.002627] [G loss: 1.000881]\n",
      "446 [D loss: 1.002441] [G loss: 1.000716]\n",
      "447 [D loss: 1.002762] [G loss: 1.001026]\n",
      "448 [D loss: 1.002684] [G loss: 1.000906]\n",
      "449 [D loss: 1.002671] [G loss: 1.000937]\n",
      "450 [D loss: 1.002864] [G loss: 1.000685]\n",
      "451 [D loss: 1.002814] [G loss: 1.000848]\n",
      "452 [D loss: 1.002704] [G loss: 1.000853]\n",
      "453 [D loss: 1.002715] [G loss: 1.000617]\n",
      "454 [D loss: 1.002978] [G loss: 1.000874]\n",
      "455 [D loss: 1.002742] [G loss: 1.000549]\n",
      "456 [D loss: 1.002853] [G loss: 1.000766]\n",
      "457 [D loss: 1.002720] [G loss: 1.000789]\n",
      "458 [D loss: 1.002733] [G loss: 1.000747]\n",
      "459 [D loss: 1.002729] [G loss: 1.000823]\n",
      "460 [D loss: 1.002728] [G loss: 1.000719]\n",
      "461 [D loss: 1.002845] [G loss: 1.000472]\n",
      "462 [D loss: 1.002743] [G loss: 1.000451]\n",
      "463 [D loss: 1.002920] [G loss: 1.000829]\n",
      "464 [D loss: 1.002776] [G loss: 1.000565]\n",
      "465 [D loss: 1.002844] [G loss: 1.000406]\n",
      "466 [D loss: 1.002819] [G loss: 1.000485]\n",
      "467 [D loss: 1.002977] [G loss: 1.000541]\n",
      "468 [D loss: 1.002625] [G loss: 1.000559]\n",
      "469 [D loss: 1.002933] [G loss: 1.000389]\n",
      "470 [D loss: 1.003010] [G loss: 1.000529]\n",
      "471 [D loss: 1.002898] [G loss: 1.000649]\n",
      "472 [D loss: 1.002780] [G loss: 1.000635]\n",
      "473 [D loss: 1.002971] [G loss: 1.000388]\n",
      "474 [D loss: 1.002853] [G loss: 1.000541]\n",
      "475 [D loss: 1.002852] [G loss: 1.000675]\n",
      "476 [D loss: 1.002918] [G loss: 1.000506]\n",
      "477 [D loss: 1.002897] [G loss: 1.000358]\n",
      "478 [D loss: 1.002707] [G loss: 1.000731]\n",
      "479 [D loss: 1.002832] [G loss: 1.000570]\n",
      "480 [D loss: 1.002820] [G loss: 1.000502]\n",
      "481 [D loss: 1.002694] [G loss: 1.000645]\n",
      "482 [D loss: 1.002871] [G loss: 1.000472]\n",
      "483 [D loss: 1.002689] [G loss: 1.000626]\n",
      "484 [D loss: 1.002873] [G loss: 1.000511]\n",
      "485 [D loss: 1.002825] [G loss: 1.000531]\n",
      "486 [D loss: 1.002866] [G loss: 1.000613]\n",
      "487 [D loss: 1.002981] [G loss: 1.000804]\n",
      "488 [D loss: 1.002944] [G loss: 1.000609]\n",
      "489 [D loss: 1.002778] [G loss: 1.000773]\n",
      "490 [D loss: 1.002705] [G loss: 1.000653]\n",
      "491 [D loss: 1.002689] [G loss: 1.000809]\n",
      "492 [D loss: 1.002838] [G loss: 1.000468]\n",
      "493 [D loss: 1.002777] [G loss: 1.000678]\n",
      "494 [D loss: 1.002997] [G loss: 1.000591]\n",
      "495 [D loss: 1.002947] [G loss: 1.000706]\n",
      "496 [D loss: 1.002787] [G loss: 1.000466]\n",
      "497 [D loss: 1.002687] [G loss: 1.000786]\n",
      "498 [D loss: 1.002823] [G loss: 1.000725]\n",
      "499 [D loss: 1.002728] [G loss: 1.000720]\n",
      "500 [D loss: 1.002859] [G loss: 1.000697]\n",
      "501 [D loss: 1.002734] [G loss: 1.000639]\n",
      "502 [D loss: 1.002824] [G loss: 1.000568]\n",
      "503 [D loss: 1.002779] [G loss: 1.000798]\n",
      "504 [D loss: 1.002860] [G loss: 1.000502]\n",
      "505 [D loss: 1.002853] [G loss: 1.000313]\n",
      "506 [D loss: 1.002995] [G loss: 1.000548]\n",
      "507 [D loss: 1.002905] [G loss: 1.000610]\n",
      "508 [D loss: 1.002830] [G loss: 1.000542]\n",
      "509 [D loss: 1.002950] [G loss: 1.000536]\n",
      "510 [D loss: 1.002748] [G loss: 1.000484]\n",
      "511 [D loss: 1.002843] [G loss: 1.000486]\n",
      "512 [D loss: 1.003140] [G loss: 1.000678]\n",
      "513 [D loss: 1.002923] [G loss: 1.000839]\n",
      "514 [D loss: 1.002957] [G loss: 1.000373]\n",
      "515 [D loss: 1.002901] [G loss: 1.000480]\n",
      "516 [D loss: 1.002999] [G loss: 1.000407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517 [D loss: 1.002894] [G loss: 1.000319]\n",
      "518 [D loss: 1.002827] [G loss: 1.000264]\n",
      "519 [D loss: 1.002990] [G loss: 1.000379]\n",
      "520 [D loss: 1.002796] [G loss: 1.000478]\n",
      "521 [D loss: 1.003104] [G loss: 1.000333]\n",
      "522 [D loss: 1.003144] [G loss: 1.000387]\n",
      "523 [D loss: 1.003061] [G loss: 1.000350]\n",
      "524 [D loss: 1.002929] [G loss: 1.000345]\n",
      "525 [D loss: 1.002996] [G loss: 1.000543]\n",
      "526 [D loss: 1.002885] [G loss: 1.000198]\n",
      "527 [D loss: 1.002925] [G loss: 1.000479]\n",
      "528 [D loss: 1.002823] [G loss: 1.000459]\n",
      "529 [D loss: 1.003009] [G loss: 1.000270]\n",
      "530 [D loss: 1.003228] [G loss: 1.000520]\n",
      "531 [D loss: 1.002855] [G loss: 1.000307]\n",
      "532 [D loss: 1.003029] [G loss: 1.000675]\n",
      "533 [D loss: 1.003157] [G loss: 1.000316]\n",
      "534 [D loss: 1.003070] [G loss: 1.000367]\n",
      "535 [D loss: 1.002815] [G loss: 1.000323]\n",
      "536 [D loss: 1.002965] [G loss: 1.000410]\n",
      "537 [D loss: 1.002957] [G loss: 1.000577]\n",
      "538 [D loss: 1.003151] [G loss: 1.000470]\n",
      "539 [D loss: 1.003173] [G loss: 1.000398]\n",
      "540 [D loss: 1.003002] [G loss: 1.000687]\n",
      "541 [D loss: 1.002938] [G loss: 1.000442]\n",
      "542 [D loss: 1.002706] [G loss: 1.000257]\n",
      "543 [D loss: 1.003040] [G loss: 1.000199]\n",
      "544 [D loss: 1.002915] [G loss: 1.000393]\n",
      "545 [D loss: 1.002828] [G loss: 1.000640]\n",
      "546 [D loss: 1.002902] [G loss: 1.000491]\n",
      "547 [D loss: 1.002785] [G loss: 1.000538]\n",
      "548 [D loss: 1.002912] [G loss: 1.000572]\n",
      "549 [D loss: 1.002828] [G loss: 1.000715]\n",
      "550 [D loss: 1.002851] [G loss: 1.000634]\n",
      "551 [D loss: 1.002775] [G loss: 1.000803]\n",
      "552 [D loss: 1.002758] [G loss: 1.000733]\n",
      "553 [D loss: 1.002612] [G loss: 1.000797]\n",
      "554 [D loss: 1.002733] [G loss: 1.000616]\n",
      "555 [D loss: 1.002683] [G loss: 1.000831]\n",
      "556 [D loss: 1.002821] [G loss: 1.000864]\n",
      "557 [D loss: 1.002664] [G loss: 1.000712]\n",
      "558 [D loss: 1.002738] [G loss: 1.000817]\n",
      "559 [D loss: 1.002841] [G loss: 1.000949]\n",
      "560 [D loss: 1.002698] [G loss: 1.000884]\n",
      "561 [D loss: 1.002859] [G loss: 1.000697]\n",
      "562 [D loss: 1.002618] [G loss: 1.000612]\n",
      "563 [D loss: 1.002810] [G loss: 1.000672]\n",
      "564 [D loss: 1.002901] [G loss: 1.000719]\n",
      "565 [D loss: 1.002792] [G loss: 1.000650]\n",
      "566 [D loss: 1.002848] [G loss: 1.000662]\n",
      "567 [D loss: 1.002966] [G loss: 1.000579]\n",
      "568 [D loss: 1.002858] [G loss: 1.000396]\n",
      "569 [D loss: 1.002893] [G loss: 1.000387]\n",
      "570 [D loss: 1.002916] [G loss: 1.000531]\n",
      "571 [D loss: 1.002990] [G loss: 1.000172]\n",
      "572 [D loss: 1.002809] [G loss: 1.000384]\n",
      "573 [D loss: 1.003012] [G loss: 1.000529]\n",
      "574 [D loss: 1.003029] [G loss: 1.000443]\n",
      "575 [D loss: 1.002993] [G loss: 1.000390]\n",
      "576 [D loss: 1.002979] [G loss: 1.000529]\n",
      "577 [D loss: 1.002932] [G loss: 1.000467]\n",
      "578 [D loss: 1.002934] [G loss: 1.000135]\n",
      "579 [D loss: 1.002759] [G loss: 1.000321]\n",
      "580 [D loss: 1.002985] [G loss: 1.000291]\n",
      "581 [D loss: 1.002882] [G loss: 1.000402]\n",
      "582 [D loss: 1.003046] [G loss: 1.000474]\n",
      "583 [D loss: 1.002852] [G loss: 1.000545]\n",
      "584 [D loss: 1.002986] [G loss: 1.000467]\n",
      "585 [D loss: 1.002986] [G loss: 1.000604]\n",
      "586 [D loss: 1.002736] [G loss: 1.000311]\n",
      "587 [D loss: 1.002959] [G loss: 1.000176]\n",
      "588 [D loss: 1.002992] [G loss: 1.000430]\n",
      "589 [D loss: 1.002874] [G loss: 1.000668]\n",
      "590 [D loss: 1.002933] [G loss: 1.000543]\n",
      "591 [D loss: 1.002859] [G loss: 1.000455]\n",
      "592 [D loss: 1.002858] [G loss: 1.000513]\n",
      "593 [D loss: 1.002688] [G loss: 1.000534]\n",
      "594 [D loss: 1.002782] [G loss: 1.000768]\n",
      "595 [D loss: 1.002854] [G loss: 1.000650]\n",
      "596 [D loss: 1.002828] [G loss: 1.000687]\n",
      "597 [D loss: 1.002723] [G loss: 1.000532]\n",
      "598 [D loss: 1.002780] [G loss: 1.000702]\n",
      "599 [D loss: 1.003093] [G loss: 1.000594]\n",
      "600 [D loss: 1.002933] [G loss: 1.000633]\n",
      "601 [D loss: 1.003037] [G loss: 1.000740]\n",
      "602 [D loss: 1.002872] [G loss: 1.000584]\n",
      "603 [D loss: 1.002798] [G loss: 1.000473]\n",
      "604 [D loss: 1.002886] [G loss: 1.000955]\n",
      "605 [D loss: 1.002907] [G loss: 1.000776]\n",
      "606 [D loss: 1.002972] [G loss: 1.000669]\n",
      "607 [D loss: 1.002932] [G loss: 1.000565]\n",
      "608 [D loss: 1.002701] [G loss: 1.000552]\n",
      "609 [D loss: 1.002809] [G loss: 1.000767]\n",
      "610 [D loss: 1.002827] [G loss: 1.000368]\n",
      "611 [D loss: 1.002900] [G loss: 1.000704]\n",
      "612 [D loss: 1.003028] [G loss: 1.000562]\n",
      "613 [D loss: 1.002811] [G loss: 1.000811]\n",
      "614 [D loss: 1.002827] [G loss: 1.000699]\n",
      "615 [D loss: 1.003028] [G loss: 1.000620]\n",
      "616 [D loss: 1.002898] [G loss: 1.000700]\n",
      "617 [D loss: 1.002777] [G loss: 1.000242]\n",
      "618 [D loss: 1.002934] [G loss: 1.000615]\n",
      "619 [D loss: 1.002836] [G loss: 1.000863]\n",
      "620 [D loss: 1.002845] [G loss: 1.000566]\n",
      "621 [D loss: 1.002841] [G loss: 1.000479]\n",
      "622 [D loss: 1.002822] [G loss: 1.000378]\n",
      "623 [D loss: 1.002926] [G loss: 1.000369]\n",
      "624 [D loss: 1.002933] [G loss: 1.000538]\n",
      "625 [D loss: 1.002901] [G loss: 1.000602]\n",
      "626 [D loss: 1.002956] [G loss: 1.000374]\n",
      "627 [D loss: 1.003016] [G loss: 1.000353]\n",
      "628 [D loss: 1.002963] [G loss: 1.000697]\n",
      "629 [D loss: 1.002940] [G loss: 1.000542]\n",
      "630 [D loss: 1.002710] [G loss: 1.000629]\n",
      "631 [D loss: 1.002923] [G loss: 1.000417]\n",
      "632 [D loss: 1.002997] [G loss: 1.000382]\n",
      "633 [D loss: 1.002941] [G loss: 1.000663]\n",
      "634 [D loss: 1.002933] [G loss: 1.000624]\n",
      "635 [D loss: 1.002816] [G loss: 1.000595]\n",
      "636 [D loss: 1.002971] [G loss: 1.000457]\n",
      "637 [D loss: 1.002859] [G loss: 1.000368]\n",
      "638 [D loss: 1.002748] [G loss: 1.000584]\n",
      "639 [D loss: 1.002860] [G loss: 1.000501]\n",
      "640 [D loss: 1.002814] [G loss: 1.000498]\n",
      "641 [D loss: 1.002907] [G loss: 1.000671]\n",
      "642 [D loss: 1.002841] [G loss: 1.000444]\n",
      "643 [D loss: 1.002743] [G loss: 1.000507]\n",
      "644 [D loss: 1.002720] [G loss: 1.000462]\n",
      "645 [D loss: 1.002698] [G loss: 1.000391]\n",
      "646 [D loss: 1.002813] [G loss: 1.000363]\n",
      "647 [D loss: 1.003045] [G loss: 1.000200]\n",
      "648 [D loss: 1.002905] [G loss: 1.000682]\n",
      "649 [D loss: 1.002953] [G loss: 1.000422]\n",
      "650 [D loss: 1.002967] [G loss: 1.000385]\n",
      "651 [D loss: 1.003016] [G loss: 1.000391]\n",
      "652 [D loss: 1.002766] [G loss: 1.000559]\n",
      "653 [D loss: 1.002831] [G loss: 1.000536]\n",
      "654 [D loss: 1.002901] [G loss: 1.000729]\n",
      "655 [D loss: 1.002877] [G loss: 1.000453]\n",
      "656 [D loss: 1.002859] [G loss: 1.000441]\n",
      "657 [D loss: 1.002812] [G loss: 1.000397]\n",
      "658 [D loss: 1.002808] [G loss: 1.000502]\n",
      "659 [D loss: 1.002850] [G loss: 1.000508]\n",
      "660 [D loss: 1.002663] [G loss: 1.000458]\n",
      "661 [D loss: 1.002957] [G loss: 1.000564]\n",
      "662 [D loss: 1.002743] [G loss: 1.000646]\n",
      "663 [D loss: 1.002774] [G loss: 1.000492]\n",
      "664 [D loss: 1.002817] [G loss: 1.000535]\n",
      "665 [D loss: 1.002946] [G loss: 1.000319]\n",
      "666 [D loss: 1.003063] [G loss: 1.000601]\n",
      "667 [D loss: 1.002824] [G loss: 1.000518]\n",
      "668 [D loss: 1.002951] [G loss: 1.000675]\n",
      "669 [D loss: 1.002865] [G loss: 1.000434]\n",
      "670 [D loss: 1.002945] [G loss: 1.000508]\n",
      "671 [D loss: 1.002932] [G loss: 1.000523]\n",
      "672 [D loss: 1.002932] [G loss: 1.000671]\n",
      "673 [D loss: 1.002851] [G loss: 1.000363]\n",
      "674 [D loss: 1.003305] [G loss: 1.000504]\n",
      "675 [D loss: 1.003230] [G loss: 1.000811]\n",
      "676 [D loss: 1.002945] [G loss: 1.000603]\n",
      "677 [D loss: 1.002863] [G loss: 1.000281]\n",
      "678 [D loss: 1.002978] [G loss: 1.000438]\n",
      "679 [D loss: 1.002973] [G loss: 1.000343]\n",
      "680 [D loss: 1.002928] [G loss: 1.000366]\n",
      "681 [D loss: 1.002815] [G loss: 1.000681]\n",
      "682 [D loss: 1.002939] [G loss: 1.000682]\n",
      "683 [D loss: 1.002984] [G loss: 1.000425]\n",
      "684 [D loss: 1.002974] [G loss: 1.000467]\n",
      "685 [D loss: 1.003048] [G loss: 1.000406]\n",
      "686 [D loss: 1.002852] [G loss: 1.000353]\n",
      "687 [D loss: 1.002939] [G loss: 1.000528]\n",
      "688 [D loss: 1.002683] [G loss: 1.000449]\n",
      "689 [D loss: 1.002954] [G loss: 1.000468]\n",
      "690 [D loss: 1.002881] [G loss: 1.000691]\n",
      "691 [D loss: 1.003051] [G loss: 1.000369]\n",
      "692 [D loss: 1.002715] [G loss: 1.000635]\n",
      "693 [D loss: 1.002685] [G loss: 1.000508]\n",
      "694 [D loss: 1.002954] [G loss: 1.000464]\n",
      "695 [D loss: 1.003061] [G loss: 1.000354]\n",
      "696 [D loss: 1.002814] [G loss: 1.000367]\n",
      "697 [D loss: 1.002923] [G loss: 1.000560]\n",
      "698 [D loss: 1.002911] [G loss: 1.000269]\n",
      "699 [D loss: 1.002931] [G loss: 1.000449]\n",
      "700 [D loss: 1.003029] [G loss: 1.000545]\n",
      "701 [D loss: 1.002876] [G loss: 1.000590]\n",
      "702 [D loss: 1.002853] [G loss: 1.000475]\n",
      "703 [D loss: 1.002857] [G loss: 1.000412]\n",
      "704 [D loss: 1.002954] [G loss: 1.000402]\n",
      "705 [D loss: 1.002879] [G loss: 1.000622]\n",
      "706 [D loss: 1.002782] [G loss: 1.000408]\n",
      "707 [D loss: 1.002848] [G loss: 1.000348]\n",
      "708 [D loss: 1.002813] [G loss: 1.000407]\n",
      "709 [D loss: 1.002958] [G loss: 1.000362]\n",
      "710 [D loss: 1.002758] [G loss: 1.000593]\n",
      "711 [D loss: 1.002739] [G loss: 1.000566]\n",
      "712 [D loss: 1.002880] [G loss: 1.000322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713 [D loss: 1.002829] [G loss: 1.000335]\n",
      "714 [D loss: 1.002736] [G loss: 1.000198]\n",
      "715 [D loss: 1.003002] [G loss: 1.000450]\n",
      "716 [D loss: 1.002957] [G loss: 1.000247]\n",
      "717 [D loss: 1.002943] [G loss: 1.000463]\n",
      "718 [D loss: 1.002897] [G loss: 1.000435]\n",
      "719 [D loss: 1.002969] [G loss: 1.000347]\n",
      "720 [D loss: 1.002986] [G loss: 1.000469]\n",
      "721 [D loss: 1.003031] [G loss: 1.000329]\n",
      "722 [D loss: 1.002911] [G loss: 1.000399]\n",
      "723 [D loss: 1.002985] [G loss: 1.000317]\n",
      "724 [D loss: 1.003043] [G loss: 1.000559]\n",
      "725 [D loss: 1.002905] [G loss: 1.000550]\n",
      "726 [D loss: 1.002871] [G loss: 1.000508]\n",
      "727 [D loss: 1.002937] [G loss: 1.000258]\n",
      "728 [D loss: 1.002979] [G loss: 1.000212]\n",
      "729 [D loss: 1.002790] [G loss: 1.000621]\n",
      "730 [D loss: 1.002846] [G loss: 1.000274]\n",
      "731 [D loss: 1.002852] [G loss: 1.000295]\n",
      "732 [D loss: 1.003065] [G loss: 1.000499]\n",
      "733 [D loss: 1.002807] [G loss: 1.000672]\n",
      "734 [D loss: 1.002836] [G loss: 1.000288]\n",
      "735 [D loss: 1.002776] [G loss: 1.000412]\n",
      "736 [D loss: 1.002956] [G loss: 1.000176]\n",
      "737 [D loss: 1.003048] [G loss: 1.000400]\n",
      "738 [D loss: 1.002907] [G loss: 1.000231]\n",
      "739 [D loss: 1.002833] [G loss: 1.000384]\n",
      "740 [D loss: 1.002877] [G loss: 1.000222]\n",
      "741 [D loss: 1.003169] [G loss: 1.000515]\n",
      "742 [D loss: 1.002932] [G loss: 1.000366]\n",
      "743 [D loss: 1.003143] [G loss: 1.000199]\n",
      "744 [D loss: 1.002899] [G loss: 1.000245]\n",
      "745 [D loss: 1.002852] [G loss: 1.000441]\n",
      "746 [D loss: 1.002933] [G loss: 1.000291]\n",
      "747 [D loss: 1.002958] [G loss: 1.000353]\n",
      "748 [D loss: 1.003176] [G loss: 1.000245]\n",
      "749 [D loss: 1.002685] [G loss: 1.000536]\n",
      "750 [D loss: 1.002839] [G loss: 1.000381]\n",
      "751 [D loss: 1.002905] [G loss: 1.000473]\n",
      "752 [D loss: 1.003075] [G loss: 1.000395]\n",
      "753 [D loss: 1.003009] [G loss: 1.000359]\n",
      "754 [D loss: 1.002927] [G loss: 1.000509]\n",
      "755 [D loss: 1.003035] [G loss: 1.000472]\n",
      "756 [D loss: 1.002904] [G loss: 1.000364]\n",
      "757 [D loss: 1.002870] [G loss: 1.000344]\n",
      "758 [D loss: 1.003110] [G loss: 1.000295]\n",
      "759 [D loss: 1.002829] [G loss: 1.000477]\n",
      "760 [D loss: 1.002985] [G loss: 1.000389]\n",
      "761 [D loss: 1.003052] [G loss: 1.000528]\n",
      "762 [D loss: 1.002717] [G loss: 1.000379]\n",
      "763 [D loss: 1.003001] [G loss: 1.000449]\n",
      "764 [D loss: 1.002837] [G loss: 1.000653]\n",
      "765 [D loss: 1.002947] [G loss: 1.000525]\n",
      "766 [D loss: 1.002894] [G loss: 1.000375]\n",
      "767 [D loss: 1.003176] [G loss: 1.000404]\n",
      "768 [D loss: 1.002796] [G loss: 1.000334]\n",
      "769 [D loss: 1.002960] [G loss: 1.000535]\n",
      "770 [D loss: 1.002836] [G loss: 1.000491]\n",
      "771 [D loss: 1.002956] [G loss: 1.000409]\n",
      "772 [D loss: 1.002836] [G loss: 1.000351]\n",
      "773 [D loss: 1.002886] [G loss: 1.000519]\n",
      "774 [D loss: 1.003030] [G loss: 1.000617]\n",
      "775 [D loss: 1.002962] [G loss: 1.000216]\n",
      "776 [D loss: 1.002828] [G loss: 1.000121]\n",
      "777 [D loss: 1.003095] [G loss: 1.000369]\n",
      "778 [D loss: 1.002976] [G loss: 1.000346]\n",
      "779 [D loss: 1.002789] [G loss: 1.000316]\n",
      "780 [D loss: 1.002982] [G loss: 1.000547]\n",
      "781 [D loss: 1.002914] [G loss: 1.000310]\n",
      "782 [D loss: 1.003050] [G loss: 1.000332]\n",
      "783 [D loss: 1.002785] [G loss: 1.000239]\n",
      "784 [D loss: 1.003145] [G loss: 1.000658]\n",
      "785 [D loss: 1.002961] [G loss: 1.000475]\n",
      "786 [D loss: 1.002948] [G loss: 1.000672]\n",
      "787 [D loss: 1.003022] [G loss: 1.000370]\n",
      "788 [D loss: 1.002895] [G loss: 1.000385]\n",
      "789 [D loss: 1.002994] [G loss: 1.000559]\n",
      "790 [D loss: 1.002993] [G loss: 1.000438]\n",
      "791 [D loss: 1.002875] [G loss: 1.000425]\n",
      "792 [D loss: 1.002967] [G loss: 1.000443]\n",
      "793 [D loss: 1.003144] [G loss: 1.000731]\n",
      "794 [D loss: 1.003015] [G loss: 1.000372]\n",
      "795 [D loss: 1.002863] [G loss: 1.000360]\n",
      "796 [D loss: 1.002953] [G loss: 1.000443]\n",
      "797 [D loss: 1.002871] [G loss: 1.000312]\n",
      "798 [D loss: 1.002903] [G loss: 1.000725]\n",
      "799 [D loss: 1.002758] [G loss: 1.000510]\n",
      "800 [D loss: 1.002983] [G loss: 1.000449]\n",
      "801 [D loss: 1.002764] [G loss: 1.000359]\n",
      "802 [D loss: 1.002955] [G loss: 1.000323]\n",
      "803 [D loss: 1.002910] [G loss: 1.000288]\n",
      "804 [D loss: 1.002826] [G loss: 1.000377]\n",
      "805 [D loss: 1.002925] [G loss: 1.000472]\n",
      "806 [D loss: 1.002887] [G loss: 1.000284]\n",
      "807 [D loss: 1.002967] [G loss: 1.000405]\n",
      "808 [D loss: 1.003003] [G loss: 1.000460]\n",
      "809 [D loss: 1.003006] [G loss: 1.000455]\n",
      "810 [D loss: 1.002830] [G loss: 1.000561]\n",
      "811 [D loss: 1.002993] [G loss: 1.000569]\n",
      "812 [D loss: 1.002973] [G loss: 1.000685]\n",
      "813 [D loss: 1.002879] [G loss: 1.000654]\n",
      "814 [D loss: 1.002844] [G loss: 1.000478]\n",
      "815 [D loss: 1.002935] [G loss: 1.000396]\n",
      "816 [D loss: 1.002819] [G loss: 1.000377]\n",
      "817 [D loss: 1.003002] [G loss: 1.000507]\n",
      "818 [D loss: 1.003018] [G loss: 1.000561]\n",
      "819 [D loss: 1.003129] [G loss: 1.000469]\n",
      "820 [D loss: 1.002862] [G loss: 1.000294]\n",
      "821 [D loss: 1.002829] [G loss: 1.000411]\n",
      "822 [D loss: 1.002896] [G loss: 1.000528]\n",
      "823 [D loss: 1.002875] [G loss: 1.000226]\n",
      "824 [D loss: 1.002780] [G loss: 1.000342]\n",
      "825 [D loss: 1.003056] [G loss: 1.000677]\n",
      "826 [D loss: 1.002900] [G loss: 1.000317]\n",
      "827 [D loss: 1.002796] [G loss: 1.000331]\n",
      "828 [D loss: 1.003081] [G loss: 1.000597]\n",
      "829 [D loss: 1.002863] [G loss: 1.000576]\n",
      "830 [D loss: 1.002954] [G loss: 1.000510]\n",
      "831 [D loss: 1.002907] [G loss: 1.000548]\n",
      "832 [D loss: 1.002601] [G loss: 1.000502]\n",
      "833 [D loss: 1.002856] [G loss: 1.000467]\n",
      "834 [D loss: 1.002863] [G loss: 1.000524]\n",
      "835 [D loss: 1.002916] [G loss: 1.000331]\n",
      "836 [D loss: 1.003006] [G loss: 1.000523]\n",
      "837 [D loss: 1.002842] [G loss: 1.000410]\n",
      "838 [D loss: 1.002894] [G loss: 1.000338]\n",
      "839 [D loss: 1.002931] [G loss: 1.000574]\n",
      "840 [D loss: 1.002790] [G loss: 1.000250]\n",
      "841 [D loss: 1.003007] [G loss: 1.000658]\n",
      "842 [D loss: 1.002912] [G loss: 1.000359]\n",
      "843 [D loss: 1.002978] [G loss: 1.000331]\n",
      "844 [D loss: 1.003040] [G loss: 1.000471]\n",
      "845 [D loss: 1.003049] [G loss: 1.000434]\n",
      "846 [D loss: 1.002745] [G loss: 1.000347]\n",
      "847 [D loss: 1.002805] [G loss: 1.000569]\n",
      "848 [D loss: 1.002839] [G loss: 1.000433]\n",
      "849 [D loss: 1.002908] [G loss: 1.000682]\n",
      "850 [D loss: 1.002937] [G loss: 1.000639]\n",
      "851 [D loss: 1.002980] [G loss: 1.000582]\n",
      "852 [D loss: 1.002873] [G loss: 1.000297]\n",
      "853 [D loss: 1.002927] [G loss: 1.000608]\n",
      "854 [D loss: 1.002799] [G loss: 1.000401]\n",
      "855 [D loss: 1.002934] [G loss: 1.000244]\n",
      "856 [D loss: 1.002840] [G loss: 1.000501]\n",
      "857 [D loss: 1.002968] [G loss: 1.000592]\n",
      "858 [D loss: 1.002832] [G loss: 1.000485]\n",
      "859 [D loss: 1.002997] [G loss: 1.000410]\n",
      "860 [D loss: 1.003019] [G loss: 1.000351]\n",
      "861 [D loss: 1.002839] [G loss: 1.000666]\n",
      "862 [D loss: 1.003086] [G loss: 1.000272]\n",
      "863 [D loss: 1.002924] [G loss: 1.000542]\n",
      "864 [D loss: 1.002864] [G loss: 1.000244]\n",
      "865 [D loss: 1.002919] [G loss: 1.000514]\n",
      "866 [D loss: 1.002864] [G loss: 1.000469]\n",
      "867 [D loss: 1.002970] [G loss: 1.000396]\n",
      "868 [D loss: 1.002937] [G loss: 1.000483]\n",
      "869 [D loss: 1.003052] [G loss: 1.000416]\n",
      "870 [D loss: 1.002964] [G loss: 1.000496]\n",
      "871 [D loss: 1.002803] [G loss: 1.000390]\n",
      "872 [D loss: 1.002877] [G loss: 1.000537]\n",
      "873 [D loss: 1.002877] [G loss: 1.000307]\n",
      "874 [D loss: 1.003018] [G loss: 1.000467]\n",
      "875 [D loss: 1.002979] [G loss: 1.000611]\n",
      "876 [D loss: 1.003022] [G loss: 1.000464]\n",
      "877 [D loss: 1.002915] [G loss: 1.000526]\n",
      "878 [D loss: 1.003001] [G loss: 1.000378]\n",
      "879 [D loss: 1.002898] [G loss: 1.000654]\n",
      "880 [D loss: 1.002784] [G loss: 1.000404]\n",
      "881 [D loss: 1.002886] [G loss: 1.000471]\n",
      "882 [D loss: 1.002834] [G loss: 1.000416]\n",
      "883 [D loss: 1.002750] [G loss: 1.000533]\n",
      "884 [D loss: 1.002852] [G loss: 1.000374]\n",
      "885 [D loss: 1.002796] [G loss: 1.000470]\n",
      "886 [D loss: 1.002937] [G loss: 1.000577]\n",
      "887 [D loss: 1.003046] [G loss: 1.000621]\n",
      "888 [D loss: 1.002920] [G loss: 1.000332]\n",
      "889 [D loss: 1.002858] [G loss: 1.000695]\n",
      "890 [D loss: 1.002885] [G loss: 1.000585]\n",
      "891 [D loss: 1.002647] [G loss: 1.000378]\n",
      "892 [D loss: 1.003073] [G loss: 1.000547]\n",
      "893 [D loss: 1.002919] [G loss: 1.000422]\n",
      "894 [D loss: 1.002835] [G loss: 1.000404]\n",
      "895 [D loss: 1.003004] [G loss: 1.000058]\n",
      "896 [D loss: 1.002904] [G loss: 1.000393]\n",
      "897 [D loss: 1.002917] [G loss: 1.000553]\n",
      "898 [D loss: 1.002755] [G loss: 1.000278]\n",
      "899 [D loss: 1.003099] [G loss: 1.000327]\n",
      "900 [D loss: 1.002801] [G loss: 1.000535]\n",
      "901 [D loss: 1.003050] [G loss: 1.000372]\n",
      "902 [D loss: 1.002888] [G loss: 1.000449]\n",
      "903 [D loss: 1.003080] [G loss: 1.000152]\n",
      "904 [D loss: 1.002928] [G loss: 1.000347]\n",
      "905 [D loss: 1.002865] [G loss: 1.000673]\n",
      "906 [D loss: 1.002927] [G loss: 1.000303]\n",
      "907 [D loss: 1.002812] [G loss: 1.000462]\n",
      "908 [D loss: 1.003001] [G loss: 1.000367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909 [D loss: 1.002827] [G loss: 1.000581]\n",
      "910 [D loss: 1.003069] [G loss: 1.000690]\n",
      "911 [D loss: 1.002933] [G loss: 1.000382]\n",
      "912 [D loss: 1.002659] [G loss: 1.000365]\n",
      "913 [D loss: 1.002824] [G loss: 1.000164]\n",
      "914 [D loss: 1.002928] [G loss: 1.000325]\n",
      "915 [D loss: 1.002871] [G loss: 1.000498]\n",
      "916 [D loss: 1.002834] [G loss: 1.000517]\n",
      "917 [D loss: 1.003041] [G loss: 1.000574]\n",
      "918 [D loss: 1.002985] [G loss: 1.000494]\n",
      "919 [D loss: 1.002931] [G loss: 1.000357]\n",
      "920 [D loss: 1.002725] [G loss: 1.000379]\n",
      "921 [D loss: 1.002800] [G loss: 1.000342]\n",
      "922 [D loss: 1.003197] [G loss: 1.000369]\n",
      "923 [D loss: 1.002817] [G loss: 1.000304]\n",
      "924 [D loss: 1.003047] [G loss: 1.000334]\n",
      "925 [D loss: 1.003119] [G loss: 1.000466]\n",
      "926 [D loss: 1.002692] [G loss: 1.000410]\n",
      "927 [D loss: 1.003228] [G loss: 1.000514]\n",
      "928 [D loss: 1.002846] [G loss: 1.000480]\n",
      "929 [D loss: 1.002897] [G loss: 1.000426]\n",
      "930 [D loss: 1.002804] [G loss: 1.000341]\n",
      "931 [D loss: 1.002880] [G loss: 1.000526]\n",
      "932 [D loss: 1.002804] [G loss: 1.000462]\n",
      "933 [D loss: 1.002943] [G loss: 1.000470]\n",
      "934 [D loss: 1.002836] [G loss: 1.000313]\n",
      "935 [D loss: 1.002766] [G loss: 1.000312]\n",
      "936 [D loss: 1.003087] [G loss: 1.000626]\n",
      "937 [D loss: 1.002821] [G loss: 1.000422]\n",
      "938 [D loss: 1.003018] [G loss: 1.000521]\n",
      "939 [D loss: 1.002979] [G loss: 1.000515]\n",
      "940 [D loss: 1.002980] [G loss: 1.000440]\n",
      "941 [D loss: 1.002982] [G loss: 1.000297]\n",
      "942 [D loss: 1.002991] [G loss: 1.000367]\n",
      "943 [D loss: 1.002760] [G loss: 1.000503]\n",
      "944 [D loss: 1.002691] [G loss: 1.000445]\n",
      "945 [D loss: 1.002998] [G loss: 1.000298]\n",
      "946 [D loss: 1.002761] [G loss: 1.000200]\n",
      "947 [D loss: 1.002862] [G loss: 1.000387]\n",
      "948 [D loss: 1.002745] [G loss: 1.000458]\n",
      "949 [D loss: 1.003041] [G loss: 1.000539]\n",
      "950 [D loss: 1.003093] [G loss: 1.000380]\n",
      "951 [D loss: 1.002805] [G loss: 1.000302]\n",
      "952 [D loss: 1.003023] [G loss: 1.000462]\n",
      "953 [D loss: 1.002746] [G loss: 1.000398]\n",
      "954 [D loss: 1.002776] [G loss: 1.000446]\n",
      "955 [D loss: 1.003058] [G loss: 1.000508]\n",
      "956 [D loss: 1.002909] [G loss: 1.000618]\n",
      "957 [D loss: 1.003083] [G loss: 1.000423]\n",
      "958 [D loss: 1.002724] [G loss: 1.000454]\n",
      "959 [D loss: 1.002849] [G loss: 1.000368]\n",
      "960 [D loss: 1.002904] [G loss: 1.000416]\n",
      "961 [D loss: 1.002966] [G loss: 1.000350]\n",
      "962 [D loss: 1.002983] [G loss: 1.000455]\n",
      "963 [D loss: 1.002781] [G loss: 1.000278]\n",
      "964 [D loss: 1.002832] [G loss: 1.000427]\n",
      "965 [D loss: 1.002919] [G loss: 1.000502]\n",
      "966 [D loss: 1.002647] [G loss: 1.000355]\n",
      "967 [D loss: 1.002970] [G loss: 1.000353]\n",
      "968 [D loss: 1.002701] [G loss: 1.000691]\n",
      "969 [D loss: 1.003019] [G loss: 1.000385]\n",
      "970 [D loss: 1.002883] [G loss: 1.000351]\n",
      "971 [D loss: 1.002945] [G loss: 1.000518]\n",
      "972 [D loss: 1.003028] [G loss: 1.000574]\n",
      "973 [D loss: 1.003002] [G loss: 1.000348]\n",
      "974 [D loss: 1.002667] [G loss: 1.000516]\n",
      "975 [D loss: 1.002878] [G loss: 1.000298]\n",
      "976 [D loss: 1.002994] [G loss: 1.000355]\n",
      "977 [D loss: 1.002885] [G loss: 1.000175]\n",
      "978 [D loss: 1.002877] [G loss: 1.000169]\n",
      "979 [D loss: 1.003003] [G loss: 1.000343]\n",
      "980 [D loss: 1.002985] [G loss: 1.000382]\n",
      "981 [D loss: 1.002992] [G loss: 1.000585]\n",
      "982 [D loss: 1.003055] [G loss: 1.000468]\n",
      "983 [D loss: 1.002894] [G loss: 1.000344]\n",
      "984 [D loss: 1.003009] [G loss: 1.000281]\n",
      "985 [D loss: 1.003079] [G loss: 1.000350]\n",
      "986 [D loss: 1.003017] [G loss: 1.000541]\n",
      "987 [D loss: 1.003027] [G loss: 1.000493]\n",
      "988 [D loss: 1.002994] [G loss: 1.000511]\n",
      "989 [D loss: 1.002976] [G loss: 1.000585]\n",
      "990 [D loss: 1.002923] [G loss: 1.000611]\n",
      "991 [D loss: 1.002955] [G loss: 1.000460]\n",
      "992 [D loss: 1.002899] [G loss: 1.000311]\n",
      "993 [D loss: 1.002987] [G loss: 1.000411]\n",
      "994 [D loss: 1.003060] [G loss: 1.000320]\n",
      "995 [D loss: 1.002975] [G loss: 1.000289]\n",
      "996 [D loss: 1.002994] [G loss: 1.000440]\n",
      "997 [D loss: 1.002990] [G loss: 1.000490]\n",
      "998 [D loss: 1.002830] [G loss: 1.000686]\n",
      "999 [D loss: 1.003098] [G loss: 1.000622]\n",
      "1000 [D loss: 1.002838] [G loss: 1.000537]\n",
      "1001 [D loss: 1.003098] [G loss: 1.000441]\n",
      "1002 [D loss: 1.002999] [G loss: 1.000451]\n",
      "1003 [D loss: 1.002942] [G loss: 1.000488]\n",
      "1004 [D loss: 1.002975] [G loss: 1.000383]\n",
      "1005 [D loss: 1.002948] [G loss: 1.000339]\n",
      "1006 [D loss: 1.003018] [G loss: 1.000446]\n",
      "1007 [D loss: 1.002803] [G loss: 1.000430]\n",
      "1008 [D loss: 1.003016] [G loss: 1.000453]\n",
      "1009 [D loss: 1.002886] [G loss: 1.000379]\n",
      "1010 [D loss: 1.002994] [G loss: 1.000428]\n",
      "1011 [D loss: 1.002971] [G loss: 1.000477]\n",
      "1012 [D loss: 1.002993] [G loss: 1.000395]\n",
      "1013 [D loss: 1.002957] [G loss: 1.000435]\n",
      "1014 [D loss: 1.002834] [G loss: 1.000351]\n",
      "1015 [D loss: 1.003092] [G loss: 1.000345]\n",
      "1016 [D loss: 1.002763] [G loss: 1.000472]\n",
      "1017 [D loss: 1.002835] [G loss: 1.000409]\n",
      "1018 [D loss: 1.002921] [G loss: 1.000596]\n",
      "1019 [D loss: 1.002974] [G loss: 1.000495]\n",
      "1020 [D loss: 1.002645] [G loss: 1.000227]\n",
      "1021 [D loss: 1.002988] [G loss: 1.000183]\n",
      "1022 [D loss: 1.002972] [G loss: 1.000599]\n",
      "1023 [D loss: 1.002916] [G loss: 1.000529]\n",
      "1024 [D loss: 1.002852] [G loss: 1.000504]\n",
      "1025 [D loss: 1.002976] [G loss: 1.000449]\n",
      "1026 [D loss: 1.002867] [G loss: 1.000451]\n",
      "1027 [D loss: 1.003011] [G loss: 1.000284]\n",
      "1028 [D loss: 1.002850] [G loss: 1.000389]\n",
      "1029 [D loss: 1.003013] [G loss: 1.000452]\n",
      "1030 [D loss: 1.002908] [G loss: 1.000407]\n",
      "1031 [D loss: 1.002987] [G loss: 1.000416]\n",
      "1032 [D loss: 1.002977] [G loss: 1.000378]\n",
      "1033 [D loss: 1.003021] [G loss: 1.000694]\n",
      "1034 [D loss: 1.002844] [G loss: 1.000323]\n",
      "1035 [D loss: 1.002833] [G loss: 1.000504]\n",
      "1036 [D loss: 1.003003] [G loss: 1.000235]\n",
      "1037 [D loss: 1.003082] [G loss: 1.000482]\n",
      "1038 [D loss: 1.002872] [G loss: 1.000485]\n",
      "1039 [D loss: 1.002920] [G loss: 1.000507]\n",
      "1040 [D loss: 1.002885] [G loss: 1.000491]\n",
      "1041 [D loss: 1.003013] [G loss: 1.000482]\n",
      "1042 [D loss: 1.003016] [G loss: 1.000573]\n",
      "1043 [D loss: 1.003051] [G loss: 1.000527]\n",
      "1044 [D loss: 1.002982] [G loss: 1.000376]\n",
      "1045 [D loss: 1.003005] [G loss: 1.000391]\n",
      "1046 [D loss: 1.002877] [G loss: 1.000453]\n",
      "1047 [D loss: 1.003022] [G loss: 1.000251]\n",
      "1048 [D loss: 1.003074] [G loss: 1.000310]\n",
      "1049 [D loss: 1.003021] [G loss: 1.000218]\n",
      "1050 [D loss: 1.002930] [G loss: 1.000627]\n",
      "1051 [D loss: 1.002956] [G loss: 1.000477]\n",
      "1052 [D loss: 1.002850] [G loss: 1.000603]\n",
      "1053 [D loss: 1.002967] [G loss: 1.000355]\n",
      "1054 [D loss: 1.002975] [G loss: 1.000444]\n",
      "1055 [D loss: 1.002945] [G loss: 1.000571]\n",
      "1056 [D loss: 1.002837] [G loss: 1.000446]\n",
      "1057 [D loss: 1.003056] [G loss: 1.000340]\n",
      "1058 [D loss: 1.003130] [G loss: 1.000090]\n",
      "1059 [D loss: 1.003012] [G loss: 1.000426]\n",
      "1060 [D loss: 1.003142] [G loss: 1.000377]\n",
      "1061 [D loss: 1.002787] [G loss: 1.000610]\n",
      "1062 [D loss: 1.003014] [G loss: 1.000381]\n",
      "1063 [D loss: 1.002885] [G loss: 1.000408]\n",
      "1064 [D loss: 1.002800] [G loss: 1.000596]\n",
      "1065 [D loss: 1.002783] [G loss: 1.000437]\n",
      "1066 [D loss: 1.002868] [G loss: 1.000224]\n",
      "1067 [D loss: 1.002831] [G loss: 1.000388]\n",
      "1068 [D loss: 1.002854] [G loss: 1.000148]\n",
      "1069 [D loss: 1.003163] [G loss: 1.000206]\n",
      "1070 [D loss: 1.002937] [G loss: 1.000301]\n",
      "1071 [D loss: 1.002912] [G loss: 1.000327]\n",
      "1072 [D loss: 1.003030] [G loss: 1.000425]\n",
      "1073 [D loss: 1.002872] [G loss: 1.000410]\n",
      "1074 [D loss: 1.002941] [G loss: 1.000518]\n",
      "1075 [D loss: 1.002882] [G loss: 1.000508]\n",
      "1076 [D loss: 1.002898] [G loss: 1.000512]\n",
      "1077 [D loss: 1.002912] [G loss: 1.000304]\n",
      "1078 [D loss: 1.002867] [G loss: 1.000169]\n",
      "1079 [D loss: 1.003054] [G loss: 1.000224]\n",
      "1080 [D loss: 1.003098] [G loss: 1.000444]\n",
      "1081 [D loss: 1.002921] [G loss: 1.000418]\n",
      "1082 [D loss: 1.002909] [G loss: 1.000271]\n",
      "1083 [D loss: 1.002976] [G loss: 1.000429]\n",
      "1084 [D loss: 1.002878] [G loss: 1.000364]\n",
      "1085 [D loss: 1.002899] [G loss: 1.000257]\n",
      "1086 [D loss: 1.002872] [G loss: 1.000335]\n",
      "1087 [D loss: 1.003002] [G loss: 1.000382]\n",
      "1088 [D loss: 1.002878] [G loss: 1.000336]\n",
      "1089 [D loss: 1.002880] [G loss: 1.000364]\n",
      "1090 [D loss: 1.002807] [G loss: 1.000472]\n",
      "1091 [D loss: 1.002831] [G loss: 1.000429]\n",
      "1092 [D loss: 1.002911] [G loss: 1.000292]\n",
      "1093 [D loss: 1.002713] [G loss: 1.000367]\n",
      "1094 [D loss: 1.003129] [G loss: 1.000448]\n",
      "1095 [D loss: 1.002822] [G loss: 1.000267]\n",
      "1096 [D loss: 1.002908] [G loss: 1.000443]\n",
      "1097 [D loss: 1.002844] [G loss: 1.000427]\n",
      "1098 [D loss: 1.003075] [G loss: 1.000501]\n",
      "1099 [D loss: 1.002728] [G loss: 1.000474]\n",
      "1100 [D loss: 1.002820] [G loss: 1.000586]\n",
      "1101 [D loss: 1.002952] [G loss: 1.000458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102 [D loss: 1.002889] [G loss: 1.000511]\n",
      "1103 [D loss: 1.002780] [G loss: 1.000466]\n",
      "1104 [D loss: 1.002989] [G loss: 1.000413]\n",
      "1105 [D loss: 1.002913] [G loss: 1.000354]\n",
      "1106 [D loss: 1.003054] [G loss: 1.000289]\n",
      "1107 [D loss: 1.003071] [G loss: 1.000510]\n",
      "1108 [D loss: 1.002976] [G loss: 1.000396]\n",
      "1109 [D loss: 1.002967] [G loss: 1.000164]\n",
      "1110 [D loss: 1.003066] [G loss: 1.000305]\n",
      "1111 [D loss: 1.003050] [G loss: 1.000463]\n",
      "1112 [D loss: 1.003057] [G loss: 1.000510]\n",
      "1113 [D loss: 1.003067] [G loss: 1.000456]\n",
      "1114 [D loss: 1.002868] [G loss: 1.000333]\n",
      "1115 [D loss: 1.003100] [G loss: 1.000393]\n",
      "1116 [D loss: 1.003033] [G loss: 1.000321]\n",
      "1117 [D loss: 1.003014] [G loss: 1.000354]\n",
      "1118 [D loss: 1.003066] [G loss: 1.000374]\n",
      "1119 [D loss: 1.003038] [G loss: 1.000454]\n",
      "1120 [D loss: 1.002877] [G loss: 1.000490]\n",
      "1121 [D loss: 1.003003] [G loss: 1.000550]\n",
      "1122 [D loss: 1.002864] [G loss: 1.000485]\n",
      "1123 [D loss: 1.002824] [G loss: 1.000557]\n",
      "1124 [D loss: 1.002859] [G loss: 1.000299]\n",
      "1125 [D loss: 1.002716] [G loss: 1.000699]\n",
      "1126 [D loss: 1.002786] [G loss: 1.000607]\n",
      "1127 [D loss: 1.002818] [G loss: 1.000638]\n",
      "1128 [D loss: 1.002991] [G loss: 1.000546]\n",
      "1129 [D loss: 1.002812] [G loss: 1.000690]\n",
      "1130 [D loss: 1.002871] [G loss: 1.000545]\n",
      "1131 [D loss: 1.002726] [G loss: 1.000515]\n",
      "1132 [D loss: 1.002915] [G loss: 1.000577]\n",
      "1133 [D loss: 1.002899] [G loss: 1.000521]\n",
      "1134 [D loss: 1.002937] [G loss: 1.000534]\n",
      "1135 [D loss: 1.002826] [G loss: 1.000724]\n",
      "1136 [D loss: 1.003002] [G loss: 1.000447]\n",
      "1137 [D loss: 1.002913] [G loss: 1.000753]\n",
      "1138 [D loss: 1.002971] [G loss: 1.000485]\n",
      "1139 [D loss: 1.002866] [G loss: 1.000420]\n",
      "1140 [D loss: 1.002941] [G loss: 1.000414]\n",
      "1141 [D loss: 1.003123] [G loss: 1.000678]\n",
      "1142 [D loss: 1.002829] [G loss: 1.000437]\n",
      "1143 [D loss: 1.002899] [G loss: 1.000680]\n",
      "1144 [D loss: 1.002997] [G loss: 1.000522]\n",
      "1145 [D loss: 1.003026] [G loss: 1.000565]\n",
      "1146 [D loss: 1.002758] [G loss: 1.000336]\n",
      "1147 [D loss: 1.002825] [G loss: 1.000430]\n",
      "1148 [D loss: 1.002920] [G loss: 1.000519]\n",
      "1149 [D loss: 1.002940] [G loss: 1.000579]\n",
      "1150 [D loss: 1.003046] [G loss: 1.000494]\n",
      "1151 [D loss: 1.002769] [G loss: 1.000320]\n",
      "1152 [D loss: 1.002808] [G loss: 1.000474]\n",
      "1153 [D loss: 1.002884] [G loss: 1.000709]\n",
      "1154 [D loss: 1.003015] [G loss: 1.000493]\n",
      "1155 [D loss: 1.002955] [G loss: 1.000719]\n",
      "1156 [D loss: 1.002975] [G loss: 1.000612]\n",
      "1157 [D loss: 1.002736] [G loss: 1.000522]\n",
      "1158 [D loss: 1.002704] [G loss: 1.000718]\n",
      "1159 [D loss: 1.002856] [G loss: 1.000188]\n",
      "1160 [D loss: 1.002771] [G loss: 1.000366]\n",
      "1161 [D loss: 1.003087] [G loss: 1.000616]\n",
      "1162 [D loss: 1.003156] [G loss: 1.000548]\n",
      "1163 [D loss: 1.002811] [G loss: 1.000563]\n",
      "1164 [D loss: 1.002814] [G loss: 1.000625]\n",
      "1165 [D loss: 1.002874] [G loss: 1.000491]\n",
      "1166 [D loss: 1.003020] [G loss: 1.000443]\n",
      "1167 [D loss: 1.002929] [G loss: 1.000652]\n",
      "1168 [D loss: 1.002830] [G loss: 1.000629]\n",
      "1169 [D loss: 1.002926] [G loss: 1.000380]\n",
      "1170 [D loss: 1.003025] [G loss: 1.000547]\n",
      "1171 [D loss: 1.003053] [G loss: 1.000440]\n",
      "1172 [D loss: 1.002920] [G loss: 1.000539]\n",
      "1173 [D loss: 1.002946] [G loss: 1.000477]\n",
      "1174 [D loss: 1.002735] [G loss: 1.000538]\n",
      "1175 [D loss: 1.002762] [G loss: 1.000459]\n",
      "1176 [D loss: 1.002891] [G loss: 1.000467]\n",
      "1177 [D loss: 1.002976] [G loss: 1.000602]\n",
      "1178 [D loss: 1.002868] [G loss: 1.000417]\n",
      "1179 [D loss: 1.002975] [G loss: 1.000590]\n",
      "1180 [D loss: 1.003145] [G loss: 1.000663]\n",
      "1181 [D loss: 1.003111] [G loss: 1.000698]\n",
      "1182 [D loss: 1.002898] [G loss: 1.000735]\n",
      "1183 [D loss: 1.002992] [G loss: 1.000502]\n",
      "1184 [D loss: 1.003102] [G loss: 1.000754]\n",
      "1185 [D loss: 1.002707] [G loss: 1.000231]\n",
      "1186 [D loss: 1.002689] [G loss: 1.000577]\n",
      "1187 [D loss: 1.002951] [G loss: 1.000555]\n",
      "1188 [D loss: 1.002845] [G loss: 1.000708]\n",
      "1189 [D loss: 1.002828] [G loss: 1.000434]\n",
      "1190 [D loss: 1.002754] [G loss: 1.000762]\n",
      "1191 [D loss: 1.003090] [G loss: 1.000456]\n",
      "1192 [D loss: 1.002902] [G loss: 1.000763]\n",
      "1193 [D loss: 1.003022] [G loss: 1.000333]\n",
      "1194 [D loss: 1.002791] [G loss: 1.000588]\n",
      "1195 [D loss: 1.003017] [G loss: 1.000568]\n",
      "1196 [D loss: 1.002651] [G loss: 1.000468]\n",
      "1197 [D loss: 1.002934] [G loss: 1.000493]\n",
      "1198 [D loss: 1.002911] [G loss: 1.000465]\n",
      "1199 [D loss: 1.002922] [G loss: 1.000372]\n",
      "1200 [D loss: 1.003104] [G loss: 1.000620]\n",
      "1201 [D loss: 1.002898] [G loss: 1.000761]\n",
      "1202 [D loss: 1.002913] [G loss: 1.000677]\n",
      "1203 [D loss: 1.002856] [G loss: 1.000662]\n",
      "1204 [D loss: 1.002915] [G loss: 1.000621]\n",
      "1205 [D loss: 1.002912] [G loss: 1.000564]\n",
      "1206 [D loss: 1.003079] [G loss: 1.000499]\n",
      "1207 [D loss: 1.002951] [G loss: 1.000552]\n",
      "1208 [D loss: 1.002854] [G loss: 1.000541]\n",
      "1209 [D loss: 1.002892] [G loss: 1.000405]\n",
      "1210 [D loss: 1.003096] [G loss: 1.000608]\n",
      "1211 [D loss: 1.002881] [G loss: 1.000607]\n",
      "1212 [D loss: 1.003081] [G loss: 1.000323]\n",
      "1213 [D loss: 1.002825] [G loss: 1.000365]\n",
      "1214 [D loss: 1.002949] [G loss: 1.000380]\n",
      "1215 [D loss: 1.002838] [G loss: 1.000471]\n",
      "1216 [D loss: 1.002798] [G loss: 1.000405]\n",
      "1217 [D loss: 1.003091] [G loss: 1.000415]\n",
      "1218 [D loss: 1.003004] [G loss: 1.000330]\n",
      "1219 [D loss: 1.002775] [G loss: 1.000349]\n",
      "1220 [D loss: 1.002796] [G loss: 1.000183]\n",
      "1221 [D loss: 1.003057] [G loss: 1.000452]\n",
      "1222 [D loss: 1.002934] [G loss: 1.000515]\n",
      "1223 [D loss: 1.003005] [G loss: 1.000467]\n",
      "1224 [D loss: 1.002979] [G loss: 1.000359]\n",
      "1225 [D loss: 1.002940] [G loss: 1.000293]\n",
      "1226 [D loss: 1.002879] [G loss: 1.000459]\n",
      "1227 [D loss: 1.002795] [G loss: 1.000262]\n",
      "1228 [D loss: 1.002926] [G loss: 1.000387]\n",
      "1229 [D loss: 1.002949] [G loss: 1.000561]\n",
      "1230 [D loss: 1.003012] [G loss: 1.000647]\n",
      "1231 [D loss: 1.002758] [G loss: 1.000689]\n",
      "1232 [D loss: 1.003030] [G loss: 1.000337]\n",
      "1233 [D loss: 1.002824] [G loss: 1.000558]\n",
      "1234 [D loss: 1.002915] [G loss: 1.000685]\n",
      "1235 [D loss: 1.002890] [G loss: 1.000315]\n",
      "1236 [D loss: 1.002862] [G loss: 1.000617]\n",
      "1237 [D loss: 1.002887] [G loss: 1.000527]\n",
      "1238 [D loss: 1.002899] [G loss: 1.000599]\n",
      "1239 [D loss: 1.002802] [G loss: 1.000458]\n",
      "1240 [D loss: 1.002860] [G loss: 1.000519]\n",
      "1241 [D loss: 1.002770] [G loss: 1.000419]\n",
      "1242 [D loss: 1.002885] [G loss: 1.000571]\n",
      "1243 [D loss: 1.002739] [G loss: 1.000492]\n",
      "1244 [D loss: 1.002966] [G loss: 1.000557]\n",
      "1245 [D loss: 1.002907] [G loss: 1.000652]\n",
      "1246 [D loss: 1.002940] [G loss: 1.000445]\n",
      "1247 [D loss: 1.002846] [G loss: 1.000441]\n",
      "1248 [D loss: 1.002907] [G loss: 1.000376]\n",
      "1249 [D loss: 1.002954] [G loss: 1.000487]\n",
      "1250 [D loss: 1.002723] [G loss: 1.000423]\n",
      "1251 [D loss: 1.002920] [G loss: 1.000738]\n",
      "1252 [D loss: 1.002862] [G loss: 1.000559]\n",
      "1253 [D loss: 1.003048] [G loss: 1.000474]\n",
      "1254 [D loss: 1.002863] [G loss: 1.000538]\n",
      "1255 [D loss: 1.002735] [G loss: 1.000491]\n",
      "1256 [D loss: 1.002945] [G loss: 1.000499]\n",
      "1257 [D loss: 1.002888] [G loss: 1.000538]\n",
      "1258 [D loss: 1.002813] [G loss: 1.000520]\n",
      "1259 [D loss: 1.003087] [G loss: 1.000406]\n",
      "1260 [D loss: 1.002776] [G loss: 1.000457]\n",
      "1261 [D loss: 1.002837] [G loss: 1.000339]\n",
      "1262 [D loss: 1.002939] [G loss: 1.000658]\n",
      "1263 [D loss: 1.002949] [G loss: 1.000542]\n",
      "1264 [D loss: 1.002830] [G loss: 1.000694]\n",
      "1265 [D loss: 1.002842] [G loss: 1.000304]\n",
      "1266 [D loss: 1.002892] [G loss: 1.000440]\n",
      "1267 [D loss: 1.002709] [G loss: 1.000637]\n",
      "1268 [D loss: 1.002751] [G loss: 1.000243]\n",
      "1269 [D loss: 1.002874] [G loss: 1.000707]\n",
      "1270 [D loss: 1.002818] [G loss: 1.000513]\n",
      "1271 [D loss: 1.002915] [G loss: 1.000226]\n",
      "1272 [D loss: 1.002987] [G loss: 1.000587]\n",
      "1273 [D loss: 1.002925] [G loss: 1.000409]\n",
      "1274 [D loss: 1.002933] [G loss: 1.000744]\n",
      "1275 [D loss: 1.002856] [G loss: 1.000452]\n",
      "1276 [D loss: 1.002901] [G loss: 1.000522]\n",
      "1277 [D loss: 1.002686] [G loss: 1.000675]\n",
      "1278 [D loss: 1.003033] [G loss: 1.000366]\n",
      "1279 [D loss: 1.002779] [G loss: 1.000464]\n",
      "1280 [D loss: 1.002846] [G loss: 1.000680]\n",
      "1281 [D loss: 1.002925] [G loss: 1.000532]\n",
      "1282 [D loss: 1.002952] [G loss: 1.000665]\n",
      "1283 [D loss: 1.002787] [G loss: 1.000345]\n",
      "1284 [D loss: 1.002717] [G loss: 1.000534]\n",
      "1285 [D loss: 1.002898] [G loss: 1.000375]\n",
      "1286 [D loss: 1.002935] [G loss: 1.000707]\n",
      "1287 [D loss: 1.002944] [G loss: 1.000431]\n",
      "1288 [D loss: 1.002983] [G loss: 1.000653]\n",
      "1289 [D loss: 1.002890] [G loss: 1.000463]\n",
      "1290 [D loss: 1.002792] [G loss: 1.000307]\n",
      "1291 [D loss: 1.002913] [G loss: 1.000551]\n",
      "1292 [D loss: 1.002961] [G loss: 1.000486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1293 [D loss: 1.003034] [G loss: 1.000516]\n",
      "1294 [D loss: 1.002948] [G loss: 1.000555]\n",
      "1295 [D loss: 1.002936] [G loss: 1.000478]\n",
      "1296 [D loss: 1.002862] [G loss: 1.000208]\n",
      "1297 [D loss: 1.002937] [G loss: 1.000493]\n",
      "1298 [D loss: 1.002709] [G loss: 1.000655]\n",
      "1299 [D loss: 1.003015] [G loss: 1.000367]\n",
      "1300 [D loss: 1.002784] [G loss: 1.000532]\n",
      "1301 [D loss: 1.002984] [G loss: 1.000379]\n",
      "1302 [D loss: 1.002925] [G loss: 1.000352]\n",
      "1303 [D loss: 1.003041] [G loss: 1.000328]\n",
      "1304 [D loss: 1.002737] [G loss: 1.000392]\n",
      "1305 [D loss: 1.003082] [G loss: 1.000305]\n",
      "1306 [D loss: 1.002911] [G loss: 1.000527]\n",
      "1307 [D loss: 1.002902] [G loss: 1.000794]\n",
      "1308 [D loss: 1.002868] [G loss: 1.000539]\n",
      "1309 [D loss: 1.002919] [G loss: 1.000265]\n",
      "1310 [D loss: 1.002880] [G loss: 1.000893]\n",
      "1311 [D loss: 1.002874] [G loss: 1.000546]\n",
      "1312 [D loss: 1.003043] [G loss: 1.000412]\n",
      "1313 [D loss: 1.002856] [G loss: 1.000322]\n",
      "1314 [D loss: 1.003045] [G loss: 1.000337]\n",
      "1315 [D loss: 1.003045] [G loss: 1.000486]\n",
      "1316 [D loss: 1.002890] [G loss: 1.000593]\n",
      "1317 [D loss: 1.002825] [G loss: 1.000341]\n",
      "1318 [D loss: 1.003013] [G loss: 1.000403]\n",
      "1319 [D loss: 1.002941] [G loss: 1.000456]\n",
      "1320 [D loss: 1.003060] [G loss: 1.000281]\n",
      "1321 [D loss: 1.003208] [G loss: 1.000303]\n",
      "1322 [D loss: 1.002961] [G loss: 1.000358]\n",
      "1323 [D loss: 1.003102] [G loss: 1.000637]\n",
      "1324 [D loss: 1.002810] [G loss: 1.000568]\n",
      "1325 [D loss: 1.002708] [G loss: 1.000388]\n",
      "1326 [D loss: 1.002748] [G loss: 1.000431]\n",
      "1327 [D loss: 1.002886] [G loss: 1.000334]\n",
      "1328 [D loss: 1.002669] [G loss: 1.000648]\n",
      "1329 [D loss: 1.003023] [G loss: 1.000688]\n",
      "1330 [D loss: 1.002998] [G loss: 1.000562]\n",
      "1331 [D loss: 1.002863] [G loss: 1.000791]\n",
      "1332 [D loss: 1.002991] [G loss: 1.000451]\n",
      "1333 [D loss: 1.003118] [G loss: 1.000664]\n",
      "1334 [D loss: 1.002967] [G loss: 1.000425]\n",
      "1335 [D loss: 1.002984] [G loss: 1.000470]\n",
      "1336 [D loss: 1.002877] [G loss: 1.000480]\n",
      "1337 [D loss: 1.003032] [G loss: 1.000362]\n",
      "1338 [D loss: 1.002990] [G loss: 1.000614]\n",
      "1339 [D loss: 1.002965] [G loss: 1.000740]\n",
      "1340 [D loss: 1.002860] [G loss: 1.000436]\n",
      "1341 [D loss: 1.002961] [G loss: 1.000370]\n",
      "1342 [D loss: 1.002971] [G loss: 1.000518]\n",
      "1343 [D loss: 1.002868] [G loss: 1.000582]\n",
      "1344 [D loss: 1.002880] [G loss: 1.000564]\n",
      "1345 [D loss: 1.002889] [G loss: 1.000420]\n",
      "1346 [D loss: 1.002982] [G loss: 1.000744]\n",
      "1347 [D loss: 1.002959] [G loss: 1.000647]\n",
      "1348 [D loss: 1.002848] [G loss: 1.000506]\n",
      "1349 [D loss: 1.002940] [G loss: 1.000578]\n",
      "1350 [D loss: 1.003187] [G loss: 1.000301]\n",
      "1351 [D loss: 1.003035] [G loss: 1.000435]\n",
      "1352 [D loss: 1.003186] [G loss: 1.000345]\n",
      "1353 [D loss: 1.002940] [G loss: 1.000319]\n",
      "1354 [D loss: 1.003175] [G loss: 1.000560]\n",
      "1355 [D loss: 1.003033] [G loss: 1.000511]\n",
      "1356 [D loss: 1.003101] [G loss: 1.000550]\n",
      "1357 [D loss: 1.002995] [G loss: 1.000535]\n",
      "1358 [D loss: 1.002997] [G loss: 1.000209]\n",
      "1359 [D loss: 1.003037] [G loss: 1.000310]\n",
      "1360 [D loss: 1.003162] [G loss: 1.000296]\n",
      "1361 [D loss: 1.003004] [G loss: 1.000567]\n",
      "1362 [D loss: 1.002901] [G loss: 1.000442]\n",
      "1363 [D loss: 1.002887] [G loss: 1.000736]\n",
      "1364 [D loss: 1.002917] [G loss: 1.000423]\n",
      "1365 [D loss: 1.002929] [G loss: 1.000517]\n",
      "1366 [D loss: 1.002742] [G loss: 1.000212]\n",
      "1367 [D loss: 1.002955] [G loss: 1.000206]\n",
      "1368 [D loss: 1.002892] [G loss: 1.000135]\n",
      "1369 [D loss: 1.002859] [G loss: 1.000693]\n",
      "1370 [D loss: 1.002835] [G loss: 1.000443]\n",
      "1371 [D loss: 1.002863] [G loss: 1.000549]\n",
      "1372 [D loss: 1.002828] [G loss: 1.000285]\n",
      "1373 [D loss: 1.002883] [G loss: 1.000315]\n",
      "1374 [D loss: 1.002973] [G loss: 1.000310]\n",
      "1375 [D loss: 1.002897] [G loss: 1.000303]\n",
      "1376 [D loss: 1.002795] [G loss: 1.000552]\n",
      "1377 [D loss: 1.003078] [G loss: 1.000512]\n",
      "1378 [D loss: 1.003084] [G loss: 1.000403]\n",
      "1379 [D loss: 1.003096] [G loss: 1.000315]\n",
      "1380 [D loss: 1.002943] [G loss: 1.000319]\n",
      "1381 [D loss: 1.002884] [G loss: 1.000263]\n",
      "1382 [D loss: 1.003089] [G loss: 1.000442]\n",
      "1383 [D loss: 1.003043] [G loss: 1.000570]\n",
      "1384 [D loss: 1.002827] [G loss: 1.000464]\n",
      "1385 [D loss: 1.003069] [G loss: 1.000549]\n",
      "1386 [D loss: 1.002835] [G loss: 1.000326]\n",
      "1387 [D loss: 1.002951] [G loss: 1.000586]\n",
      "1388 [D loss: 1.003048] [G loss: 1.000615]\n",
      "1389 [D loss: 1.002947] [G loss: 1.000633]\n",
      "1390 [D loss: 1.003075] [G loss: 1.000416]\n",
      "1391 [D loss: 1.002939] [G loss: 1.000431]\n",
      "1392 [D loss: 1.003001] [G loss: 1.000587]\n",
      "1393 [D loss: 1.002937] [G loss: 1.000655]\n",
      "1394 [D loss: 1.003062] [G loss: 1.000602]\n",
      "1395 [D loss: 1.002772] [G loss: 1.000659]\n",
      "1396 [D loss: 1.002922] [G loss: 1.000579]\n",
      "1397 [D loss: 1.002874] [G loss: 1.000475]\n",
      "1398 [D loss: 1.002951] [G loss: 1.000562]\n",
      "1399 [D loss: 1.002785] [G loss: 1.000633]\n",
      "1400 [D loss: 1.003079] [G loss: 1.000336]\n",
      "1401 [D loss: 1.002894] [G loss: 1.000180]\n",
      "1402 [D loss: 1.002961] [G loss: 1.000471]\n",
      "1403 [D loss: 1.003133] [G loss: 1.000353]\n",
      "1404 [D loss: 1.002958] [G loss: 1.000520]\n",
      "1405 [D loss: 1.003069] [G loss: 1.000326]\n",
      "1406 [D loss: 1.002928] [G loss: 1.000389]\n",
      "1407 [D loss: 1.002824] [G loss: 1.000496]\n",
      "1408 [D loss: 1.002881] [G loss: 1.000363]\n",
      "1409 [D loss: 1.003031] [G loss: 1.000470]\n",
      "1410 [D loss: 1.002833] [G loss: 1.000412]\n",
      "1411 [D loss: 1.002766] [G loss: 1.000535]\n",
      "1412 [D loss: 1.002952] [G loss: 1.000215]\n",
      "1413 [D loss: 1.002801] [G loss: 1.000328]\n",
      "1414 [D loss: 1.002925] [G loss: 1.000429]\n",
      "1415 [D loss: 1.003117] [G loss: 1.000563]\n",
      "1416 [D loss: 1.002670] [G loss: 1.000483]\n",
      "1417 [D loss: 1.002992] [G loss: 1.000176]\n",
      "1418 [D loss: 1.002978] [G loss: 1.000333]\n",
      "1419 [D loss: 1.003007] [G loss: 1.000768]\n",
      "1420 [D loss: 1.002855] [G loss: 1.000581]\n",
      "1421 [D loss: 1.002944] [G loss: 1.000504]\n",
      "1422 [D loss: 1.003125] [G loss: 1.000520]\n",
      "1423 [D loss: 1.002687] [G loss: 1.000598]\n",
      "1424 [D loss: 1.003005] [G loss: 1.000368]\n",
      "1425 [D loss: 1.002878] [G loss: 1.000281]\n",
      "1426 [D loss: 1.002882] [G loss: 1.000169]\n",
      "1427 [D loss: 1.002847] [G loss: 1.000351]\n",
      "1428 [D loss: 1.003018] [G loss: 1.000481]\n",
      "1429 [D loss: 1.003015] [G loss: 1.000462]\n",
      "1430 [D loss: 1.003004] [G loss: 1.000358]\n",
      "1431 [D loss: 1.002906] [G loss: 1.000589]\n",
      "1432 [D loss: 1.003006] [G loss: 1.000329]\n",
      "1433 [D loss: 1.003036] [G loss: 1.000496]\n",
      "1434 [D loss: 1.002844] [G loss: 1.000133]\n",
      "1435 [D loss: 1.003017] [G loss: 1.000549]\n",
      "1436 [D loss: 1.002903] [G loss: 1.000434]\n",
      "1437 [D loss: 1.002819] [G loss: 1.000437]\n",
      "1438 [D loss: 1.002900] [G loss: 1.000300]\n",
      "1439 [D loss: 1.002877] [G loss: 1.000487]\n",
      "1440 [D loss: 1.002866] [G loss: 1.000718]\n",
      "1441 [D loss: 1.002935] [G loss: 1.000484]\n",
      "1442 [D loss: 1.002900] [G loss: 1.000497]\n",
      "1443 [D loss: 1.002958] [G loss: 1.000671]\n",
      "1444 [D loss: 1.002926] [G loss: 1.000607]\n",
      "1445 [D loss: 1.002772] [G loss: 1.000283]\n",
      "1446 [D loss: 1.002709] [G loss: 1.000378]\n",
      "1447 [D loss: 1.002933] [G loss: 1.000621]\n",
      "1448 [D loss: 1.002732] [G loss: 1.000540]\n",
      "1449 [D loss: 1.002864] [G loss: 1.000381]\n",
      "1450 [D loss: 1.003171] [G loss: 1.000640]\n",
      "1451 [D loss: 1.002856] [G loss: 1.000418]\n",
      "1452 [D loss: 1.002963] [G loss: 1.000530]\n",
      "1453 [D loss: 1.002975] [G loss: 1.000702]\n",
      "1454 [D loss: 1.002987] [G loss: 1.000359]\n",
      "1455 [D loss: 1.002947] [G loss: 1.000668]\n",
      "1456 [D loss: 1.002935] [G loss: 1.000510]\n",
      "1457 [D loss: 1.002949] [G loss: 1.000623]\n",
      "1458 [D loss: 1.002950] [G loss: 1.000564]\n",
      "1459 [D loss: 1.002916] [G loss: 1.000475]\n",
      "1460 [D loss: 1.003005] [G loss: 1.000717]\n",
      "1461 [D loss: 1.002888] [G loss: 1.000336]\n",
      "1462 [D loss: 1.002928] [G loss: 1.000528]\n",
      "1463 [D loss: 1.002756] [G loss: 1.000562]\n",
      "1464 [D loss: 1.003044] [G loss: 1.000243]\n",
      "1465 [D loss: 1.002930] [G loss: 1.000460]\n",
      "1466 [D loss: 1.002918] [G loss: 1.000272]\n",
      "1467 [D loss: 1.002816] [G loss: 1.000607]\n",
      "1468 [D loss: 1.002903] [G loss: 1.000444]\n",
      "1469 [D loss: 1.002958] [G loss: 1.000496]\n",
      "1470 [D loss: 1.003044] [G loss: 1.000424]\n",
      "1471 [D loss: 1.002902] [G loss: 1.000466]\n",
      "1472 [D loss: 1.002961] [G loss: 1.000397]\n",
      "1473 [D loss: 1.002696] [G loss: 1.000450]\n",
      "1474 [D loss: 1.003085] [G loss: 1.000674]\n",
      "1475 [D loss: 1.003029] [G loss: 1.000596]\n",
      "1476 [D loss: 1.002908] [G loss: 1.000475]\n",
      "1477 [D loss: 1.002930] [G loss: 1.000590]\n",
      "1478 [D loss: 1.002990] [G loss: 1.000599]\n",
      "1479 [D loss: 1.003010] [G loss: 1.000514]\n",
      "1480 [D loss: 1.002916] [G loss: 1.000311]\n",
      "1481 [D loss: 1.003024] [G loss: 1.000404]\n",
      "1482 [D loss: 1.002959] [G loss: 1.000205]\n",
      "1483 [D loss: 1.002986] [G loss: 1.000267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484 [D loss: 1.003045] [G loss: 1.000204]\n",
      "1485 [D loss: 1.002920] [G loss: 1.000694]\n",
      "1486 [D loss: 1.002880] [G loss: 1.000586]\n",
      "1487 [D loss: 1.002863] [G loss: 1.000654]\n",
      "1488 [D loss: 1.002882] [G loss: 1.000485]\n",
      "1489 [D loss: 1.003005] [G loss: 1.000535]\n",
      "1490 [D loss: 1.003133] [G loss: 1.000380]\n",
      "1491 [D loss: 1.003188] [G loss: 1.000157]\n",
      "1492 [D loss: 1.003036] [G loss: 1.000396]\n",
      "1493 [D loss: 1.003162] [G loss: 1.000717]\n",
      "1494 [D loss: 1.003016] [G loss: 1.000339]\n",
      "1495 [D loss: 1.002684] [G loss: 1.000463]\n",
      "1496 [D loss: 1.003001] [G loss: 1.000418]\n",
      "1497 [D loss: 1.002846] [G loss: 1.000183]\n",
      "1498 [D loss: 1.002841] [G loss: 1.000449]\n",
      "1499 [D loss: 1.002940] [G loss: 1.000236]\n",
      "1500 [D loss: 1.002950] [G loss: 1.000453]\n",
      "1501 [D loss: 1.002970] [G loss: 1.000600]\n",
      "1502 [D loss: 1.002786] [G loss: 1.000421]\n",
      "1503 [D loss: 1.002969] [G loss: 1.000358]\n",
      "1504 [D loss: 1.002787] [G loss: 1.000496]\n",
      "1505 [D loss: 1.002923] [G loss: 1.000359]\n",
      "1506 [D loss: 1.002899] [G loss: 1.000183]\n",
      "1507 [D loss: 1.003003] [G loss: 1.000806]\n",
      "1508 [D loss: 1.002779] [G loss: 1.000448]\n",
      "1509 [D loss: 1.003101] [G loss: 1.000628]\n",
      "1510 [D loss: 1.002837] [G loss: 1.000514]\n",
      "1511 [D loss: 1.002932] [G loss: 1.000103]\n",
      "1512 [D loss: 1.002897] [G loss: 1.000656]\n",
      "1513 [D loss: 1.002966] [G loss: 1.000345]\n",
      "1514 [D loss: 1.003012] [G loss: 1.000426]\n",
      "1515 [D loss: 1.002943] [G loss: 1.000369]\n",
      "1516 [D loss: 1.002917] [G loss: 1.000579]\n",
      "1517 [D loss: 1.003002] [G loss: 1.000455]\n",
      "1518 [D loss: 1.002930] [G loss: 1.000401]\n",
      "1519 [D loss: 1.003038] [G loss: 1.000423]\n",
      "1520 [D loss: 1.002976] [G loss: 1.000312]\n",
      "1521 [D loss: 1.002876] [G loss: 1.000203]\n",
      "1522 [D loss: 1.003056] [G loss: 1.000162]\n",
      "1523 [D loss: 1.003058] [G loss: 1.000505]\n",
      "1524 [D loss: 1.002875] [G loss: 1.000462]\n",
      "1525 [D loss: 1.002761] [G loss: 1.000536]\n",
      "1526 [D loss: 1.002804] [G loss: 1.000318]\n",
      "1527 [D loss: 1.002940] [G loss: 1.000445]\n",
      "1528 [D loss: 1.002799] [G loss: 1.000332]\n",
      "1529 [D loss: 1.002905] [G loss: 1.000791]\n",
      "1530 [D loss: 1.002869] [G loss: 1.000620]\n",
      "1531 [D loss: 1.002936] [G loss: 1.000378]\n",
      "1532 [D loss: 1.002712] [G loss: 1.000416]\n",
      "1533 [D loss: 1.002955] [G loss: 1.000439]\n",
      "1534 [D loss: 1.003012] [G loss: 1.000419]\n",
      "1535 [D loss: 1.003069] [G loss: 1.000543]\n",
      "1536 [D loss: 1.002841] [G loss: 1.000331]\n",
      "1537 [D loss: 1.002669] [G loss: 1.000510]\n",
      "1538 [D loss: 1.003003] [G loss: 1.000268]\n",
      "1539 [D loss: 1.003021] [G loss: 1.000714]\n",
      "1540 [D loss: 1.002952] [G loss: 1.000071]\n",
      "1541 [D loss: 1.002882] [G loss: 1.000440]\n",
      "1542 [D loss: 1.002893] [G loss: 1.000377]\n",
      "1543 [D loss: 1.003047] [G loss: 1.000547]\n",
      "1544 [D loss: 1.003081] [G loss: 1.000599]\n",
      "1545 [D loss: 1.002909] [G loss: 1.000488]\n",
      "1546 [D loss: 1.003017] [G loss: 1.000378]\n",
      "1547 [D loss: 1.002796] [G loss: 1.000545]\n",
      "1548 [D loss: 1.002796] [G loss: 1.000288]\n",
      "1549 [D loss: 1.002779] [G loss: 1.000507]\n",
      "1550 [D loss: 1.002924] [G loss: 1.000434]\n",
      "1551 [D loss: 1.002964] [G loss: 1.000601]\n",
      "1552 [D loss: 1.003114] [G loss: 1.000556]\n",
      "1553 [D loss: 1.002984] [G loss: 1.000213]\n",
      "1554 [D loss: 1.002881] [G loss: 1.000331]\n",
      "1555 [D loss: 1.002908] [G loss: 1.000650]\n",
      "1556 [D loss: 1.002828] [G loss: 1.000512]\n",
      "1557 [D loss: 1.002751] [G loss: 1.000377]\n",
      "1558 [D loss: 1.002961] [G loss: 1.000595]\n",
      "1559 [D loss: 1.002898] [G loss: 1.000523]\n",
      "1560 [D loss: 1.002985] [G loss: 1.000491]\n",
      "1561 [D loss: 1.002865] [G loss: 1.000507]\n",
      "1562 [D loss: 1.002850] [G loss: 1.000410]\n",
      "1563 [D loss: 1.002926] [G loss: 1.000533]\n",
      "1564 [D loss: 1.003010] [G loss: 1.000479]\n",
      "1565 [D loss: 1.003035] [G loss: 1.000416]\n",
      "1566 [D loss: 1.002894] [G loss: 1.000447]\n",
      "1567 [D loss: 1.002997] [G loss: 1.000526]\n",
      "1568 [D loss: 1.002856] [G loss: 1.000306]\n",
      "1569 [D loss: 1.002860] [G loss: 1.000258]\n",
      "1570 [D loss: 1.002924] [G loss: 1.000128]\n",
      "1571 [D loss: 1.002764] [G loss: 1.000430]\n",
      "1572 [D loss: 1.003159] [G loss: 1.000546]\n",
      "1573 [D loss: 1.002917] [G loss: 1.000457]\n",
      "1574 [D loss: 1.002879] [G loss: 1.000475]\n",
      "1575 [D loss: 1.002760] [G loss: 1.000608]\n",
      "1576 [D loss: 1.002840] [G loss: 1.000423]\n",
      "1577 [D loss: 1.002933] [G loss: 1.000394]\n",
      "1578 [D loss: 1.002859] [G loss: 1.000380]\n",
      "1579 [D loss: 1.003092] [G loss: 1.000512]\n",
      "1580 [D loss: 1.002786] [G loss: 1.000494]\n",
      "1581 [D loss: 1.002963] [G loss: 1.000620]\n",
      "1582 [D loss: 1.003045] [G loss: 1.000240]\n",
      "1583 [D loss: 1.002914] [G loss: 1.000485]\n",
      "1584 [D loss: 1.002856] [G loss: 1.000416]\n",
      "1585 [D loss: 1.003061] [G loss: 1.000515]\n",
      "1586 [D loss: 1.002829] [G loss: 1.000328]\n",
      "1587 [D loss: 1.002813] [G loss: 1.000587]\n",
      "1588 [D loss: 1.003021] [G loss: 1.000528]\n",
      "1589 [D loss: 1.002891] [G loss: 1.000636]\n",
      "1590 [D loss: 1.002793] [G loss: 1.000502]\n",
      "1591 [D loss: 1.002936] [G loss: 1.000525]\n",
      "1592 [D loss: 1.002800] [G loss: 1.000495]\n",
      "1593 [D loss: 1.002777] [G loss: 1.000348]\n",
      "1594 [D loss: 1.002968] [G loss: 1.000566]\n",
      "1595 [D loss: 1.003147] [G loss: 1.000563]\n",
      "1596 [D loss: 1.003000] [G loss: 1.000430]\n",
      "1597 [D loss: 1.002978] [G loss: 1.000217]\n",
      "1598 [D loss: 1.002911] [G loss: 1.000428]\n",
      "1599 [D loss: 1.002991] [G loss: 1.000400]\n",
      "1600 [D loss: 1.002978] [G loss: 1.000318]\n",
      "1601 [D loss: 1.002798] [G loss: 1.000147]\n",
      "1602 [D loss: 1.002926] [G loss: 1.000491]\n",
      "1603 [D loss: 1.002863] [G loss: 1.000459]\n",
      "1604 [D loss: 1.002897] [G loss: 1.000348]\n",
      "1605 [D loss: 1.003023] [G loss: 1.000618]\n",
      "1606 [D loss: 1.003014] [G loss: 1.000392]\n",
      "1607 [D loss: 1.002919] [G loss: 1.000297]\n",
      "1608 [D loss: 1.003018] [G loss: 1.000349]\n",
      "1609 [D loss: 1.003143] [G loss: 1.000605]\n",
      "1610 [D loss: 1.002809] [G loss: 1.000534]\n",
      "1611 [D loss: 1.002770] [G loss: 1.000623]\n",
      "1612 [D loss: 1.002838] [G loss: 1.000414]\n",
      "1613 [D loss: 1.002886] [G loss: 1.000416]\n",
      "1614 [D loss: 1.003001] [G loss: 1.000589]\n",
      "1615 [D loss: 1.003053] [G loss: 1.000474]\n",
      "1616 [D loss: 1.003105] [G loss: 1.000504]\n",
      "1617 [D loss: 1.002900] [G loss: 1.000674]\n",
      "1618 [D loss: 1.002869] [G loss: 1.000820]\n",
      "1619 [D loss: 1.003121] [G loss: 1.000686]\n",
      "1620 [D loss: 1.002880] [G loss: 1.000466]\n",
      "1621 [D loss: 1.002922] [G loss: 1.000490]\n",
      "1622 [D loss: 1.002863] [G loss: 1.000518]\n",
      "1623 [D loss: 1.002970] [G loss: 1.000660]\n",
      "1624 [D loss: 1.002824] [G loss: 1.000476]\n",
      "1625 [D loss: 1.002871] [G loss: 1.000527]\n",
      "1626 [D loss: 1.002962] [G loss: 1.000546]\n",
      "1627 [D loss: 1.002967] [G loss: 1.000276]\n",
      "1628 [D loss: 1.003054] [G loss: 1.000264]\n",
      "1629 [D loss: 1.002919] [G loss: 1.000461]\n",
      "1630 [D loss: 1.003121] [G loss: 1.000300]\n",
      "1631 [D loss: 1.002968] [G loss: 1.000552]\n",
      "1632 [D loss: 1.002921] [G loss: 1.000249]\n",
      "1633 [D loss: 1.002728] [G loss: 1.000203]\n",
      "1634 [D loss: 1.002800] [G loss: 1.000520]\n",
      "1635 [D loss: 1.002929] [G loss: 1.000413]\n",
      "1636 [D loss: 1.002896] [G loss: 1.000491]\n",
      "1637 [D loss: 1.002957] [G loss: 1.000388]\n",
      "1638 [D loss: 1.002974] [G loss: 1.000355]\n",
      "1639 [D loss: 1.003073] [G loss: 1.000276]\n",
      "1640 [D loss: 1.003119] [G loss: 1.000518]\n",
      "1641 [D loss: 1.003061] [G loss: 1.000592]\n",
      "1642 [D loss: 1.002869] [G loss: 1.000416]\n",
      "1643 [D loss: 1.002817] [G loss: 1.000462]\n",
      "1644 [D loss: 1.002829] [G loss: 1.000603]\n",
      "1645 [D loss: 1.002910] [G loss: 1.000340]\n",
      "1646 [D loss: 1.003059] [G loss: 1.000510]\n",
      "1647 [D loss: 1.002953] [G loss: 1.000118]\n",
      "1648 [D loss: 1.002859] [G loss: 1.000513]\n",
      "1649 [D loss: 1.003023] [G loss: 1.000464]\n",
      "1650 [D loss: 1.002770] [G loss: 1.000338]\n",
      "1651 [D loss: 1.003146] [G loss: 1.000631]\n",
      "1652 [D loss: 1.002872] [G loss: 1.000398]\n",
      "1653 [D loss: 1.002868] [G loss: 1.000491]\n",
      "1654 [D loss: 1.003049] [G loss: 1.000460]\n",
      "1655 [D loss: 1.003151] [G loss: 1.000572]\n",
      "1656 [D loss: 1.002975] [G loss: 1.000333]\n",
      "1657 [D loss: 1.002766] [G loss: 1.000520]\n",
      "1658 [D loss: 1.003079] [G loss: 1.000412]\n",
      "1659 [D loss: 1.002888] [G loss: 1.000223]\n",
      "1660 [D loss: 1.002945] [G loss: 1.000498]\n",
      "1661 [D loss: 1.002982] [G loss: 1.000895]\n",
      "1662 [D loss: 1.002903] [G loss: 1.000718]\n",
      "1663 [D loss: 1.002873] [G loss: 1.000496]\n",
      "1664 [D loss: 1.002739] [G loss: 1.000437]\n",
      "1665 [D loss: 1.002993] [G loss: 1.000734]\n",
      "1666 [D loss: 1.002909] [G loss: 1.000648]\n",
      "1667 [D loss: 1.002915] [G loss: 1.000540]\n",
      "1668 [D loss: 1.003141] [G loss: 1.000413]\n",
      "1669 [D loss: 1.002888] [G loss: 1.000489]\n",
      "1670 [D loss: 1.002913] [G loss: 1.000340]\n",
      "1671 [D loss: 1.002826] [G loss: 1.000566]\n",
      "1672 [D loss: 1.002944] [G loss: 1.000391]\n",
      "1673 [D loss: 1.002915] [G loss: 1.000335]\n",
      "1674 [D loss: 1.002900] [G loss: 1.000418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1675 [D loss: 1.003113] [G loss: 1.000403]\n",
      "1676 [D loss: 1.002990] [G loss: 1.000549]\n",
      "1677 [D loss: 1.002720] [G loss: 1.000509]\n",
      "1678 [D loss: 1.002951] [G loss: 1.000217]\n",
      "1679 [D loss: 1.002925] [G loss: 1.000462]\n",
      "1680 [D loss: 1.002917] [G loss: 1.000298]\n",
      "1681 [D loss: 1.003066] [G loss: 1.000545]\n",
      "1682 [D loss: 1.003037] [G loss: 1.000287]\n",
      "1683 [D loss: 1.003006] [G loss: 1.000493]\n",
      "1684 [D loss: 1.003112] [G loss: 1.000371]\n",
      "1685 [D loss: 1.002846] [G loss: 1.000394]\n",
      "1686 [D loss: 1.002787] [G loss: 1.000385]\n",
      "1687 [D loss: 1.002905] [G loss: 1.000542]\n",
      "1688 [D loss: 1.003227] [G loss: 1.000255]\n",
      "1689 [D loss: 1.002918] [G loss: 1.000461]\n",
      "1690 [D loss: 1.003017] [G loss: 1.000224]\n",
      "1691 [D loss: 1.002787] [G loss: 1.000570]\n",
      "1692 [D loss: 1.002967] [G loss: 1.000443]\n",
      "1693 [D loss: 1.003097] [G loss: 1.000571]\n",
      "1694 [D loss: 1.002981] [G loss: 1.000493]\n",
      "1695 [D loss: 1.002971] [G loss: 1.000541]\n",
      "1696 [D loss: 1.002863] [G loss: 1.000152]\n",
      "1697 [D loss: 1.002926] [G loss: 1.000418]\n",
      "1698 [D loss: 1.002912] [G loss: 1.000410]\n",
      "1699 [D loss: 1.002915] [G loss: 1.000259]\n",
      "1700 [D loss: 1.003042] [G loss: 1.000418]\n",
      "1701 [D loss: 1.003018] [G loss: 1.000283]\n",
      "1702 [D loss: 1.003074] [G loss: 1.000391]\n",
      "1703 [D loss: 1.002760] [G loss: 1.000714]\n",
      "1704 [D loss: 1.002824] [G loss: 1.000374]\n",
      "1705 [D loss: 1.002955] [G loss: 1.000353]\n",
      "1706 [D loss: 1.002933] [G loss: 1.000362]\n",
      "1707 [D loss: 1.002846] [G loss: 1.000419]\n",
      "1708 [D loss: 1.002868] [G loss: 1.000482]\n",
      "1709 [D loss: 1.002922] [G loss: 1.000476]\n",
      "1710 [D loss: 1.002882] [G loss: 1.000221]\n",
      "1711 [D loss: 1.002902] [G loss: 1.000371]\n",
      "1712 [D loss: 1.002821] [G loss: 1.000492]\n",
      "1713 [D loss: 1.002913] [G loss: 1.000377]\n",
      "1714 [D loss: 1.003100] [G loss: 1.000356]\n",
      "1715 [D loss: 1.002847] [G loss: 1.000415]\n",
      "1716 [D loss: 1.002780] [G loss: 1.000310]\n",
      "1717 [D loss: 1.002963] [G loss: 1.000579]\n",
      "1718 [D loss: 1.002884] [G loss: 1.000518]\n",
      "1719 [D loss: 1.002858] [G loss: 1.000696]\n",
      "1720 [D loss: 1.002934] [G loss: 1.000519]\n",
      "1721 [D loss: 1.002992] [G loss: 1.000274]\n",
      "1722 [D loss: 1.002915] [G loss: 1.000539]\n",
      "1723 [D loss: 1.002884] [G loss: 1.000602]\n",
      "1724 [D loss: 1.002857] [G loss: 1.000299]\n",
      "1725 [D loss: 1.002712] [G loss: 1.000409]\n",
      "1726 [D loss: 1.002859] [G loss: 1.000498]\n",
      "1727 [D loss: 1.002899] [G loss: 1.000639]\n",
      "1728 [D loss: 1.002678] [G loss: 1.000321]\n",
      "1729 [D loss: 1.003150] [G loss: 1.000616]\n",
      "1730 [D loss: 1.003067] [G loss: 1.000492]\n",
      "1731 [D loss: 1.002896] [G loss: 1.000339]\n",
      "1732 [D loss: 1.002973] [G loss: 1.000568]\n",
      "1733 [D loss: 1.002915] [G loss: 1.000340]\n",
      "1734 [D loss: 1.002680] [G loss: 1.000260]\n",
      "1735 [D loss: 1.002971] [G loss: 1.000408]\n",
      "1736 [D loss: 1.002760] [G loss: 1.000378]\n",
      "1737 [D loss: 1.002856] [G loss: 1.000433]\n",
      "1738 [D loss: 1.003049] [G loss: 1.000467]\n",
      "1739 [D loss: 1.002739] [G loss: 1.000419]\n",
      "1740 [D loss: 1.002977] [G loss: 1.000372]\n",
      "1741 [D loss: 1.002989] [G loss: 1.000528]\n",
      "1742 [D loss: 1.002883] [G loss: 1.000576]\n",
      "1743 [D loss: 1.002972] [G loss: 1.000454]\n",
      "1744 [D loss: 1.003019] [G loss: 1.000716]\n",
      "1745 [D loss: 1.002912] [G loss: 1.000464]\n",
      "1746 [D loss: 1.002854] [G loss: 1.000252]\n",
      "1747 [D loss: 1.002749] [G loss: 1.000412]\n",
      "1748 [D loss: 1.002795] [G loss: 1.000574]\n",
      "1749 [D loss: 1.003005] [G loss: 1.000379]\n",
      "1750 [D loss: 1.002949] [G loss: 1.000302]\n",
      "1751 [D loss: 1.002763] [G loss: 1.000313]\n",
      "1752 [D loss: 1.002958] [G loss: 1.000219]\n",
      "1753 [D loss: 1.002738] [G loss: 1.000235]\n",
      "1754 [D loss: 1.002900] [G loss: 1.000425]\n",
      "1755 [D loss: 1.003185] [G loss: 1.000442]\n",
      "1756 [D loss: 1.002935] [G loss: 1.000616]\n",
      "1757 [D loss: 1.002882] [G loss: 1.000444]\n",
      "1758 [D loss: 1.002883] [G loss: 1.000503]\n",
      "1759 [D loss: 1.002718] [G loss: 1.000578]\n",
      "1760 [D loss: 1.003054] [G loss: 1.000471]\n",
      "1761 [D loss: 1.003002] [G loss: 1.000476]\n",
      "1762 [D loss: 1.003052] [G loss: 1.000658]\n",
      "1763 [D loss: 1.002899] [G loss: 1.000362]\n",
      "1764 [D loss: 1.002953] [G loss: 1.000460]\n",
      "1765 [D loss: 1.002755] [G loss: 1.000665]\n",
      "1766 [D loss: 1.003099] [G loss: 1.000626]\n",
      "1767 [D loss: 1.003103] [G loss: 1.000443]\n",
      "1768 [D loss: 1.002921] [G loss: 1.000385]\n",
      "1769 [D loss: 1.002984] [G loss: 1.000458]\n",
      "1770 [D loss: 1.002749] [G loss: 1.000456]\n",
      "1771 [D loss: 1.002949] [G loss: 1.000460]\n",
      "1772 [D loss: 1.002954] [G loss: 1.000399]\n",
      "1773 [D loss: 1.002957] [G loss: 1.000430]\n",
      "1774 [D loss: 1.002910] [G loss: 1.000249]\n",
      "1775 [D loss: 1.002960] [G loss: 1.000169]\n",
      "1776 [D loss: 1.002847] [G loss: 1.000452]\n",
      "1777 [D loss: 1.002971] [G loss: 1.000312]\n",
      "1778 [D loss: 1.003087] [G loss: 1.000499]\n",
      "1779 [D loss: 1.002840] [G loss: 1.000406]\n",
      "1780 [D loss: 1.002796] [G loss: 1.000537]\n",
      "1781 [D loss: 1.002807] [G loss: 1.000550]\n",
      "1782 [D loss: 1.002869] [G loss: 1.000251]\n",
      "1783 [D loss: 1.003044] [G loss: 1.000377]\n",
      "1784 [D loss: 1.002883] [G loss: 1.000262]\n",
      "1785 [D loss: 1.002794] [G loss: 1.000521]\n",
      "1786 [D loss: 1.002803] [G loss: 1.000337]\n",
      "1787 [D loss: 1.002973] [G loss: 1.000435]\n",
      "1788 [D loss: 1.002892] [G loss: 1.000324]\n",
      "1789 [D loss: 1.002764] [G loss: 1.000394]\n",
      "1790 [D loss: 1.002773] [G loss: 1.000360]\n",
      "1791 [D loss: 1.002959] [G loss: 1.000369]\n",
      "1792 [D loss: 1.002931] [G loss: 1.000447]\n",
      "1793 [D loss: 1.003036] [G loss: 1.000293]\n",
      "1794 [D loss: 1.002927] [G loss: 1.000484]\n",
      "1795 [D loss: 1.002788] [G loss: 1.000431]\n",
      "1796 [D loss: 1.003009] [G loss: 1.000442]\n",
      "1797 [D loss: 1.002787] [G loss: 1.000523]\n",
      "1798 [D loss: 1.002957] [G loss: 1.000388]\n",
      "1799 [D loss: 1.002963] [G loss: 1.000639]\n",
      "1800 [D loss: 1.002947] [G loss: 1.000361]\n",
      "1801 [D loss: 1.002938] [G loss: 1.000259]\n",
      "1802 [D loss: 1.002991] [G loss: 1.000406]\n",
      "1803 [D loss: 1.002949] [G loss: 1.000338]\n",
      "1804 [D loss: 1.003044] [G loss: 1.000389]\n",
      "1805 [D loss: 1.002899] [G loss: 1.000535]\n",
      "1806 [D loss: 1.003051] [G loss: 1.000329]\n",
      "1807 [D loss: 1.002954] [G loss: 1.000432]\n",
      "1808 [D loss: 1.002991] [G loss: 1.000357]\n",
      "1809 [D loss: 1.002942] [G loss: 1.000365]\n",
      "1810 [D loss: 1.003084] [G loss: 1.000216]\n",
      "1811 [D loss: 1.003077] [G loss: 1.000265]\n",
      "1812 [D loss: 1.003013] [G loss: 1.000389]\n",
      "1813 [D loss: 1.003142] [G loss: 1.000494]\n",
      "1814 [D loss: 1.002959] [G loss: 1.000309]\n",
      "1815 [D loss: 1.002823] [G loss: 1.000316]\n",
      "1816 [D loss: 1.002777] [G loss: 1.000520]\n",
      "1817 [D loss: 1.003090] [G loss: 1.000171]\n",
      "1818 [D loss: 1.003044] [G loss: 1.000345]\n",
      "1819 [D loss: 1.003085] [G loss: 1.000471]\n",
      "1820 [D loss: 1.002854] [G loss: 1.000440]\n",
      "1821 [D loss: 1.002851] [G loss: 1.000337]\n",
      "1822 [D loss: 1.003091] [G loss: 1.000339]\n",
      "1823 [D loss: 1.003114] [G loss: 1.000312]\n",
      "1824 [D loss: 1.003022] [G loss: 1.000421]\n",
      "1825 [D loss: 1.003018] [G loss: 1.000414]\n",
      "1826 [D loss: 1.003032] [G loss: 1.000386]\n",
      "1827 [D loss: 1.002712] [G loss: 1.000449]\n",
      "1828 [D loss: 1.003128] [G loss: 1.000448]\n",
      "1829 [D loss: 1.002769] [G loss: 1.000413]\n",
      "1830 [D loss: 1.002750] [G loss: 1.000628]\n",
      "1831 [D loss: 1.002966] [G loss: 1.000587]\n",
      "1832 [D loss: 1.003000] [G loss: 1.000550]\n",
      "1833 [D loss: 1.002935] [G loss: 1.000579]\n",
      "1834 [D loss: 1.003021] [G loss: 1.000400]\n",
      "1835 [D loss: 1.003118] [G loss: 1.000444]\n",
      "1836 [D loss: 1.002891] [G loss: 1.000613]\n",
      "1837 [D loss: 1.002889] [G loss: 1.000301]\n",
      "1838 [D loss: 1.002890] [G loss: 1.000243]\n",
      "1839 [D loss: 1.002869] [G loss: 1.000501]\n",
      "1840 [D loss: 1.003017] [G loss: 1.000269]\n",
      "1841 [D loss: 1.002919] [G loss: 1.000379]\n",
      "1842 [D loss: 1.002930] [G loss: 1.000306]\n",
      "1843 [D loss: 1.003227] [G loss: 1.000318]\n",
      "1844 [D loss: 1.002774] [G loss: 1.000235]\n",
      "1845 [D loss: 1.002926] [G loss: 1.000234]\n",
      "1846 [D loss: 1.002869] [G loss: 1.000200]\n",
      "1847 [D loss: 1.002921] [G loss: 1.000366]\n",
      "1848 [D loss: 1.002813] [G loss: 1.000313]\n",
      "1849 [D loss: 1.003177] [G loss: 1.000315]\n",
      "1850 [D loss: 1.002982] [G loss: 1.000610]\n",
      "1851 [D loss: 1.003019] [G loss: 1.000129]\n",
      "1852 [D loss: 1.002894] [G loss: 1.000149]\n",
      "1853 [D loss: 1.002843] [G loss: 1.000147]\n",
      "1854 [D loss: 1.003040] [G loss: 1.000202]\n",
      "1855 [D loss: 1.003236] [G loss: 1.000486]\n",
      "1856 [D loss: 1.002925] [G loss: 1.000366]\n",
      "1857 [D loss: 1.003131] [G loss: 1.000236]\n",
      "1858 [D loss: 1.003131] [G loss: 1.000232]\n",
      "1859 [D loss: 1.003140] [G loss: 1.000290]\n",
      "1860 [D loss: 1.003046] [G loss: 1.000437]\n",
      "1861 [D loss: 1.002715] [G loss: 1.000367]\n",
      "1862 [D loss: 1.002821] [G loss: 1.000482]\n",
      "1863 [D loss: 1.003101] [G loss: 1.000537]\n",
      "1864 [D loss: 1.003024] [G loss: 1.000537]\n",
      "1865 [D loss: 1.002875] [G loss: 1.000383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866 [D loss: 1.003024] [G loss: 1.000338]\n",
      "1867 [D loss: 1.002842] [G loss: 1.000326]\n",
      "1868 [D loss: 1.003041] [G loss: 1.000424]\n",
      "1869 [D loss: 1.002943] [G loss: 1.000213]\n",
      "1870 [D loss: 1.002962] [G loss: 1.000428]\n",
      "1871 [D loss: 1.003120] [G loss: 1.000352]\n",
      "1872 [D loss: 1.003052] [G loss: 1.000405]\n",
      "1873 [D loss: 1.003014] [G loss: 1.000612]\n",
      "1874 [D loss: 1.002725] [G loss: 1.000547]\n",
      "1875 [D loss: 1.003024] [G loss: 1.000325]\n",
      "1876 [D loss: 1.003114] [G loss: 1.000253]\n",
      "1877 [D loss: 1.002851] [G loss: 1.000451]\n",
      "1878 [D loss: 1.002828] [G loss: 1.000252]\n",
      "1879 [D loss: 1.002924] [G loss: 1.000324]\n",
      "1880 [D loss: 1.002970] [G loss: 1.000167]\n",
      "1881 [D loss: 1.003050] [G loss: 1.000366]\n",
      "1882 [D loss: 1.003016] [G loss: 1.000370]\n",
      "1883 [D loss: 1.002999] [G loss: 1.000134]\n",
      "1884 [D loss: 1.003065] [G loss: 1.000253]\n",
      "1885 [D loss: 1.002919] [G loss: 1.000343]\n",
      "1886 [D loss: 1.002790] [G loss: 0.999948]\n",
      "1887 [D loss: 1.002882] [G loss: 1.000418]\n",
      "1888 [D loss: 1.002866] [G loss: 1.000206]\n",
      "1889 [D loss: 1.003153] [G loss: 1.000259]\n",
      "1890 [D loss: 1.002975] [G loss: 1.000265]\n",
      "1891 [D loss: 1.003084] [G loss: 1.000358]\n",
      "1892 [D loss: 1.002993] [G loss: 1.000233]\n",
      "1893 [D loss: 1.002901] [G loss: 1.000359]\n",
      "1894 [D loss: 1.003055] [G loss: 1.000217]\n",
      "1895 [D loss: 1.002934] [G loss: 1.000135]\n",
      "1896 [D loss: 1.002879] [G loss: 1.000350]\n",
      "1897 [D loss: 1.003123] [G loss: 1.000188]\n",
      "1898 [D loss: 1.003109] [G loss: 1.000466]\n",
      "1899 [D loss: 1.002966] [G loss: 1.000417]\n",
      "1900 [D loss: 1.003023] [G loss: 1.000254]\n",
      "1901 [D loss: 1.002805] [G loss: 1.000090]\n",
      "1902 [D loss: 1.003088] [G loss: 1.000099]\n",
      "1903 [D loss: 1.002944] [G loss: 1.000409]\n",
      "1904 [D loss: 1.003245] [G loss: 1.000273]\n",
      "1905 [D loss: 1.002834] [G loss: 1.000220]\n",
      "1906 [D loss: 1.003029] [G loss: 1.000172]\n",
      "1907 [D loss: 1.003063] [G loss: 1.000364]\n",
      "1908 [D loss: 1.003071] [G loss: 1.000259]\n",
      "1909 [D loss: 1.002937] [G loss: 1.000446]\n",
      "1910 [D loss: 1.002810] [G loss: 1.000350]\n",
      "1911 [D loss: 1.002837] [G loss: 1.000407]\n",
      "1912 [D loss: 1.003080] [G loss: 1.000169]\n",
      "1913 [D loss: 1.002979] [G loss: 1.000238]\n",
      "1914 [D loss: 1.002989] [G loss: 1.000376]\n",
      "1915 [D loss: 1.003231] [G loss: 1.000416]\n",
      "1916 [D loss: 1.003011] [G loss: 1.000338]\n",
      "1917 [D loss: 1.002872] [G loss: 1.000166]\n",
      "1918 [D loss: 1.002870] [G loss: 1.000195]\n",
      "1919 [D loss: 1.002980] [G loss: 1.000236]\n",
      "1920 [D loss: 1.002863] [G loss: 1.000394]\n",
      "1921 [D loss: 1.003086] [G loss: 1.000346]\n",
      "1922 [D loss: 1.002907] [G loss: 1.000359]\n",
      "1923 [D loss: 1.002846] [G loss: 1.000377]\n",
      "1924 [D loss: 1.002930] [G loss: 1.000367]\n",
      "1925 [D loss: 1.002877] [G loss: 1.000337]\n",
      "1926 [D loss: 1.003152] [G loss: 1.000270]\n",
      "1927 [D loss: 1.002737] [G loss: 1.000222]\n",
      "1928 [D loss: 1.003184] [G loss: 1.000455]\n",
      "1929 [D loss: 1.003018] [G loss: 1.000350]\n",
      "1930 [D loss: 1.003024] [G loss: 1.000326]\n",
      "1931 [D loss: 1.002802] [G loss: 1.000177]\n",
      "1932 [D loss: 1.003098] [G loss: 1.000297]\n",
      "1933 [D loss: 1.002948] [G loss: 1.000552]\n",
      "1934 [D loss: 1.003234] [G loss: 1.000409]\n",
      "1935 [D loss: 1.003069] [G loss: 1.000210]\n",
      "1936 [D loss: 1.002738] [G loss: 1.000258]\n",
      "1937 [D loss: 1.002646] [G loss: 1.000342]\n",
      "1938 [D loss: 1.002868] [G loss: 1.000585]\n",
      "1939 [D loss: 1.002966] [G loss: 1.000421]\n",
      "1940 [D loss: 1.002975] [G loss: 1.000331]\n",
      "1941 [D loss: 1.002923] [G loss: 1.000263]\n",
      "1942 [D loss: 1.003131] [G loss: 1.000629]\n",
      "1943 [D loss: 1.002871] [G loss: 1.000287]\n",
      "1944 [D loss: 1.003030] [G loss: 1.000386]\n",
      "1945 [D loss: 1.002909] [G loss: 1.000242]\n",
      "1946 [D loss: 1.003102] [G loss: 1.000420]\n",
      "1947 [D loss: 1.002913] [G loss: 1.000106]\n",
      "1948 [D loss: 1.002996] [G loss: 1.000264]\n",
      "1949 [D loss: 1.002922] [G loss: 1.000257]\n",
      "1950 [D loss: 1.003139] [G loss: 1.000115]\n",
      "1951 [D loss: 1.002721] [G loss: 1.000338]\n",
      "1952 [D loss: 1.003107] [G loss: 1.000157]\n",
      "1953 [D loss: 1.003118] [G loss: 1.000293]\n",
      "1954 [D loss: 1.002990] [G loss: 1.000235]\n",
      "1955 [D loss: 1.003117] [G loss: 1.000173]\n",
      "1956 [D loss: 1.002906] [G loss: 1.000178]\n",
      "1957 [D loss: 1.003009] [G loss: 1.000222]\n",
      "1958 [D loss: 1.002859] [G loss: 1.000484]\n",
      "1959 [D loss: 1.003028] [G loss: 1.000165]\n",
      "1960 [D loss: 1.002997] [G loss: 1.000265]\n",
      "1961 [D loss: 1.003038] [G loss: 1.000616]\n",
      "1962 [D loss: 1.002963] [G loss: 1.000321]\n",
      "1963 [D loss: 1.002993] [G loss: 1.000104]\n",
      "1964 [D loss: 1.003029] [G loss: 1.000551]\n",
      "1965 [D loss: 1.002984] [G loss: 1.000289]\n",
      "1966 [D loss: 1.003000] [G loss: 1.000030]\n",
      "1967 [D loss: 1.003049] [G loss: 1.000152]\n",
      "1968 [D loss: 1.002838] [G loss: 1.000316]\n",
      "1969 [D loss: 1.003093] [G loss: 1.000380]\n",
      "1970 [D loss: 1.002904] [G loss: 1.000192]\n",
      "1971 [D loss: 1.003133] [G loss: 1.000329]\n",
      "1972 [D loss: 1.002833] [G loss: 1.000365]\n",
      "1973 [D loss: 1.003033] [G loss: 1.000151]\n",
      "1974 [D loss: 1.003032] [G loss: 1.000276]\n",
      "1975 [D loss: 1.002776] [G loss: 1.000362]\n",
      "1976 [D loss: 1.003131] [G loss: 1.000347]\n",
      "1977 [D loss: 1.002871] [G loss: 1.000225]\n",
      "1978 [D loss: 1.002874] [G loss: 1.000140]\n",
      "1979 [D loss: 1.003188] [G loss: 1.000219]\n",
      "1980 [D loss: 1.002899] [G loss: 1.000361]\n",
      "1981 [D loss: 1.003000] [G loss: 1.000264]\n",
      "1982 [D loss: 1.003135] [G loss: 1.000261]\n",
      "1983 [D loss: 1.002779] [G loss: 1.000326]\n",
      "1984 [D loss: 1.003058] [G loss: 1.000115]\n",
      "1985 [D loss: 1.002941] [G loss: 1.000242]\n",
      "1986 [D loss: 1.002954] [G loss: 1.000295]\n",
      "1987 [D loss: 1.002982] [G loss: 1.000168]\n",
      "1988 [D loss: 1.002983] [G loss: 1.000357]\n",
      "1989 [D loss: 1.003139] [G loss: 1.000388]\n",
      "1990 [D loss: 1.002923] [G loss: 1.000346]\n",
      "1991 [D loss: 1.002964] [G loss: 1.000526]\n",
      "1992 [D loss: 1.002905] [G loss: 1.000241]\n",
      "1993 [D loss: 1.003076] [G loss: 1.000177]\n",
      "1994 [D loss: 1.002982] [G loss: 1.000310]\n",
      "1995 [D loss: 1.003064] [G loss: 1.000200]\n",
      "1996 [D loss: 1.003088] [G loss: 0.999973]\n",
      "1997 [D loss: 1.003013] [G loss: 1.000275]\n",
      "1998 [D loss: 1.003027] [G loss: 1.000265]\n",
      "1999 [D loss: 1.002994] [G loss: 1.000351]\n",
      "2000 [D loss: 1.002932] [G loss: 1.000098]\n",
      "(14461, 768)\n",
      "2001 [D loss: 1.003097] [G loss: 1.000386]\n",
      "2002 [D loss: 1.002842] [G loss: 1.000491]\n",
      "2003 [D loss: 1.003154] [G loss: 1.000228]\n",
      "2004 [D loss: 1.003117] [G loss: 1.000372]\n",
      "2005 [D loss: 1.003019] [G loss: 1.000271]\n",
      "2006 [D loss: 1.002849] [G loss: 1.000213]\n",
      "2007 [D loss: 1.002840] [G loss: 1.000279]\n",
      "2008 [D loss: 1.002884] [G loss: 1.000596]\n",
      "2009 [D loss: 1.003043] [G loss: 1.000298]\n",
      "2010 [D loss: 1.002860] [G loss: 1.000481]\n",
      "2011 [D loss: 1.003080] [G loss: 1.000280]\n",
      "2012 [D loss: 1.003162] [G loss: 1.000283]\n",
      "2013 [D loss: 1.003038] [G loss: 1.000197]\n",
      "2014 [D loss: 1.002996] [G loss: 1.000285]\n",
      "2015 [D loss: 1.002897] [G loss: 1.000276]\n",
      "2016 [D loss: 1.002827] [G loss: 1.000327]\n",
      "2017 [D loss: 1.003194] [G loss: 1.000576]\n",
      "2018 [D loss: 1.003043] [G loss: 1.000270]\n",
      "2019 [D loss: 1.003042] [G loss: 1.000109]\n",
      "2020 [D loss: 1.002813] [G loss: 1.000317]\n",
      "2021 [D loss: 1.002968] [G loss: 1.000447]\n",
      "2022 [D loss: 1.002986] [G loss: 1.000427]\n",
      "2023 [D loss: 1.003053] [G loss: 1.000062]\n",
      "2024 [D loss: 1.003041] [G loss: 1.000153]\n",
      "2025 [D loss: 1.003185] [G loss: 1.000449]\n",
      "2026 [D loss: 1.002856] [G loss: 1.000256]\n",
      "2027 [D loss: 1.002951] [G loss: 1.000432]\n",
      "2028 [D loss: 1.003101] [G loss: 1.000403]\n",
      "2029 [D loss: 1.002994] [G loss: 1.000242]\n",
      "2030 [D loss: 1.002874] [G loss: 1.000356]\n",
      "2031 [D loss: 1.002997] [G loss: 1.000180]\n",
      "2032 [D loss: 1.002931] [G loss: 1.000181]\n",
      "2033 [D loss: 1.003049] [G loss: 1.000144]\n",
      "2034 [D loss: 1.003102] [G loss: 1.000309]\n",
      "2035 [D loss: 1.002948] [G loss: 1.000227]\n",
      "2036 [D loss: 1.003014] [G loss: 1.000400]\n",
      "2037 [D loss: 1.002867] [G loss: 1.000186]\n",
      "2038 [D loss: 1.003091] [G loss: 1.000213]\n",
      "2039 [D loss: 1.002971] [G loss: 1.000483]\n",
      "2040 [D loss: 1.003010] [G loss: 1.000258]\n",
      "2041 [D loss: 1.003155] [G loss: 1.000335]\n",
      "2042 [D loss: 1.002957] [G loss: 1.000141]\n",
      "2043 [D loss: 1.003067] [G loss: 1.000368]\n",
      "2044 [D loss: 1.003011] [G loss: 1.000515]\n",
      "2045 [D loss: 1.002955] [G loss: 1.000340]\n",
      "2046 [D loss: 1.003085] [G loss: 1.000235]\n",
      "2047 [D loss: 1.002875] [G loss: 1.000445]\n",
      "2048 [D loss: 1.003162] [G loss: 1.000319]\n",
      "2049 [D loss: 1.003044] [G loss: 1.000491]\n",
      "2050 [D loss: 1.002889] [G loss: 1.000411]\n",
      "2051 [D loss: 1.003022] [G loss: 1.000344]\n",
      "2052 [D loss: 1.002965] [G loss: 1.000383]\n",
      "2053 [D loss: 1.003066] [G loss: 1.000317]\n",
      "2054 [D loss: 1.002943] [G loss: 1.000218]\n",
      "2055 [D loss: 1.003106] [G loss: 1.000118]\n",
      "2056 [D loss: 1.003006] [G loss: 1.000172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2057 [D loss: 1.002995] [G loss: 1.000444]\n",
      "2058 [D loss: 1.003165] [G loss: 1.000419]\n",
      "2059 [D loss: 1.002982] [G loss: 1.000289]\n",
      "2060 [D loss: 1.003260] [G loss: 1.000275]\n",
      "2061 [D loss: 1.002895] [G loss: 0.999959]\n",
      "2062 [D loss: 1.003047] [G loss: 1.000317]\n",
      "2063 [D loss: 1.003001] [G loss: 1.000183]\n",
      "2064 [D loss: 1.002974] [G loss: 1.000091]\n",
      "2065 [D loss: 1.003041] [G loss: 1.000320]\n",
      "2066 [D loss: 1.003174] [G loss: 1.000347]\n",
      "2067 [D loss: 1.003001] [G loss: 1.000185]\n",
      "2068 [D loss: 1.002871] [G loss: 1.000085]\n",
      "2069 [D loss: 1.003125] [G loss: 1.000136]\n",
      "2070 [D loss: 1.002935] [G loss: 1.000392]\n",
      "2071 [D loss: 1.003121] [G loss: 1.000294]\n",
      "2072 [D loss: 1.003044] [G loss: 1.000287]\n",
      "2073 [D loss: 1.002793] [G loss: 1.000154]\n",
      "2074 [D loss: 1.002963] [G loss: 1.000319]\n",
      "2075 [D loss: 1.002970] [G loss: 1.000257]\n",
      "2076 [D loss: 1.002897] [G loss: 1.000358]\n",
      "2077 [D loss: 1.002893] [G loss: 1.000137]\n",
      "2078 [D loss: 1.002926] [G loss: 1.000110]\n",
      "2079 [D loss: 1.002876] [G loss: 1.000398]\n",
      "2080 [D loss: 1.003016] [G loss: 1.000255]\n",
      "2081 [D loss: 1.002961] [G loss: 1.000112]\n",
      "2082 [D loss: 1.003080] [G loss: 1.000361]\n",
      "2083 [D loss: 1.002956] [G loss: 1.000104]\n",
      "2084 [D loss: 1.002914] [G loss: 1.000215]\n",
      "2085 [D loss: 1.003087] [G loss: 1.000334]\n",
      "2086 [D loss: 1.003053] [G loss: 1.000285]\n",
      "2087 [D loss: 1.002948] [G loss: 1.000428]\n",
      "2088 [D loss: 1.002906] [G loss: 1.000273]\n",
      "2089 [D loss: 1.003154] [G loss: 1.000298]\n",
      "2090 [D loss: 1.003143] [G loss: 1.000293]\n",
      "2091 [D loss: 1.002981] [G loss: 1.000259]\n",
      "2092 [D loss: 1.002950] [G loss: 1.000354]\n",
      "2093 [D loss: 1.003097] [G loss: 1.000083]\n",
      "2094 [D loss: 1.002949] [G loss: 1.000519]\n",
      "2095 [D loss: 1.003085] [G loss: 1.000168]\n",
      "2096 [D loss: 1.002960] [G loss: 1.000330]\n",
      "2097 [D loss: 1.002905] [G loss: 1.000148]\n",
      "2098 [D loss: 1.003102] [G loss: 1.000300]\n",
      "2099 [D loss: 1.003055] [G loss: 1.000219]\n",
      "2100 [D loss: 1.003299] [G loss: 1.000052]\n",
      "2101 [D loss: 1.002978] [G loss: 1.000457]\n",
      "2102 [D loss: 1.003182] [G loss: 1.000287]\n",
      "2103 [D loss: 1.002956] [G loss: 1.000321]\n",
      "2104 [D loss: 1.002964] [G loss: 1.000335]\n",
      "2105 [D loss: 1.003045] [G loss: 1.000089]\n",
      "2106 [D loss: 1.003071] [G loss: 1.000031]\n",
      "2107 [D loss: 1.003053] [G loss: 1.000173]\n",
      "2108 [D loss: 1.003100] [G loss: 1.000028]\n",
      "2109 [D loss: 1.003088] [G loss: 1.000152]\n",
      "2110 [D loss: 1.003085] [G loss: 1.000123]\n",
      "2111 [D loss: 1.002996] [G loss: 1.000446]\n",
      "2112 [D loss: 1.003059] [G loss: 1.000157]\n",
      "2113 [D loss: 1.002956] [G loss: 1.000062]\n",
      "2114 [D loss: 1.003093] [G loss: 1.000280]\n",
      "2115 [D loss: 1.002922] [G loss: 1.000231]\n",
      "2116 [D loss: 1.002824] [G loss: 1.000178]\n",
      "2117 [D loss: 1.003075] [G loss: 1.000267]\n",
      "2118 [D loss: 1.003074] [G loss: 1.000201]\n",
      "2119 [D loss: 1.002795] [G loss: 1.000211]\n",
      "2120 [D loss: 1.003094] [G loss: 1.000262]\n",
      "2121 [D loss: 1.002928] [G loss: 1.000156]\n",
      "2122 [D loss: 1.003056] [G loss: 1.000415]\n",
      "2123 [D loss: 1.002929] [G loss: 0.999954]\n",
      "2124 [D loss: 1.002848] [G loss: 1.000177]\n",
      "2125 [D loss: 1.003022] [G loss: 1.000121]\n",
      "2126 [D loss: 1.002982] [G loss: 1.000206]\n",
      "2127 [D loss: 1.002996] [G loss: 1.000248]\n",
      "2128 [D loss: 1.002969] [G loss: 1.000217]\n",
      "2129 [D loss: 1.003065] [G loss: 1.000015]\n",
      "2130 [D loss: 1.003069] [G loss: 1.000178]\n",
      "2131 [D loss: 1.002987] [G loss: 1.000026]\n",
      "2132 [D loss: 1.002954] [G loss: 1.000153]\n",
      "2133 [D loss: 1.002970] [G loss: 1.000296]\n",
      "2134 [D loss: 1.002907] [G loss: 1.000327]\n",
      "2135 [D loss: 1.002939] [G loss: 1.000413]\n",
      "2136 [D loss: 1.003138] [G loss: 1.000198]\n",
      "2137 [D loss: 1.003236] [G loss: 0.999994]\n",
      "2138 [D loss: 1.003122] [G loss: 1.000050]\n",
      "2139 [D loss: 1.003046] [G loss: 1.000071]\n",
      "2140 [D loss: 1.002870] [G loss: 1.000172]\n",
      "2141 [D loss: 1.002943] [G loss: 1.000046]\n",
      "2142 [D loss: 1.003138] [G loss: 1.000340]\n",
      "2143 [D loss: 1.002898] [G loss: 1.000229]\n",
      "2144 [D loss: 1.003074] [G loss: 1.000217]\n",
      "2145 [D loss: 1.003090] [G loss: 1.000232]\n",
      "2146 [D loss: 1.003158] [G loss: 1.000340]\n",
      "2147 [D loss: 1.003034] [G loss: 1.000259]\n",
      "2148 [D loss: 1.002913] [G loss: 1.000313]\n",
      "2149 [D loss: 1.002810] [G loss: 1.000416]\n",
      "2150 [D loss: 1.003136] [G loss: 1.000220]\n",
      "2151 [D loss: 1.002890] [G loss: 1.000290]\n",
      "2152 [D loss: 1.003254] [G loss: 1.000114]\n",
      "2153 [D loss: 1.002960] [G loss: 1.000357]\n",
      "2154 [D loss: 1.003039] [G loss: 1.000111]\n",
      "2155 [D loss: 1.002960] [G loss: 0.999926]\n",
      "2156 [D loss: 1.002964] [G loss: 1.000207]\n",
      "2157 [D loss: 1.002917] [G loss: 1.000219]\n",
      "2158 [D loss: 1.003057] [G loss: 1.000345]\n",
      "2159 [D loss: 1.002820] [G loss: 1.000144]\n",
      "2160 [D loss: 1.003077] [G loss: 1.000166]\n",
      "2161 [D loss: 1.003070] [G loss: 1.000309]\n",
      "2162 [D loss: 1.002861] [G loss: 1.000434]\n",
      "2163 [D loss: 1.002853] [G loss: 1.000219]\n",
      "2164 [D loss: 1.002872] [G loss: 1.000144]\n",
      "2165 [D loss: 1.002963] [G loss: 1.000096]\n",
      "2166 [D loss: 1.003187] [G loss: 1.000334]\n",
      "2167 [D loss: 1.003115] [G loss: 1.000374]\n",
      "2168 [D loss: 1.003019] [G loss: 1.000366]\n",
      "2169 [D loss: 1.002975] [G loss: 1.000313]\n",
      "2170 [D loss: 1.003061] [G loss: 1.000390]\n",
      "2171 [D loss: 1.002975] [G loss: 1.000108]\n",
      "2172 [D loss: 1.003060] [G loss: 1.000306]\n",
      "2173 [D loss: 1.002895] [G loss: 1.000298]\n",
      "2174 [D loss: 1.003135] [G loss: 1.000400]\n",
      "2175 [D loss: 1.003065] [G loss: 1.000420]\n",
      "2176 [D loss: 1.002782] [G loss: 1.000155]\n",
      "2177 [D loss: 1.003058] [G loss: 1.000441]\n",
      "2178 [D loss: 1.002726] [G loss: 1.000197]\n",
      "2179 [D loss: 1.003003] [G loss: 1.000164]\n",
      "2180 [D loss: 1.003091] [G loss: 1.000061]\n",
      "2181 [D loss: 1.003018] [G loss: 1.000153]\n",
      "2182 [D loss: 1.003218] [G loss: 1.000424]\n",
      "2183 [D loss: 1.003177] [G loss: 1.000300]\n",
      "2184 [D loss: 1.003002] [G loss: 1.000291]\n",
      "2185 [D loss: 1.002885] [G loss: 1.000247]\n",
      "2186 [D loss: 1.002980] [G loss: 1.000236]\n",
      "2187 [D loss: 1.003114] [G loss: 1.000241]\n",
      "2188 [D loss: 1.002945] [G loss: 1.000440]\n",
      "2189 [D loss: 1.002977] [G loss: 1.000271]\n",
      "2190 [D loss: 1.003066] [G loss: 1.000055]\n",
      "2191 [D loss: 1.003159] [G loss: 1.000416]\n",
      "2192 [D loss: 1.003009] [G loss: 1.000256]\n",
      "2193 [D loss: 1.002882] [G loss: 1.000181]\n",
      "2194 [D loss: 1.002973] [G loss: 1.000377]\n",
      "2195 [D loss: 1.002967] [G loss: 1.000132]\n",
      "2196 [D loss: 1.003144] [G loss: 1.000292]\n",
      "2197 [D loss: 1.003023] [G loss: 1.000213]\n",
      "2198 [D loss: 1.002977] [G loss: 1.000277]\n",
      "2199 [D loss: 1.002910] [G loss: 1.000274]\n",
      "2200 [D loss: 1.003163] [G loss: 1.000456]\n",
      "2201 [D loss: 1.003152] [G loss: 1.000323]\n",
      "2202 [D loss: 1.002900] [G loss: 1.000176]\n",
      "2203 [D loss: 1.002950] [G loss: 1.000435]\n",
      "2204 [D loss: 1.002932] [G loss: 1.000257]\n",
      "2205 [D loss: 1.003019] [G loss: 1.000229]\n",
      "2206 [D loss: 1.003020] [G loss: 1.000133]\n",
      "2207 [D loss: 1.003014] [G loss: 0.999971]\n",
      "2208 [D loss: 1.002712] [G loss: 1.000207]\n",
      "2209 [D loss: 1.002890] [G loss: 1.000188]\n",
      "2210 [D loss: 1.002929] [G loss: 1.000433]\n",
      "2211 [D loss: 1.002891] [G loss: 1.000277]\n",
      "2212 [D loss: 1.003095] [G loss: 1.000301]\n",
      "2213 [D loss: 1.003115] [G loss: 1.000567]\n",
      "2214 [D loss: 1.003010] [G loss: 1.000224]\n",
      "2215 [D loss: 1.003007] [G loss: 1.000381]\n",
      "2216 [D loss: 1.003026] [G loss: 1.000366]\n",
      "2217 [D loss: 1.003016] [G loss: 1.000045]\n",
      "2218 [D loss: 1.003029] [G loss: 1.000004]\n",
      "2219 [D loss: 1.002884] [G loss: 1.000112]\n",
      "2220 [D loss: 1.002854] [G loss: 1.000194]\n",
      "2221 [D loss: 1.002729] [G loss: 1.000119]\n",
      "2222 [D loss: 1.002785] [G loss: 1.000054]\n",
      "2223 [D loss: 1.003153] [G loss: 1.000314]\n",
      "2224 [D loss: 1.003156] [G loss: 1.000109]\n",
      "2225 [D loss: 1.002733] [G loss: 1.000310]\n",
      "2226 [D loss: 1.002809] [G loss: 1.000064]\n",
      "2227 [D loss: 1.002970] [G loss: 1.000154]\n",
      "2228 [D loss: 1.002996] [G loss: 1.000298]\n",
      "2229 [D loss: 1.003047] [G loss: 1.000264]\n",
      "2230 [D loss: 1.002926] [G loss: 0.999980]\n",
      "2231 [D loss: 1.003025] [G loss: 1.000256]\n",
      "2232 [D loss: 1.003232] [G loss: 1.000161]\n",
      "2233 [D loss: 1.003080] [G loss: 1.000351]\n",
      "2234 [D loss: 1.003061] [G loss: 1.000210]\n",
      "2235 [D loss: 1.002995] [G loss: 1.000140]\n",
      "2236 [D loss: 1.003102] [G loss: 1.000347]\n",
      "2237 [D loss: 1.003066] [G loss: 1.000298]\n",
      "2238 [D loss: 1.002926] [G loss: 1.000218]\n",
      "2239 [D loss: 1.002914] [G loss: 1.000297]\n",
      "2240 [D loss: 1.003014] [G loss: 1.000496]\n",
      "2241 [D loss: 1.003005] [G loss: 1.000632]\n",
      "2242 [D loss: 1.003028] [G loss: 1.000328]\n",
      "2243 [D loss: 1.003155] [G loss: 1.000349]\n",
      "2244 [D loss: 1.003061] [G loss: 1.000264]\n",
      "2245 [D loss: 1.002959] [G loss: 1.000352]\n",
      "2246 [D loss: 1.002955] [G loss: 1.000428]\n",
      "2247 [D loss: 1.002826] [G loss: 1.000410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2248 [D loss: 1.003057] [G loss: 1.000254]\n",
      "2249 [D loss: 1.002966] [G loss: 1.000187]\n",
      "2250 [D loss: 1.002968] [G loss: 1.000246]\n",
      "2251 [D loss: 1.003028] [G loss: 1.000420]\n",
      "2252 [D loss: 1.002876] [G loss: 1.000380]\n",
      "2253 [D loss: 1.003099] [G loss: 1.000586]\n",
      "2254 [D loss: 1.002995] [G loss: 1.000456]\n",
      "2255 [D loss: 1.003094] [G loss: 1.000111]\n",
      "2256 [D loss: 1.002750] [G loss: 1.000238]\n",
      "2257 [D loss: 1.002982] [G loss: 1.000402]\n",
      "2258 [D loss: 1.003002] [G loss: 1.000286]\n",
      "2259 [D loss: 1.003326] [G loss: 1.000201]\n",
      "2260 [D loss: 1.002935] [G loss: 1.000093]\n",
      "2261 [D loss: 1.002902] [G loss: 1.000246]\n",
      "2262 [D loss: 1.003076] [G loss: 1.000233]\n",
      "2263 [D loss: 1.002974] [G loss: 1.000140]\n",
      "2264 [D loss: 1.003246] [G loss: 1.000277]\n",
      "2265 [D loss: 1.003048] [G loss: 0.999975]\n",
      "2266 [D loss: 1.002973] [G loss: 1.000062]\n",
      "2267 [D loss: 1.003191] [G loss: 1.000440]\n",
      "2268 [D loss: 1.002774] [G loss: 1.000235]\n",
      "2269 [D loss: 1.002835] [G loss: 1.000225]\n",
      "2270 [D loss: 1.003035] [G loss: 1.000083]\n",
      "2271 [D loss: 1.002937] [G loss: 1.000283]\n",
      "2272 [D loss: 1.002809] [G loss: 1.000175]\n",
      "2273 [D loss: 1.002819] [G loss: 1.000336]\n",
      "2274 [D loss: 1.002909] [G loss: 1.000416]\n",
      "2275 [D loss: 1.002967] [G loss: 1.000209]\n",
      "2276 [D loss: 1.003106] [G loss: 1.000379]\n",
      "2277 [D loss: 1.002953] [G loss: 1.000420]\n",
      "2278 [D loss: 1.002891] [G loss: 1.000203]\n",
      "2279 [D loss: 1.002992] [G loss: 1.000457]\n",
      "2280 [D loss: 1.002772] [G loss: 1.000322]\n",
      "2281 [D loss: 1.002974] [G loss: 1.000043]\n",
      "2282 [D loss: 1.002978] [G loss: 1.000345]\n",
      "2283 [D loss: 1.002982] [G loss: 1.000340]\n",
      "2284 [D loss: 1.002999] [G loss: 1.000207]\n",
      "2285 [D loss: 1.002934] [G loss: 1.000254]\n",
      "2286 [D loss: 1.002883] [G loss: 1.000405]\n",
      "2287 [D loss: 1.002991] [G loss: 1.000144]\n",
      "2288 [D loss: 1.002989] [G loss: 1.000285]\n",
      "2289 [D loss: 1.002980] [G loss: 1.000557]\n",
      "2290 [D loss: 1.002754] [G loss: 1.000371]\n",
      "2291 [D loss: 1.002932] [G loss: 1.000275]\n",
      "2292 [D loss: 1.002822] [G loss: 1.000273]\n",
      "2293 [D loss: 1.003074] [G loss: 1.000120]\n",
      "2294 [D loss: 1.003182] [G loss: 1.000336]\n",
      "2295 [D loss: 1.003257] [G loss: 1.000268]\n",
      "2296 [D loss: 1.003120] [G loss: 1.000324]\n",
      "2297 [D loss: 1.003091] [G loss: 1.000377]\n",
      "2298 [D loss: 1.002965] [G loss: 1.000573]\n",
      "2299 [D loss: 1.002917] [G loss: 1.000590]\n",
      "2300 [D loss: 1.002884] [G loss: 1.000352]\n",
      "2301 [D loss: 1.002793] [G loss: 1.000201]\n",
      "2302 [D loss: 1.002933] [G loss: 1.000161]\n",
      "2303 [D loss: 1.003006] [G loss: 1.000308]\n",
      "2304 [D loss: 1.002841] [G loss: 1.000248]\n",
      "2305 [D loss: 1.002961] [G loss: 1.000294]\n",
      "2306 [D loss: 1.003067] [G loss: 1.000143]\n",
      "2307 [D loss: 1.003094] [G loss: 1.000287]\n",
      "2308 [D loss: 1.003009] [G loss: 1.000202]\n",
      "2309 [D loss: 1.002959] [G loss: 1.000481]\n",
      "2310 [D loss: 1.003129] [G loss: 0.999964]\n",
      "2311 [D loss: 1.003119] [G loss: 1.000380]\n",
      "2312 [D loss: 1.002858] [G loss: 1.000381]\n",
      "2313 [D loss: 1.002923] [G loss: 1.000249]\n",
      "2314 [D loss: 1.003108] [G loss: 1.000244]\n",
      "2315 [D loss: 1.003128] [G loss: 1.000217]\n",
      "2316 [D loss: 1.002972] [G loss: 1.000324]\n",
      "2317 [D loss: 1.002980] [G loss: 1.000338]\n",
      "2318 [D loss: 1.002836] [G loss: 1.000538]\n",
      "2319 [D loss: 1.003002] [G loss: 1.000148]\n",
      "2320 [D loss: 1.003007] [G loss: 1.000434]\n",
      "2321 [D loss: 1.003041] [G loss: 1.000134]\n",
      "2322 [D loss: 1.003072] [G loss: 1.000271]\n",
      "2323 [D loss: 1.002829] [G loss: 1.000463]\n",
      "2324 [D loss: 1.003131] [G loss: 1.000221]\n",
      "2325 [D loss: 1.003021] [G loss: 1.000064]\n",
      "2326 [D loss: 1.003128] [G loss: 1.000189]\n",
      "2327 [D loss: 1.002930] [G loss: 1.000291]\n",
      "2328 [D loss: 1.002831] [G loss: 1.000193]\n",
      "2329 [D loss: 1.002921] [G loss: 1.000236]\n",
      "2330 [D loss: 1.002966] [G loss: 1.000085]\n",
      "2331 [D loss: 1.003166] [G loss: 1.000309]\n",
      "2332 [D loss: 1.003161] [G loss: 1.000230]\n",
      "2333 [D loss: 1.003029] [G loss: 1.000173]\n",
      "2334 [D loss: 1.002964] [G loss: 1.000086]\n",
      "2335 [D loss: 1.003101] [G loss: 1.000203]\n",
      "2336 [D loss: 1.002777] [G loss: 1.000562]\n",
      "2337 [D loss: 1.003099] [G loss: 1.000396]\n",
      "2338 [D loss: 1.002849] [G loss: 1.000243]\n",
      "2339 [D loss: 1.003207] [G loss: 1.000341]\n",
      "2340 [D loss: 1.002809] [G loss: 1.000179]\n",
      "2341 [D loss: 1.003142] [G loss: 1.000386]\n",
      "2342 [D loss: 1.002852] [G loss: 1.000299]\n",
      "2343 [D loss: 1.002868] [G loss: 1.000563]\n",
      "2344 [D loss: 1.003107] [G loss: 1.000265]\n",
      "2345 [D loss: 1.002969] [G loss: 1.000303]\n",
      "2346 [D loss: 1.003085] [G loss: 1.000580]\n",
      "2347 [D loss: 1.002866] [G loss: 1.000367]\n",
      "2348 [D loss: 1.003037] [G loss: 1.000427]\n",
      "2349 [D loss: 1.002912] [G loss: 1.000151]\n",
      "2350 [D loss: 1.002832] [G loss: 1.000168]\n",
      "2351 [D loss: 1.002953] [G loss: 1.000195]\n",
      "2352 [D loss: 1.003061] [G loss: 1.000235]\n",
      "2353 [D loss: 1.003033] [G loss: 1.000443]\n",
      "2354 [D loss: 1.002955] [G loss: 1.000361]\n",
      "2355 [D loss: 1.002962] [G loss: 1.000425]\n",
      "2356 [D loss: 1.003016] [G loss: 1.000153]\n",
      "2357 [D loss: 1.002977] [G loss: 1.000301]\n",
      "2358 [D loss: 1.003070] [G loss: 1.000395]\n",
      "2359 [D loss: 1.002754] [G loss: 1.000243]\n",
      "2360 [D loss: 1.003092] [G loss: 1.000335]\n",
      "2361 [D loss: 1.002912] [G loss: 1.000419]\n",
      "2362 [D loss: 1.002968] [G loss: 1.000279]\n",
      "2363 [D loss: 1.003208] [G loss: 1.000190]\n",
      "2364 [D loss: 1.002992] [G loss: 1.000380]\n",
      "2365 [D loss: 1.003003] [G loss: 1.000267]\n",
      "2366 [D loss: 1.003058] [G loss: 1.000474]\n",
      "2367 [D loss: 1.002985] [G loss: 1.000295]\n",
      "2368 [D loss: 1.002985] [G loss: 1.000184]\n",
      "2369 [D loss: 1.002951] [G loss: 1.000180]\n",
      "2370 [D loss: 1.002934] [G loss: 1.000113]\n",
      "2371 [D loss: 1.003069] [G loss: 1.000114]\n",
      "2372 [D loss: 1.003049] [G loss: 1.000222]\n",
      "2373 [D loss: 1.002969] [G loss: 1.000213]\n",
      "2374 [D loss: 1.002874] [G loss: 1.000277]\n",
      "2375 [D loss: 1.003024] [G loss: 1.000115]\n",
      "2376 [D loss: 1.003034] [G loss: 1.000184]\n",
      "2377 [D loss: 1.002957] [G loss: 0.999968]\n",
      "2378 [D loss: 1.003124] [G loss: 1.000243]\n",
      "2379 [D loss: 1.003164] [G loss: 1.000257]\n",
      "2380 [D loss: 1.002947] [G loss: 1.000172]\n",
      "2381 [D loss: 1.003130] [G loss: 1.000046]\n",
      "2382 [D loss: 1.003090] [G loss: 1.000283]\n",
      "2383 [D loss: 1.003130] [G loss: 1.000196]\n",
      "2384 [D loss: 1.003134] [G loss: 1.000071]\n",
      "2385 [D loss: 1.003084] [G loss: 1.000107]\n",
      "2386 [D loss: 1.003005] [G loss: 1.000319]\n",
      "2387 [D loss: 1.003045] [G loss: 1.000262]\n",
      "2388 [D loss: 1.003002] [G loss: 1.000226]\n",
      "2389 [D loss: 1.003093] [G loss: 1.000049]\n",
      "2390 [D loss: 1.002858] [G loss: 1.000170]\n",
      "2391 [D loss: 1.002886] [G loss: 1.000443]\n",
      "2392 [D loss: 1.002867] [G loss: 1.000306]\n",
      "2393 [D loss: 1.002859] [G loss: 1.000374]\n",
      "2394 [D loss: 1.003025] [G loss: 1.000231]\n",
      "2395 [D loss: 1.003007] [G loss: 1.000183]\n",
      "2396 [D loss: 1.003224] [G loss: 1.000444]\n",
      "2397 [D loss: 1.003149] [G loss: 0.999972]\n",
      "2398 [D loss: 1.002999] [G loss: 1.000168]\n",
      "2399 [D loss: 1.003134] [G loss: 1.000500]\n",
      "2400 [D loss: 1.003105] [G loss: 1.000201]\n",
      "2401 [D loss: 1.003157] [G loss: 0.999783]\n",
      "2402 [D loss: 1.003043] [G loss: 1.000379]\n",
      "2403 [D loss: 1.003030] [G loss: 1.000246]\n",
      "2404 [D loss: 1.003071] [G loss: 1.000188]\n",
      "2405 [D loss: 1.002851] [G loss: 1.000246]\n",
      "2406 [D loss: 1.002808] [G loss: 1.000316]\n",
      "2407 [D loss: 1.003002] [G loss: 1.000291]\n",
      "2408 [D loss: 1.002840] [G loss: 1.000396]\n",
      "2409 [D loss: 1.002895] [G loss: 1.000199]\n",
      "2410 [D loss: 1.003076] [G loss: 1.000477]\n",
      "2411 [D loss: 1.002940] [G loss: 1.000277]\n",
      "2412 [D loss: 1.002980] [G loss: 1.000366]\n",
      "2413 [D loss: 1.002861] [G loss: 1.000254]\n",
      "2414 [D loss: 1.003139] [G loss: 1.000443]\n",
      "2415 [D loss: 1.003114] [G loss: 1.000230]\n",
      "2416 [D loss: 1.002998] [G loss: 1.000322]\n",
      "2417 [D loss: 1.002901] [G loss: 1.000037]\n",
      "2418 [D loss: 1.002968] [G loss: 1.000222]\n",
      "2419 [D loss: 1.003250] [G loss: 1.000132]\n",
      "2420 [D loss: 1.003003] [G loss: 1.000420]\n",
      "2421 [D loss: 1.003133] [G loss: 1.000193]\n",
      "2422 [D loss: 1.002960] [G loss: 1.000215]\n",
      "2423 [D loss: 1.002858] [G loss: 1.000676]\n",
      "2424 [D loss: 1.002992] [G loss: 1.000108]\n",
      "2425 [D loss: 1.002831] [G loss: 1.000437]\n",
      "2426 [D loss: 1.003024] [G loss: 1.000185]\n",
      "2427 [D loss: 1.003059] [G loss: 1.000007]\n",
      "2428 [D loss: 1.003029] [G loss: 1.000343]\n",
      "2429 [D loss: 1.002936] [G loss: 1.000404]\n",
      "2430 [D loss: 1.002925] [G loss: 1.000148]\n",
      "2431 [D loss: 1.002915] [G loss: 1.000339]\n",
      "2432 [D loss: 1.003092] [G loss: 1.000196]\n",
      "2433 [D loss: 1.002870] [G loss: 1.000356]\n",
      "2434 [D loss: 1.003176] [G loss: 1.000084]\n",
      "2435 [D loss: 1.002952] [G loss: 1.000319]\n",
      "2436 [D loss: 1.002830] [G loss: 1.000407]\n",
      "2437 [D loss: 1.003086] [G loss: 1.000234]\n",
      "2438 [D loss: 1.003255] [G loss: 1.000412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2439 [D loss: 1.002971] [G loss: 1.000292]\n",
      "2440 [D loss: 1.003136] [G loss: 1.000239]\n",
      "2441 [D loss: 1.003038] [G loss: 1.000289]\n",
      "2442 [D loss: 1.003037] [G loss: 1.000345]\n",
      "2443 [D loss: 1.002911] [G loss: 1.000419]\n",
      "2444 [D loss: 1.002980] [G loss: 1.000501]\n",
      "2445 [D loss: 1.002834] [G loss: 1.000430]\n",
      "2446 [D loss: 1.003148] [G loss: 1.000284]\n",
      "2447 [D loss: 1.002931] [G loss: 1.000283]\n",
      "2448 [D loss: 1.003029] [G loss: 1.000362]\n",
      "2449 [D loss: 1.002970] [G loss: 1.000468]\n",
      "2450 [D loss: 1.002958] [G loss: 1.000302]\n",
      "2451 [D loss: 1.003022] [G loss: 1.000190]\n",
      "2452 [D loss: 1.003016] [G loss: 1.000612]\n",
      "2453 [D loss: 1.002936] [G loss: 1.000286]\n",
      "2454 [D loss: 1.002787] [G loss: 1.000098]\n",
      "2455 [D loss: 1.002910] [G loss: 1.000453]\n",
      "2456 [D loss: 1.002892] [G loss: 1.000236]\n",
      "2457 [D loss: 1.003005] [G loss: 1.000353]\n",
      "2458 [D loss: 1.003084] [G loss: 1.000294]\n",
      "2459 [D loss: 1.003009] [G loss: 1.000292]\n",
      "2460 [D loss: 1.003172] [G loss: 1.000332]\n",
      "2461 [D loss: 1.002921] [G loss: 1.000310]\n",
      "2462 [D loss: 1.003064] [G loss: 1.000288]\n",
      "2463 [D loss: 1.002886] [G loss: 1.000388]\n",
      "2464 [D loss: 1.003035] [G loss: 1.000382]\n",
      "2465 [D loss: 1.002836] [G loss: 1.000199]\n",
      "2466 [D loss: 1.003229] [G loss: 1.000021]\n",
      "2467 [D loss: 1.002753] [G loss: 1.000205]\n",
      "2468 [D loss: 1.003028] [G loss: 0.999994]\n",
      "2469 [D loss: 1.002952] [G loss: 1.000253]\n",
      "2470 [D loss: 1.002943] [G loss: 1.000167]\n",
      "2471 [D loss: 1.002753] [G loss: 1.000147]\n",
      "2472 [D loss: 1.003158] [G loss: 1.000163]\n",
      "2473 [D loss: 1.003136] [G loss: 1.000243]\n",
      "2474 [D loss: 1.003190] [G loss: 1.000186]\n",
      "2475 [D loss: 1.002893] [G loss: 1.000213]\n",
      "2476 [D loss: 1.003060] [G loss: 1.000093]\n",
      "2477 [D loss: 1.002982] [G loss: 1.000287]\n",
      "2478 [D loss: 1.002934] [G loss: 1.000374]\n",
      "2479 [D loss: 1.002801] [G loss: 1.000209]\n",
      "2480 [D loss: 1.003051] [G loss: 1.000280]\n",
      "2481 [D loss: 1.003163] [G loss: 1.000345]\n",
      "2482 [D loss: 1.002944] [G loss: 1.000262]\n",
      "2483 [D loss: 1.002855] [G loss: 1.000223]\n",
      "2484 [D loss: 1.002966] [G loss: 1.000290]\n",
      "2485 [D loss: 1.003009] [G loss: 1.000365]\n",
      "2486 [D loss: 1.003090] [G loss: 1.000393]\n",
      "2487 [D loss: 1.003047] [G loss: 1.000290]\n",
      "2488 [D loss: 1.003041] [G loss: 1.000261]\n",
      "2489 [D loss: 1.003205] [G loss: 1.000145]\n",
      "2490 [D loss: 1.002875] [G loss: 1.000138]\n",
      "2491 [D loss: 1.003161] [G loss: 1.000313]\n",
      "2492 [D loss: 1.003089] [G loss: 1.000230]\n",
      "2493 [D loss: 1.002949] [G loss: 1.000325]\n",
      "2494 [D loss: 1.002870] [G loss: 1.000399]\n",
      "2495 [D loss: 1.003072] [G loss: 1.000128]\n",
      "2496 [D loss: 1.002934] [G loss: 1.000111]\n",
      "2497 [D loss: 1.003085] [G loss: 1.000193]\n",
      "2498 [D loss: 1.003189] [G loss: 1.000296]\n",
      "2499 [D loss: 1.002988] [G loss: 1.000196]\n",
      "2500 [D loss: 1.003015] [G loss: 1.000146]\n",
      "2501 [D loss: 1.003001] [G loss: 1.000282]\n",
      "2502 [D loss: 1.003130] [G loss: 1.000486]\n",
      "2503 [D loss: 1.002867] [G loss: 1.000243]\n",
      "2504 [D loss: 1.003030] [G loss: 1.000205]\n",
      "2505 [D loss: 1.002778] [G loss: 1.000402]\n",
      "2506 [D loss: 1.002904] [G loss: 1.000325]\n",
      "2507 [D loss: 1.002885] [G loss: 1.000320]\n",
      "2508 [D loss: 1.002858] [G loss: 1.000219]\n",
      "2509 [D loss: 1.002918] [G loss: 1.000278]\n",
      "2510 [D loss: 1.002783] [G loss: 1.000308]\n",
      "2511 [D loss: 1.002735] [G loss: 1.000374]\n",
      "2512 [D loss: 1.002990] [G loss: 1.000284]\n",
      "2513 [D loss: 1.003056] [G loss: 1.000208]\n",
      "2514 [D loss: 1.002826] [G loss: 1.000291]\n",
      "2515 [D loss: 1.002838] [G loss: 1.000281]\n",
      "2516 [D loss: 1.003004] [G loss: 1.000157]\n",
      "2517 [D loss: 1.002987] [G loss: 1.000397]\n",
      "2518 [D loss: 1.002841] [G loss: 1.000290]\n",
      "2519 [D loss: 1.002809] [G loss: 1.000268]\n",
      "2520 [D loss: 1.002785] [G loss: 1.000431]\n",
      "2521 [D loss: 1.003230] [G loss: 1.000332]\n",
      "2522 [D loss: 1.002870] [G loss: 1.000461]\n",
      "2523 [D loss: 1.003035] [G loss: 1.000260]\n",
      "2524 [D loss: 1.003016] [G loss: 1.000451]\n",
      "2525 [D loss: 1.002955] [G loss: 1.000132]\n",
      "2526 [D loss: 1.003107] [G loss: 1.000491]\n",
      "2527 [D loss: 1.002913] [G loss: 1.000261]\n",
      "2528 [D loss: 1.002911] [G loss: 1.000273]\n",
      "2529 [D loss: 1.002955] [G loss: 1.000250]\n",
      "2530 [D loss: 1.003014] [G loss: 1.000405]\n",
      "2531 [D loss: 1.002936] [G loss: 1.000369]\n",
      "2532 [D loss: 1.002971] [G loss: 1.000374]\n",
      "2533 [D loss: 1.003078] [G loss: 1.000490]\n",
      "2534 [D loss: 1.002957] [G loss: 1.000130]\n",
      "2535 [D loss: 1.002953] [G loss: 1.000067]\n",
      "2536 [D loss: 1.002949] [G loss: 1.000415]\n",
      "2537 [D loss: 1.003091] [G loss: 1.000102]\n",
      "2538 [D loss: 1.003038] [G loss: 0.999973]\n",
      "2539 [D loss: 1.003011] [G loss: 1.000283]\n",
      "2540 [D loss: 1.003121] [G loss: 1.000359]\n",
      "2541 [D loss: 1.002981] [G loss: 1.000182]\n",
      "2542 [D loss: 1.002837] [G loss: 1.000291]\n",
      "2543 [D loss: 1.002535] [G loss: 1.000317]\n",
      "2544 [D loss: 1.003084] [G loss: 1.000237]\n",
      "2545 [D loss: 1.002840] [G loss: 1.000357]\n",
      "2546 [D loss: 1.002852] [G loss: 1.000188]\n",
      "2547 [D loss: 1.003047] [G loss: 1.000474]\n",
      "2548 [D loss: 1.002963] [G loss: 1.000210]\n",
      "2549 [D loss: 1.002943] [G loss: 1.000435]\n",
      "2550 [D loss: 1.003047] [G loss: 1.000102]\n",
      "2551 [D loss: 1.002761] [G loss: 1.000360]\n",
      "2552 [D loss: 1.003032] [G loss: 1.000327]\n",
      "2553 [D loss: 1.003067] [G loss: 1.000324]\n",
      "2554 [D loss: 1.003010] [G loss: 1.000431]\n",
      "2555 [D loss: 1.002931] [G loss: 1.000281]\n",
      "2556 [D loss: 1.002899] [G loss: 1.000069]\n",
      "2557 [D loss: 1.003051] [G loss: 1.000227]\n",
      "2558 [D loss: 1.002879] [G loss: 1.000290]\n",
      "2559 [D loss: 1.003023] [G loss: 1.000086]\n",
      "2560 [D loss: 1.002956] [G loss: 1.000246]\n",
      "2561 [D loss: 1.003164] [G loss: 1.000167]\n",
      "2562 [D loss: 1.003118] [G loss: 1.000516]\n",
      "2563 [D loss: 1.003266] [G loss: 1.000150]\n",
      "2564 [D loss: 1.002974] [G loss: 1.000406]\n",
      "2565 [D loss: 1.003113] [G loss: 1.000286]\n",
      "2566 [D loss: 1.003021] [G loss: 1.000346]\n",
      "2567 [D loss: 1.003025] [G loss: 1.000524]\n",
      "2568 [D loss: 1.002722] [G loss: 1.000278]\n",
      "2569 [D loss: 1.002792] [G loss: 1.000744]\n",
      "2570 [D loss: 1.003084] [G loss: 1.000443]\n",
      "2571 [D loss: 1.003264] [G loss: 1.000280]\n",
      "2572 [D loss: 1.002821] [G loss: 1.000344]\n",
      "2573 [D loss: 1.002867] [G loss: 1.000352]\n",
      "2574 [D loss: 1.003100] [G loss: 0.999929]\n",
      "2575 [D loss: 1.002854] [G loss: 1.000153]\n",
      "2576 [D loss: 1.002978] [G loss: 1.000273]\n",
      "2577 [D loss: 1.003090] [G loss: 1.000405]\n",
      "2578 [D loss: 1.003087] [G loss: 1.000203]\n",
      "2579 [D loss: 1.002768] [G loss: 1.000129]\n",
      "2580 [D loss: 1.003155] [G loss: 1.000117]\n",
      "2581 [D loss: 1.003235] [G loss: 0.999993]\n",
      "2582 [D loss: 1.003017] [G loss: 1.000083]\n",
      "2583 [D loss: 1.002966] [G loss: 1.000133]\n",
      "2584 [D loss: 1.003111] [G loss: 1.000356]\n",
      "2585 [D loss: 1.003071] [G loss: 1.000272]\n",
      "2586 [D loss: 1.003042] [G loss: 1.000196]\n",
      "2587 [D loss: 1.003069] [G loss: 1.000250]\n",
      "2588 [D loss: 1.003121] [G loss: 1.000166]\n",
      "2589 [D loss: 1.003070] [G loss: 1.000069]\n",
      "2590 [D loss: 1.003187] [G loss: 1.000248]\n",
      "2591 [D loss: 1.003134] [G loss: 1.000199]\n",
      "2592 [D loss: 1.002939] [G loss: 1.000357]\n",
      "2593 [D loss: 1.003105] [G loss: 1.000232]\n",
      "2594 [D loss: 1.003053] [G loss: 1.000052]\n",
      "2595 [D loss: 1.002928] [G loss: 1.000389]\n",
      "2596 [D loss: 1.003127] [G loss: 1.000226]\n",
      "2597 [D loss: 1.003030] [G loss: 1.000104]\n",
      "2598 [D loss: 1.002723] [G loss: 1.000269]\n",
      "2599 [D loss: 1.003159] [G loss: 1.000219]\n",
      "2600 [D loss: 1.003007] [G loss: 1.000391]\n",
      "2601 [D loss: 1.003083] [G loss: 1.000338]\n",
      "2602 [D loss: 1.002992] [G loss: 1.000235]\n",
      "2603 [D loss: 1.002827] [G loss: 1.000422]\n",
      "2604 [D loss: 1.003139] [G loss: 1.000110]\n",
      "2605 [D loss: 1.003095] [G loss: 1.000257]\n",
      "2606 [D loss: 1.002990] [G loss: 1.000415]\n",
      "2607 [D loss: 1.002851] [G loss: 1.000012]\n",
      "2608 [D loss: 1.003018] [G loss: 1.000338]\n",
      "2609 [D loss: 1.002866] [G loss: 1.000371]\n",
      "2610 [D loss: 1.002874] [G loss: 1.000362]\n",
      "2611 [D loss: 1.002965] [G loss: 1.000270]\n",
      "2612 [D loss: 1.002843] [G loss: 1.000320]\n",
      "2613 [D loss: 1.003014] [G loss: 1.000539]\n",
      "2614 [D loss: 1.003025] [G loss: 1.000181]\n",
      "2615 [D loss: 1.002910] [G loss: 1.000426]\n",
      "2616 [D loss: 1.002823] [G loss: 1.000338]\n",
      "2617 [D loss: 1.003090] [G loss: 1.000387]\n",
      "2618 [D loss: 1.002842] [G loss: 1.000107]\n",
      "2619 [D loss: 1.002957] [G loss: 1.000251]\n",
      "2620 [D loss: 1.003075] [G loss: 1.000054]\n",
      "2621 [D loss: 1.003074] [G loss: 1.000328]\n",
      "2622 [D loss: 1.003071] [G loss: 1.000251]\n",
      "2623 [D loss: 1.002942] [G loss: 1.000160]\n",
      "2624 [D loss: 1.002978] [G loss: 1.000400]\n",
      "2625 [D loss: 1.002826] [G loss: 1.000354]\n",
      "2626 [D loss: 1.003210] [G loss: 1.000201]\n",
      "2627 [D loss: 1.002687] [G loss: 1.000146]\n",
      "2628 [D loss: 1.002979] [G loss: 1.000274]\n",
      "2629 [D loss: 1.003016] [G loss: 1.000200]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2630 [D loss: 1.002955] [G loss: 1.000205]\n",
      "2631 [D loss: 1.002965] [G loss: 1.000332]\n",
      "2632 [D loss: 1.002874] [G loss: 1.000261]\n",
      "2633 [D loss: 1.002811] [G loss: 1.000224]\n",
      "2634 [D loss: 1.003047] [G loss: 1.000452]\n",
      "2635 [D loss: 1.002905] [G loss: 1.000104]\n",
      "2636 [D loss: 1.002939] [G loss: 1.000398]\n",
      "2637 [D loss: 1.003027] [G loss: 1.000253]\n",
      "2638 [D loss: 1.003264] [G loss: 1.000236]\n",
      "2639 [D loss: 1.002844] [G loss: 1.000308]\n",
      "2640 [D loss: 1.003215] [G loss: 1.000331]\n",
      "2641 [D loss: 1.002921] [G loss: 1.000301]\n",
      "2642 [D loss: 1.002872] [G loss: 1.000326]\n",
      "2643 [D loss: 1.003209] [G loss: 1.000195]\n",
      "2644 [D loss: 1.003083] [G loss: 1.000253]\n",
      "2645 [D loss: 1.002853] [G loss: 1.000121]\n",
      "2646 [D loss: 1.002959] [G loss: 1.000168]\n",
      "2647 [D loss: 1.002937] [G loss: 1.000221]\n",
      "2648 [D loss: 1.002924] [G loss: 1.000342]\n",
      "2649 [D loss: 1.002969] [G loss: 1.000308]\n",
      "2650 [D loss: 1.003289] [G loss: 1.000308]\n",
      "2651 [D loss: 1.003151] [G loss: 1.000200]\n",
      "2652 [D loss: 1.003076] [G loss: 1.000121]\n",
      "2653 [D loss: 1.003117] [G loss: 1.000311]\n",
      "2654 [D loss: 1.002866] [G loss: 1.000201]\n",
      "2655 [D loss: 1.003202] [G loss: 1.000235]\n",
      "2656 [D loss: 1.002757] [G loss: 1.000207]\n",
      "2657 [D loss: 1.003210] [G loss: 1.000161]\n",
      "2658 [D loss: 1.002908] [G loss: 1.000270]\n",
      "2659 [D loss: 1.003038] [G loss: 1.000572]\n",
      "2660 [D loss: 1.003091] [G loss: 1.000401]\n",
      "2661 [D loss: 1.002877] [G loss: 1.000112]\n",
      "2662 [D loss: 1.002938] [G loss: 1.000333]\n",
      "2663 [D loss: 1.003037] [G loss: 1.000363]\n",
      "2664 [D loss: 1.003047] [G loss: 1.000443]\n",
      "2665 [D loss: 1.003002] [G loss: 1.000119]\n",
      "2666 [D loss: 1.003067] [G loss: 1.000188]\n",
      "2667 [D loss: 1.003050] [G loss: 1.000425]\n",
      "2668 [D loss: 1.003123] [G loss: 1.000454]\n",
      "2669 [D loss: 1.002956] [G loss: 1.000323]\n",
      "2670 [D loss: 1.003142] [G loss: 1.000194]\n",
      "2671 [D loss: 1.003197] [G loss: 1.000218]\n",
      "2672 [D loss: 1.003114] [G loss: 1.000361]\n",
      "2673 [D loss: 1.003014] [G loss: 1.000207]\n",
      "2674 [D loss: 1.003070] [G loss: 1.000294]\n",
      "2675 [D loss: 1.003201] [G loss: 1.000012]\n",
      "2676 [D loss: 1.002979] [G loss: 1.000191]\n",
      "2677 [D loss: 1.002901] [G loss: 1.000108]\n",
      "2678 [D loss: 1.002876] [G loss: 1.000085]\n",
      "2679 [D loss: 1.002969] [G loss: 1.000253]\n",
      "2680 [D loss: 1.003063] [G loss: 1.000111]\n",
      "2681 [D loss: 1.002988] [G loss: 1.000298]\n",
      "2682 [D loss: 1.002829] [G loss: 1.000479]\n",
      "2683 [D loss: 1.002734] [G loss: 1.000042]\n",
      "2684 [D loss: 1.002948] [G loss: 0.999903]\n",
      "2685 [D loss: 1.003064] [G loss: 1.000336]\n",
      "2686 [D loss: 1.002971] [G loss: 1.000128]\n",
      "2687 [D loss: 1.003106] [G loss: 1.000049]\n",
      "2688 [D loss: 1.002892] [G loss: 1.000321]\n",
      "2689 [D loss: 1.002996] [G loss: 1.000304]\n",
      "2690 [D loss: 1.002999] [G loss: 1.000237]\n",
      "2691 [D loss: 1.003083] [G loss: 1.000088]\n",
      "2692 [D loss: 1.002919] [G loss: 1.000204]\n",
      "2693 [D loss: 1.003038] [G loss: 1.000188]\n",
      "2694 [D loss: 1.002964] [G loss: 1.000298]\n",
      "2695 [D loss: 1.003061] [G loss: 1.000258]\n",
      "2696 [D loss: 1.002974] [G loss: 1.000154]\n",
      "2697 [D loss: 1.002866] [G loss: 0.999978]\n",
      "2698 [D loss: 1.002979] [G loss: 1.000198]\n",
      "2699 [D loss: 1.002925] [G loss: 1.000098]\n",
      "2700 [D loss: 1.002804] [G loss: 1.000119]\n",
      "2701 [D loss: 1.003084] [G loss: 1.000325]\n",
      "2702 [D loss: 1.003076] [G loss: 1.000062]\n",
      "2703 [D loss: 1.003071] [G loss: 1.000120]\n",
      "2704 [D loss: 1.003012] [G loss: 1.000226]\n",
      "2705 [D loss: 1.003043] [G loss: 1.000373]\n",
      "2706 [D loss: 1.002913] [G loss: 1.000183]\n",
      "2707 [D loss: 1.003059] [G loss: 1.000435]\n",
      "2708 [D loss: 1.002940] [G loss: 1.000368]\n",
      "2709 [D loss: 1.003037] [G loss: 1.000125]\n",
      "2710 [D loss: 1.003021] [G loss: 1.000106]\n",
      "2711 [D loss: 1.002905] [G loss: 1.000252]\n",
      "2712 [D loss: 1.002968] [G loss: 1.000406]\n",
      "2713 [D loss: 1.002771] [G loss: 1.000211]\n",
      "2714 [D loss: 1.003165] [G loss: 0.999975]\n",
      "2715 [D loss: 1.003048] [G loss: 1.000085]\n",
      "2716 [D loss: 1.003035] [G loss: 1.000257]\n",
      "2717 [D loss: 1.002816] [G loss: 1.000161]\n",
      "2718 [D loss: 1.003081] [G loss: 1.000334]\n",
      "2719 [D loss: 1.002988] [G loss: 1.000258]\n",
      "2720 [D loss: 1.003108] [G loss: 1.000214]\n",
      "2721 [D loss: 1.002944] [G loss: 1.000452]\n",
      "2722 [D loss: 1.003051] [G loss: 1.000347]\n",
      "2723 [D loss: 1.002990] [G loss: 1.000116]\n",
      "2724 [D loss: 1.003002] [G loss: 1.000019]\n",
      "2725 [D loss: 1.003117] [G loss: 1.000077]\n",
      "2726 [D loss: 1.002952] [G loss: 1.000233]\n",
      "2727 [D loss: 1.002923] [G loss: 1.000296]\n",
      "2728 [D loss: 1.002879] [G loss: 1.000170]\n",
      "2729 [D loss: 1.002893] [G loss: 1.000255]\n",
      "2730 [D loss: 1.002986] [G loss: 1.000412]\n",
      "2731 [D loss: 1.003020] [G loss: 1.000297]\n",
      "2732 [D loss: 1.003065] [G loss: 1.000281]\n",
      "2733 [D loss: 1.002957] [G loss: 1.000363]\n",
      "2734 [D loss: 1.002959] [G loss: 1.000207]\n",
      "2735 [D loss: 1.002939] [G loss: 1.000307]\n",
      "2736 [D loss: 1.002675] [G loss: 1.000191]\n",
      "2737 [D loss: 1.003126] [G loss: 1.000354]\n",
      "2738 [D loss: 1.002968] [G loss: 1.000200]\n",
      "2739 [D loss: 1.003055] [G loss: 1.000150]\n",
      "2740 [D loss: 1.002863] [G loss: 1.000019]\n",
      "2741 [D loss: 1.002646] [G loss: 1.000316]\n",
      "2742 [D loss: 1.003000] [G loss: 1.000178]\n",
      "2743 [D loss: 1.003001] [G loss: 1.000237]\n",
      "2744 [D loss: 1.003095] [G loss: 1.000173]\n",
      "2745 [D loss: 1.003059] [G loss: 0.999896]\n",
      "2746 [D loss: 1.003133] [G loss: 1.000323]\n",
      "2747 [D loss: 1.003145] [G loss: 1.000260]\n",
      "2748 [D loss: 1.003061] [G loss: 1.000104]\n",
      "2749 [D loss: 1.003066] [G loss: 1.000073]\n",
      "2750 [D loss: 1.002727] [G loss: 1.000225]\n",
      "2751 [D loss: 1.002954] [G loss: 1.000269]\n",
      "2752 [D loss: 1.003086] [G loss: 1.000111]\n",
      "2753 [D loss: 1.002945] [G loss: 1.000192]\n",
      "2754 [D loss: 1.003028] [G loss: 1.000216]\n",
      "2755 [D loss: 1.002964] [G loss: 1.000122]\n",
      "2756 [D loss: 1.002945] [G loss: 1.000145]\n",
      "2757 [D loss: 1.002922] [G loss: 1.000302]\n",
      "2758 [D loss: 1.003078] [G loss: 1.000019]\n",
      "2759 [D loss: 1.003093] [G loss: 1.000340]\n",
      "2760 [D loss: 1.002919] [G loss: 0.999937]\n",
      "2761 [D loss: 1.002901] [G loss: 1.000367]\n",
      "2762 [D loss: 1.002927] [G loss: 1.000300]\n",
      "2763 [D loss: 1.003020] [G loss: 1.000372]\n",
      "2764 [D loss: 1.003010] [G loss: 1.000192]\n",
      "2765 [D loss: 1.002859] [G loss: 1.000391]\n",
      "2766 [D loss: 1.002805] [G loss: 1.000241]\n",
      "2767 [D loss: 1.002901] [G loss: 1.000275]\n",
      "2768 [D loss: 1.002823] [G loss: 1.000183]\n",
      "2769 [D loss: 1.003021] [G loss: 1.000294]\n",
      "2770 [D loss: 1.002948] [G loss: 1.000085]\n",
      "2771 [D loss: 1.002874] [G loss: 1.000524]\n",
      "2772 [D loss: 1.003325] [G loss: 1.000059]\n",
      "2773 [D loss: 1.003000] [G loss: 1.000422]\n",
      "2774 [D loss: 1.002859] [G loss: 1.000015]\n",
      "2775 [D loss: 1.002814] [G loss: 1.000289]\n",
      "2776 [D loss: 1.003070] [G loss: 1.000125]\n",
      "2777 [D loss: 1.002876] [G loss: 1.000305]\n",
      "2778 [D loss: 1.002711] [G loss: 1.000243]\n",
      "2779 [D loss: 1.002921] [G loss: 0.999836]\n",
      "2780 [D loss: 1.002976] [G loss: 1.000304]\n",
      "2781 [D loss: 1.002946] [G loss: 1.000357]\n",
      "2782 [D loss: 1.002987] [G loss: 1.000157]\n",
      "2783 [D loss: 1.003038] [G loss: 0.999867]\n",
      "2784 [D loss: 1.002906] [G loss: 1.000137]\n",
      "2785 [D loss: 1.003044] [G loss: 0.999969]\n",
      "2786 [D loss: 1.002976] [G loss: 1.000340]\n",
      "2787 [D loss: 1.002913] [G loss: 1.000335]\n",
      "2788 [D loss: 1.002997] [G loss: 1.000181]\n",
      "2789 [D loss: 1.002905] [G loss: 1.000261]\n",
      "2790 [D loss: 1.003102] [G loss: 1.000228]\n",
      "2791 [D loss: 1.003189] [G loss: 1.000321]\n",
      "2792 [D loss: 1.002818] [G loss: 1.000252]\n",
      "2793 [D loss: 1.002973] [G loss: 1.000336]\n",
      "2794 [D loss: 1.002975] [G loss: 1.000482]\n",
      "2795 [D loss: 1.003170] [G loss: 1.000153]\n",
      "2796 [D loss: 1.003003] [G loss: 1.000083]\n",
      "2797 [D loss: 1.002912] [G loss: 1.000318]\n",
      "2798 [D loss: 1.002898] [G loss: 1.000372]\n",
      "2799 [D loss: 1.002886] [G loss: 1.000143]\n",
      "2800 [D loss: 1.002869] [G loss: 1.000104]\n",
      "2801 [D loss: 1.003128] [G loss: 1.000427]\n",
      "2802 [D loss: 1.003023] [G loss: 1.000094]\n",
      "2803 [D loss: 1.002938] [G loss: 1.000159]\n",
      "2804 [D loss: 1.003009] [G loss: 1.000523]\n",
      "2805 [D loss: 1.002909] [G loss: 1.000223]\n",
      "2806 [D loss: 1.003050] [G loss: 1.000440]\n",
      "2807 [D loss: 1.002780] [G loss: 1.000516]\n",
      "2808 [D loss: 1.002998] [G loss: 1.000315]\n",
      "2809 [D loss: 1.003000] [G loss: 1.000483]\n",
      "2810 [D loss: 1.002957] [G loss: 1.000173]\n",
      "2811 [D loss: 1.003037] [G loss: 1.000402]\n",
      "2812 [D loss: 1.003180] [G loss: 1.000260]\n",
      "2813 [D loss: 1.002953] [G loss: 1.000129]\n",
      "2814 [D loss: 1.002935] [G loss: 1.000095]\n",
      "2815 [D loss: 1.002906] [G loss: 1.000207]\n",
      "2816 [D loss: 1.003105] [G loss: 1.000334]\n",
      "2817 [D loss: 1.002861] [G loss: 1.000441]\n",
      "2818 [D loss: 1.003041] [G loss: 1.000010]\n",
      "2819 [D loss: 1.003021] [G loss: 0.999964]\n",
      "2820 [D loss: 1.003095] [G loss: 1.000130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2821 [D loss: 1.002885] [G loss: 1.000108]\n",
      "2822 [D loss: 1.002921] [G loss: 1.000177]\n",
      "2823 [D loss: 1.003055] [G loss: 1.000151]\n",
      "2824 [D loss: 1.003039] [G loss: 1.000216]\n",
      "2825 [D loss: 1.003108] [G loss: 1.000180]\n",
      "2826 [D loss: 1.003005] [G loss: 1.000184]\n",
      "2827 [D loss: 1.002839] [G loss: 1.000089]\n",
      "2828 [D loss: 1.003036] [G loss: 0.999933]\n",
      "2829 [D loss: 1.003106] [G loss: 1.000105]\n",
      "2830 [D loss: 1.002974] [G loss: 1.000298]\n",
      "2831 [D loss: 1.002950] [G loss: 1.000148]\n",
      "2832 [D loss: 1.002877] [G loss: 1.000500]\n",
      "2833 [D loss: 1.003133] [G loss: 1.000059]\n",
      "2834 [D loss: 1.003115] [G loss: 1.000091]\n",
      "2835 [D loss: 1.003016] [G loss: 1.000115]\n",
      "2836 [D loss: 1.002950] [G loss: 1.000125]\n",
      "2837 [D loss: 1.002883] [G loss: 1.000130]\n",
      "2838 [D loss: 1.002889] [G loss: 1.000298]\n",
      "2839 [D loss: 1.002897] [G loss: 0.999926]\n",
      "2840 [D loss: 1.002973] [G loss: 1.000028]\n",
      "2841 [D loss: 1.002923] [G loss: 1.000325]\n",
      "2842 [D loss: 1.003047] [G loss: 1.000028]\n",
      "2843 [D loss: 1.003048] [G loss: 0.999971]\n",
      "2844 [D loss: 1.003085] [G loss: 1.000189]\n",
      "2845 [D loss: 1.003008] [G loss: 1.000035]\n",
      "2846 [D loss: 1.002974] [G loss: 1.000160]\n",
      "2847 [D loss: 1.002934] [G loss: 1.000053]\n",
      "2848 [D loss: 1.002814] [G loss: 1.000202]\n",
      "2849 [D loss: 1.002954] [G loss: 1.000029]\n",
      "2850 [D loss: 1.002799] [G loss: 1.000255]\n",
      "2851 [D loss: 1.002939] [G loss: 1.000213]\n",
      "2852 [D loss: 1.002836] [G loss: 1.000343]\n",
      "2853 [D loss: 1.002853] [G loss: 1.000307]\n",
      "2854 [D loss: 1.002887] [G loss: 1.000095]\n",
      "2855 [D loss: 1.002709] [G loss: 1.000113]\n",
      "2856 [D loss: 1.003042] [G loss: 1.000064]\n",
      "2857 [D loss: 1.002911] [G loss: 0.999941]\n",
      "2858 [D loss: 1.002813] [G loss: 1.000030]\n",
      "2859 [D loss: 1.002816] [G loss: 0.999836]\n",
      "2860 [D loss: 1.003072] [G loss: 1.000012]\n",
      "2861 [D loss: 1.002729] [G loss: 1.000172]\n",
      "2862 [D loss: 1.002821] [G loss: 1.000584]\n",
      "2863 [D loss: 1.003224] [G loss: 1.000208]\n",
      "2864 [D loss: 1.003039] [G loss: 0.999981]\n",
      "2865 [D loss: 1.002926] [G loss: 1.000224]\n",
      "2866 [D loss: 1.002885] [G loss: 1.000359]\n",
      "2867 [D loss: 1.002880] [G loss: 1.000271]\n",
      "2868 [D loss: 1.003087] [G loss: 1.000226]\n",
      "2869 [D loss: 1.003026] [G loss: 1.000245]\n",
      "2870 [D loss: 1.002963] [G loss: 0.999824]\n",
      "2871 [D loss: 1.002938] [G loss: 1.000095]\n",
      "2872 [D loss: 1.003096] [G loss: 1.000485]\n",
      "2873 [D loss: 1.002967] [G loss: 1.000267]\n",
      "2874 [D loss: 1.002991] [G loss: 1.000051]\n",
      "2875 [D loss: 1.002890] [G loss: 0.999998]\n",
      "2876 [D loss: 1.002921] [G loss: 1.000386]\n",
      "2877 [D loss: 1.003116] [G loss: 0.999995]\n",
      "2878 [D loss: 1.003041] [G loss: 1.000057]\n",
      "2879 [D loss: 1.002767] [G loss: 1.000182]\n",
      "2880 [D loss: 1.002824] [G loss: 1.000232]\n",
      "2881 [D loss: 1.002860] [G loss: 1.000172]\n",
      "2882 [D loss: 1.002886] [G loss: 1.000192]\n",
      "2883 [D loss: 1.003043] [G loss: 1.000147]\n",
      "2884 [D loss: 1.002866] [G loss: 1.000283]\n",
      "2885 [D loss: 1.002953] [G loss: 1.000468]\n",
      "2886 [D loss: 1.002972] [G loss: 1.000349]\n",
      "2887 [D loss: 1.002903] [G loss: 0.999977]\n",
      "2888 [D loss: 1.002819] [G loss: 1.000312]\n",
      "2889 [D loss: 1.003057] [G loss: 1.000138]\n",
      "2890 [D loss: 1.002957] [G loss: 0.999984]\n",
      "2891 [D loss: 1.003030] [G loss: 1.000055]\n",
      "2892 [D loss: 1.003073] [G loss: 1.000036]\n",
      "2893 [D loss: 1.003148] [G loss: 0.999870]\n",
      "2894 [D loss: 1.003128] [G loss: 1.000090]\n",
      "2895 [D loss: 1.003131] [G loss: 1.000026]\n",
      "2896 [D loss: 1.002833] [G loss: 0.999992]\n",
      "2897 [D loss: 1.003081] [G loss: 1.000273]\n",
      "2898 [D loss: 1.003037] [G loss: 1.000378]\n",
      "2899 [D loss: 1.003160] [G loss: 1.000268]\n",
      "2900 [D loss: 1.002994] [G loss: 1.000275]\n",
      "2901 [D loss: 1.003012] [G loss: 1.000166]\n",
      "2902 [D loss: 1.002869] [G loss: 1.000129]\n",
      "2903 [D loss: 1.003102] [G loss: 1.000401]\n",
      "2904 [D loss: 1.002943] [G loss: 1.000236]\n",
      "2905 [D loss: 1.003126] [G loss: 1.000278]\n",
      "2906 [D loss: 1.003186] [G loss: 0.999945]\n",
      "2907 [D loss: 1.002786] [G loss: 1.000231]\n",
      "2908 [D loss: 1.003035] [G loss: 1.000369]\n",
      "2909 [D loss: 1.002945] [G loss: 1.000159]\n",
      "2910 [D loss: 1.002937] [G loss: 1.000296]\n",
      "2911 [D loss: 1.003031] [G loss: 1.000041]\n",
      "2912 [D loss: 1.002840] [G loss: 1.000033]\n",
      "2913 [D loss: 1.003045] [G loss: 1.000252]\n",
      "2914 [D loss: 1.002936] [G loss: 1.000042]\n",
      "2915 [D loss: 1.002771] [G loss: 1.000176]\n",
      "2916 [D loss: 1.003023] [G loss: 1.000347]\n",
      "2917 [D loss: 1.002859] [G loss: 1.000075]\n",
      "2918 [D loss: 1.002886] [G loss: 1.000291]\n",
      "2919 [D loss: 1.003025] [G loss: 1.000165]\n",
      "2920 [D loss: 1.002781] [G loss: 1.000279]\n",
      "2921 [D loss: 1.003115] [G loss: 1.000262]\n",
      "2922 [D loss: 1.002978] [G loss: 1.000139]\n",
      "2923 [D loss: 1.003014] [G loss: 1.000213]\n",
      "2924 [D loss: 1.002931] [G loss: 1.000224]\n",
      "2925 [D loss: 1.002884] [G loss: 1.000199]\n",
      "2926 [D loss: 1.002952] [G loss: 1.000031]\n",
      "2927 [D loss: 1.003137] [G loss: 1.000293]\n",
      "2928 [D loss: 1.002790] [G loss: 0.999961]\n",
      "2929 [D loss: 1.003008] [G loss: 1.000048]\n",
      "2930 [D loss: 1.003074] [G loss: 1.000317]\n",
      "2931 [D loss: 1.003021] [G loss: 0.999989]\n",
      "2932 [D loss: 1.002945] [G loss: 1.000181]\n",
      "2933 [D loss: 1.002897] [G loss: 1.000342]\n",
      "2934 [D loss: 1.003098] [G loss: 1.000094]\n",
      "2935 [D loss: 1.002863] [G loss: 1.000108]\n",
      "2936 [D loss: 1.002893] [G loss: 1.000051]\n",
      "2937 [D loss: 1.002964] [G loss: 1.000253]\n",
      "2938 [D loss: 1.002837] [G loss: 1.000346]\n",
      "2939 [D loss: 1.002907] [G loss: 1.000230]\n",
      "2940 [D loss: 1.002912] [G loss: 1.000082]\n",
      "2941 [D loss: 1.003063] [G loss: 1.000467]\n",
      "2942 [D loss: 1.002948] [G loss: 1.000030]\n",
      "2943 [D loss: 1.002944] [G loss: 1.000229]\n",
      "2944 [D loss: 1.002965] [G loss: 1.000327]\n",
      "2945 [D loss: 1.003114] [G loss: 1.000323]\n",
      "2946 [D loss: 1.002847] [G loss: 1.000123]\n",
      "2947 [D loss: 1.002998] [G loss: 0.999860]\n",
      "2948 [D loss: 1.002998] [G loss: 0.999951]\n",
      "2949 [D loss: 1.002966] [G loss: 1.000038]\n",
      "2950 [D loss: 1.002993] [G loss: 1.000246]\n",
      "2951 [D loss: 1.002910] [G loss: 1.000225]\n",
      "2952 [D loss: 1.003022] [G loss: 1.000322]\n",
      "2953 [D loss: 1.002952] [G loss: 1.000375]\n",
      "2954 [D loss: 1.003166] [G loss: 1.000244]\n",
      "2955 [D loss: 1.002887] [G loss: 1.000141]\n",
      "2956 [D loss: 1.002796] [G loss: 1.000167]\n",
      "2957 [D loss: 1.002954] [G loss: 1.000219]\n",
      "2958 [D loss: 1.002952] [G loss: 1.000385]\n",
      "2959 [D loss: 1.003025] [G loss: 1.000158]\n",
      "2960 [D loss: 1.002966] [G loss: 1.000168]\n",
      "2961 [D loss: 1.002992] [G loss: 1.000358]\n",
      "2962 [D loss: 1.003139] [G loss: 1.000387]\n",
      "2963 [D loss: 1.002795] [G loss: 1.000347]\n",
      "2964 [D loss: 1.002938] [G loss: 1.000205]\n",
      "2965 [D loss: 1.003013] [G loss: 1.000223]\n",
      "2966 [D loss: 1.002883] [G loss: 1.000066]\n",
      "2967 [D loss: 1.002774] [G loss: 1.000167]\n",
      "2968 [D loss: 1.002845] [G loss: 1.000319]\n",
      "2969 [D loss: 1.003077] [G loss: 1.000058]\n",
      "2970 [D loss: 1.003073] [G loss: 1.000093]\n",
      "2971 [D loss: 1.002679] [G loss: 1.000259]\n",
      "2972 [D loss: 1.002969] [G loss: 1.000214]\n",
      "2973 [D loss: 1.002994] [G loss: 0.999947]\n",
      "2974 [D loss: 1.003124] [G loss: 1.000124]\n",
      "2975 [D loss: 1.003067] [G loss: 1.000031]\n",
      "2976 [D loss: 1.002987] [G loss: 1.000153]\n",
      "2977 [D loss: 1.002934] [G loss: 1.000149]\n",
      "2978 [D loss: 1.002831] [G loss: 1.000238]\n",
      "2979 [D loss: 1.002859] [G loss: 1.000253]\n",
      "2980 [D loss: 1.002812] [G loss: 1.000190]\n",
      "2981 [D loss: 1.002936] [G loss: 1.000170]\n",
      "2982 [D loss: 1.003026] [G loss: 1.000129]\n",
      "2983 [D loss: 1.002959] [G loss: 1.000301]\n",
      "2984 [D loss: 1.002930] [G loss: 1.000119]\n",
      "2985 [D loss: 1.003227] [G loss: 1.000375]\n",
      "2986 [D loss: 1.003038] [G loss: 1.000480]\n",
      "2987 [D loss: 1.003075] [G loss: 1.000258]\n",
      "2988 [D loss: 1.003090] [G loss: 1.000070]\n",
      "2989 [D loss: 1.002880] [G loss: 1.000143]\n",
      "2990 [D loss: 1.002919] [G loss: 1.000390]\n",
      "2991 [D loss: 1.002787] [G loss: 1.000163]\n",
      "2992 [D loss: 1.002816] [G loss: 0.999827]\n",
      "2993 [D loss: 1.002890] [G loss: 0.999887]\n",
      "2994 [D loss: 1.003106] [G loss: 1.000310]\n",
      "2995 [D loss: 1.003097] [G loss: 1.000161]\n",
      "2996 [D loss: 1.002815] [G loss: 0.999840]\n",
      "2997 [D loss: 1.003038] [G loss: 1.000322]\n",
      "2998 [D loss: 1.002974] [G loss: 1.000203]\n",
      "2999 [D loss: 1.002996] [G loss: 0.999943]\n",
      "3000 [D loss: 1.003046] [G loss: 1.000072]\n",
      "3001 [D loss: 1.002903] [G loss: 1.000221]\n",
      "3002 [D loss: 1.002975] [G loss: 1.000115]\n",
      "3003 [D loss: 1.002854] [G loss: 1.000099]\n",
      "3004 [D loss: 1.002838] [G loss: 1.000109]\n",
      "3005 [D loss: 1.002899] [G loss: 1.000354]\n",
      "3006 [D loss: 1.002991] [G loss: 1.000180]\n",
      "3007 [D loss: 1.002763] [G loss: 1.000208]\n",
      "3008 [D loss: 1.002731] [G loss: 1.000174]\n",
      "3009 [D loss: 1.002902] [G loss: 0.999982]\n",
      "3010 [D loss: 1.003066] [G loss: 1.000231]\n",
      "3011 [D loss: 1.002784] [G loss: 0.999990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3012 [D loss: 1.003025] [G loss: 1.000232]\n",
      "3013 [D loss: 1.002856] [G loss: 1.000178]\n",
      "3014 [D loss: 1.003014] [G loss: 1.000286]\n",
      "3015 [D loss: 1.003088] [G loss: 1.000177]\n",
      "3016 [D loss: 1.003016] [G loss: 1.000220]\n",
      "3017 [D loss: 1.002816] [G loss: 1.000184]\n",
      "3018 [D loss: 1.003001] [G loss: 1.000211]\n",
      "3019 [D loss: 1.002877] [G loss: 1.000031]\n",
      "3020 [D loss: 1.002930] [G loss: 1.000050]\n",
      "3021 [D loss: 1.003089] [G loss: 1.000207]\n",
      "3022 [D loss: 1.002864] [G loss: 1.000004]\n",
      "3023 [D loss: 1.003101] [G loss: 1.000133]\n",
      "3024 [D loss: 1.002941] [G loss: 1.000200]\n",
      "3025 [D loss: 1.003039] [G loss: 1.000192]\n",
      "3026 [D loss: 1.003025] [G loss: 1.000072]\n",
      "3027 [D loss: 1.002829] [G loss: 1.000086]\n",
      "3028 [D loss: 1.003006] [G loss: 1.000126]\n",
      "3029 [D loss: 1.002964] [G loss: 1.000061]\n",
      "3030 [D loss: 1.003079] [G loss: 1.000151]\n",
      "3031 [D loss: 1.002977] [G loss: 1.000181]\n",
      "3032 [D loss: 1.002870] [G loss: 1.000233]\n",
      "3033 [D loss: 1.002973] [G loss: 1.000170]\n",
      "3034 [D loss: 1.003099] [G loss: 1.000299]\n",
      "3035 [D loss: 1.003168] [G loss: 1.000091]\n",
      "3036 [D loss: 1.003034] [G loss: 1.000221]\n",
      "3037 [D loss: 1.002746] [G loss: 1.000249]\n",
      "3038 [D loss: 1.002940] [G loss: 1.000004]\n",
      "3039 [D loss: 1.003051] [G loss: 1.000310]\n",
      "3040 [D loss: 1.003022] [G loss: 1.000094]\n",
      "3041 [D loss: 1.003014] [G loss: 1.000215]\n",
      "3042 [D loss: 1.003058] [G loss: 1.000358]\n",
      "3043 [D loss: 1.003057] [G loss: 1.000040]\n",
      "3044 [D loss: 1.002997] [G loss: 1.000354]\n",
      "3045 [D loss: 1.002910] [G loss: 1.000299]\n",
      "3046 [D loss: 1.003215] [G loss: 1.000197]\n",
      "3047 [D loss: 1.003165] [G loss: 1.000065]\n",
      "3048 [D loss: 1.003030] [G loss: 0.999886]\n",
      "3049 [D loss: 1.003009] [G loss: 1.000345]\n",
      "3050 [D loss: 1.003094] [G loss: 1.000345]\n",
      "3051 [D loss: 1.002909] [G loss: 1.000284]\n",
      "3052 [D loss: 1.003039] [G loss: 1.000222]\n",
      "3053 [D loss: 1.003020] [G loss: 1.000288]\n",
      "3054 [D loss: 1.003067] [G loss: 1.000207]\n",
      "3055 [D loss: 1.002867] [G loss: 1.000172]\n",
      "3056 [D loss: 1.003026] [G loss: 1.000189]\n",
      "3057 [D loss: 1.002978] [G loss: 1.000072]\n",
      "3058 [D loss: 1.002987] [G loss: 1.000057]\n",
      "3059 [D loss: 1.003002] [G loss: 0.999911]\n",
      "3060 [D loss: 1.002897] [G loss: 1.000253]\n",
      "3061 [D loss: 1.003279] [G loss: 0.999994]\n",
      "3062 [D loss: 1.002975] [G loss: 1.000163]\n",
      "3063 [D loss: 1.003088] [G loss: 1.000058]\n",
      "3064 [D loss: 1.003071] [G loss: 1.000210]\n",
      "3065 [D loss: 1.003103] [G loss: 1.000170]\n",
      "3066 [D loss: 1.003065] [G loss: 0.999940]\n",
      "3067 [D loss: 1.002940] [G loss: 1.000173]\n",
      "3068 [D loss: 1.002908] [G loss: 1.000123]\n",
      "3069 [D loss: 1.002783] [G loss: 1.000186]\n",
      "3070 [D loss: 1.003134] [G loss: 1.000158]\n",
      "3071 [D loss: 1.002874] [G loss: 1.000042]\n",
      "3072 [D loss: 1.002880] [G loss: 0.999836]\n",
      "3073 [D loss: 1.002899] [G loss: 1.000111]\n",
      "3074 [D loss: 1.003097] [G loss: 0.999969]\n",
      "3075 [D loss: 1.002965] [G loss: 1.000248]\n",
      "3076 [D loss: 1.002910] [G loss: 1.000210]\n",
      "3077 [D loss: 1.003133] [G loss: 1.000224]\n",
      "3078 [D loss: 1.003070] [G loss: 1.000203]\n",
      "3079 [D loss: 1.002936] [G loss: 1.000408]\n",
      "3080 [D loss: 1.002846] [G loss: 1.000099]\n",
      "3081 [D loss: 1.002849] [G loss: 1.000139]\n",
      "3082 [D loss: 1.003088] [G loss: 1.000360]\n",
      "3083 [D loss: 1.003017] [G loss: 1.000318]\n",
      "3084 [D loss: 1.002928] [G loss: 1.000188]\n",
      "3085 [D loss: 1.003142] [G loss: 1.000242]\n",
      "3086 [D loss: 1.003045] [G loss: 1.000050]\n",
      "3087 [D loss: 1.003008] [G loss: 1.000263]\n",
      "3088 [D loss: 1.003169] [G loss: 0.999918]\n",
      "3089 [D loss: 1.003096] [G loss: 0.999939]\n",
      "3090 [D loss: 1.002923] [G loss: 1.000050]\n",
      "3091 [D loss: 1.002825] [G loss: 1.000036]\n",
      "3092 [D loss: 1.002846] [G loss: 1.000180]\n",
      "3093 [D loss: 1.002751] [G loss: 0.999878]\n",
      "3094 [D loss: 1.003114] [G loss: 0.999994]\n",
      "3095 [D loss: 1.002987] [G loss: 1.000358]\n",
      "3096 [D loss: 1.002775] [G loss: 1.000233]\n",
      "3097 [D loss: 1.002877] [G loss: 1.000250]\n",
      "3098 [D loss: 1.002750] [G loss: 0.999874]\n",
      "3099 [D loss: 1.003155] [G loss: 1.000388]\n",
      "3100 [D loss: 1.002830] [G loss: 1.000085]\n",
      "3101 [D loss: 1.003036] [G loss: 0.999969]\n",
      "3102 [D loss: 1.002898] [G loss: 0.999938]\n",
      "3103 [D loss: 1.002910] [G loss: 1.000162]\n",
      "3104 [D loss: 1.003029] [G loss: 1.000092]\n",
      "3105 [D loss: 1.002881] [G loss: 1.000196]\n",
      "3106 [D loss: 1.003096] [G loss: 1.000151]\n",
      "3107 [D loss: 1.002926] [G loss: 1.000025]\n",
      "3108 [D loss: 1.002957] [G loss: 0.999940]\n",
      "3109 [D loss: 1.003043] [G loss: 1.000184]\n",
      "3110 [D loss: 1.003052] [G loss: 1.000196]\n",
      "3111 [D loss: 1.003149] [G loss: 1.000206]\n",
      "3112 [D loss: 1.002833] [G loss: 1.000490]\n",
      "3113 [D loss: 1.002920] [G loss: 1.000054]\n",
      "3114 [D loss: 1.002953] [G loss: 1.000028]\n",
      "3115 [D loss: 1.003104] [G loss: 1.000021]\n",
      "3116 [D loss: 1.002961] [G loss: 1.000374]\n",
      "3117 [D loss: 1.002992] [G loss: 0.999822]\n",
      "3118 [D loss: 1.002956] [G loss: 1.000180]\n",
      "3119 [D loss: 1.002701] [G loss: 1.000198]\n",
      "3120 [D loss: 1.002896] [G loss: 1.000105]\n",
      "3121 [D loss: 1.002971] [G loss: 1.000128]\n",
      "3122 [D loss: 1.002935] [G loss: 1.000074]\n",
      "3123 [D loss: 1.002830] [G loss: 1.000132]\n",
      "3124 [D loss: 1.003151] [G loss: 1.000157]\n",
      "3125 [D loss: 1.003144] [G loss: 1.000244]\n",
      "3126 [D loss: 1.002994] [G loss: 1.000093]\n",
      "3127 [D loss: 1.002870] [G loss: 1.000335]\n",
      "3128 [D loss: 1.003106] [G loss: 1.000074]\n",
      "3129 [D loss: 1.002926] [G loss: 0.999988]\n",
      "3130 [D loss: 1.003046] [G loss: 1.000184]\n",
      "3131 [D loss: 1.003056] [G loss: 1.000124]\n",
      "3132 [D loss: 1.003002] [G loss: 1.000324]\n",
      "3133 [D loss: 1.003033] [G loss: 1.000311]\n",
      "3134 [D loss: 1.002859] [G loss: 1.000075]\n",
      "3135 [D loss: 1.002897] [G loss: 1.000222]\n",
      "3136 [D loss: 1.002857] [G loss: 1.000196]\n",
      "3137 [D loss: 1.003145] [G loss: 1.000059]\n",
      "3138 [D loss: 1.003011] [G loss: 1.000435]\n",
      "3139 [D loss: 1.003067] [G loss: 1.000239]\n",
      "3140 [D loss: 1.003010] [G loss: 1.000343]\n",
      "3141 [D loss: 1.003112] [G loss: 1.000252]\n",
      "3142 [D loss: 1.002949] [G loss: 1.000205]\n",
      "3143 [D loss: 1.002993] [G loss: 1.000244]\n",
      "3144 [D loss: 1.003096] [G loss: 1.000191]\n",
      "3145 [D loss: 1.002887] [G loss: 1.000193]\n",
      "3146 [D loss: 1.002860] [G loss: 1.000162]\n",
      "3147 [D loss: 1.003034] [G loss: 1.000034]\n",
      "3148 [D loss: 1.002653] [G loss: 1.000223]\n",
      "3149 [D loss: 1.003003] [G loss: 1.000227]\n",
      "3150 [D loss: 1.002997] [G loss: 1.000195]\n",
      "3151 [D loss: 1.002798] [G loss: 1.000000]\n",
      "3152 [D loss: 1.002912] [G loss: 1.000098]\n",
      "3153 [D loss: 1.002918] [G loss: 1.000491]\n",
      "3154 [D loss: 1.002920] [G loss: 1.000225]\n",
      "3155 [D loss: 1.002955] [G loss: 1.000037]\n",
      "3156 [D loss: 1.003080] [G loss: 1.000127]\n",
      "3157 [D loss: 1.003054] [G loss: 0.999779]\n",
      "3158 [D loss: 1.003032] [G loss: 1.000266]\n",
      "3159 [D loss: 1.002996] [G loss: 0.999977]\n",
      "3160 [D loss: 1.002993] [G loss: 1.000304]\n",
      "3161 [D loss: 1.003138] [G loss: 0.999839]\n",
      "3162 [D loss: 1.002941] [G loss: 1.000167]\n",
      "3163 [D loss: 1.002984] [G loss: 1.000216]\n",
      "3164 [D loss: 1.003020] [G loss: 1.000105]\n",
      "3165 [D loss: 1.003034] [G loss: 1.000153]\n",
      "3166 [D loss: 1.003047] [G loss: 0.999984]\n",
      "3167 [D loss: 1.003055] [G loss: 1.000029]\n",
      "3168 [D loss: 1.002950] [G loss: 1.000075]\n",
      "3169 [D loss: 1.002944] [G loss: 1.000064]\n",
      "3170 [D loss: 1.003079] [G loss: 1.000097]\n",
      "3171 [D loss: 1.003105] [G loss: 0.999909]\n",
      "3172 [D loss: 1.003002] [G loss: 1.000103]\n",
      "3173 [D loss: 1.002990] [G loss: 1.000114]\n",
      "3174 [D loss: 1.003070] [G loss: 1.000144]\n",
      "3175 [D loss: 1.003028] [G loss: 1.000139]\n",
      "3176 [D loss: 1.002919] [G loss: 1.000278]\n",
      "3177 [D loss: 1.003120] [G loss: 1.000140]\n",
      "3178 [D loss: 1.003247] [G loss: 1.000356]\n",
      "3179 [D loss: 1.003123] [G loss: 1.000131]\n",
      "3180 [D loss: 1.003075] [G loss: 1.000236]\n",
      "3181 [D loss: 1.003159] [G loss: 1.000413]\n",
      "3182 [D loss: 1.002860] [G loss: 1.000133]\n",
      "3183 [D loss: 1.003033] [G loss: 1.000009]\n",
      "3184 [D loss: 1.002941] [G loss: 1.000013]\n",
      "3185 [D loss: 1.003146] [G loss: 1.000169]\n",
      "3186 [D loss: 1.002768] [G loss: 1.000300]\n",
      "3187 [D loss: 1.003054] [G loss: 1.000108]\n",
      "3188 [D loss: 1.002983] [G loss: 1.000155]\n",
      "3189 [D loss: 1.002943] [G loss: 1.000170]\n",
      "3190 [D loss: 1.002937] [G loss: 1.000043]\n",
      "3191 [D loss: 1.003031] [G loss: 1.000012]\n",
      "3192 [D loss: 1.002955] [G loss: 1.000364]\n",
      "3193 [D loss: 1.002914] [G loss: 1.000052]\n",
      "3194 [D loss: 1.002797] [G loss: 1.000004]\n",
      "3195 [D loss: 1.002862] [G loss: 0.999970]\n",
      "3196 [D loss: 1.002874] [G loss: 1.000157]\n",
      "3197 [D loss: 1.003275] [G loss: 1.000027]\n",
      "3198 [D loss: 1.002948] [G loss: 1.000134]\n",
      "3199 [D loss: 1.003017] [G loss: 0.999986]\n",
      "3200 [D loss: 1.003005] [G loss: 1.000080]\n",
      "3201 [D loss: 1.002901] [G loss: 1.000128]\n",
      "3202 [D loss: 1.002878] [G loss: 1.000010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3203 [D loss: 1.003137] [G loss: 1.000229]\n",
      "3204 [D loss: 1.002974] [G loss: 1.000370]\n",
      "3205 [D loss: 1.002877] [G loss: 0.999957]\n",
      "3206 [D loss: 1.003067] [G loss: 1.000254]\n",
      "3207 [D loss: 1.003007] [G loss: 0.999979]\n",
      "3208 [D loss: 1.003134] [G loss: 1.000227]\n",
      "3209 [D loss: 1.003127] [G loss: 1.000172]\n",
      "3210 [D loss: 1.002855] [G loss: 1.000229]\n",
      "3211 [D loss: 1.002880] [G loss: 1.000081]\n",
      "3212 [D loss: 1.002998] [G loss: 1.000231]\n",
      "3213 [D loss: 1.002825] [G loss: 1.000341]\n",
      "3214 [D loss: 1.002780] [G loss: 1.000097]\n",
      "3215 [D loss: 1.002974] [G loss: 1.000027]\n",
      "3216 [D loss: 1.002904] [G loss: 1.000475]\n",
      "3217 [D loss: 1.002992] [G loss: 1.000293]\n",
      "3218 [D loss: 1.003007] [G loss: 0.999954]\n",
      "3219 [D loss: 1.002911] [G loss: 1.000071]\n",
      "3220 [D loss: 1.002754] [G loss: 1.000088]\n",
      "3221 [D loss: 1.002901] [G loss: 1.000412]\n",
      "3222 [D loss: 1.002882] [G loss: 1.000092]\n",
      "3223 [D loss: 1.003176] [G loss: 1.000159]\n",
      "3224 [D loss: 1.003030] [G loss: 1.000196]\n",
      "3225 [D loss: 1.002977] [G loss: 1.000202]\n",
      "3226 [D loss: 1.002697] [G loss: 0.999999]\n",
      "3227 [D loss: 1.002832] [G loss: 1.000176]\n",
      "3228 [D loss: 1.003024] [G loss: 0.999993]\n",
      "3229 [D loss: 1.003020] [G loss: 1.000177]\n",
      "3230 [D loss: 1.003306] [G loss: 0.999954]\n",
      "3231 [D loss: 1.003202] [G loss: 1.000025]\n",
      "3232 [D loss: 1.002948] [G loss: 1.000182]\n",
      "3233 [D loss: 1.003144] [G loss: 0.999946]\n",
      "3234 [D loss: 1.002958] [G loss: 1.000110]\n",
      "3235 [D loss: 1.002951] [G loss: 1.000257]\n",
      "3236 [D loss: 1.003113] [G loss: 1.000003]\n",
      "3237 [D loss: 1.002918] [G loss: 1.000170]\n",
      "3238 [D loss: 1.002884] [G loss: 1.000171]\n",
      "3239 [D loss: 1.002990] [G loss: 1.000301]\n",
      "3240 [D loss: 1.002845] [G loss: 0.999909]\n",
      "3241 [D loss: 1.003237] [G loss: 1.000370]\n",
      "3242 [D loss: 1.003060] [G loss: 1.000198]\n",
      "3243 [D loss: 1.003038] [G loss: 1.000292]\n",
      "3244 [D loss: 1.002796] [G loss: 1.000155]\n",
      "3245 [D loss: 1.002871] [G loss: 1.000141]\n",
      "3246 [D loss: 1.002931] [G loss: 1.000182]\n",
      "3247 [D loss: 1.002850] [G loss: 1.000330]\n",
      "3248 [D loss: 1.002954] [G loss: 1.000247]\n",
      "3249 [D loss: 1.002904] [G loss: 1.000192]\n",
      "3250 [D loss: 1.002969] [G loss: 1.000243]\n",
      "3251 [D loss: 1.002945] [G loss: 1.000153]\n",
      "3252 [D loss: 1.002890] [G loss: 1.000089]\n",
      "3253 [D loss: 1.002950] [G loss: 1.000038]\n",
      "3254 [D loss: 1.003082] [G loss: 1.000228]\n",
      "3255 [D loss: 1.003097] [G loss: 1.000246]\n",
      "3256 [D loss: 1.002897] [G loss: 1.000230]\n",
      "3257 [D loss: 1.002957] [G loss: 1.000173]\n",
      "3258 [D loss: 1.002985] [G loss: 1.000093]\n",
      "3259 [D loss: 1.002962] [G loss: 1.000305]\n",
      "3260 [D loss: 1.003184] [G loss: 1.000263]\n",
      "3261 [D loss: 1.002878] [G loss: 1.000084]\n",
      "3262 [D loss: 1.002985] [G loss: 1.000440]\n",
      "3263 [D loss: 1.003114] [G loss: 1.000125]\n",
      "3264 [D loss: 1.003074] [G loss: 1.000173]\n",
      "3265 [D loss: 1.002924] [G loss: 1.000075]\n",
      "3266 [D loss: 1.002941] [G loss: 1.000317]\n",
      "3267 [D loss: 1.002846] [G loss: 1.000048]\n",
      "3268 [D loss: 1.003045] [G loss: 1.000161]\n",
      "3269 [D loss: 1.002964] [G loss: 1.000187]\n",
      "3270 [D loss: 1.003017] [G loss: 1.000119]\n",
      "3271 [D loss: 1.002759] [G loss: 1.000108]\n",
      "3272 [D loss: 1.002929] [G loss: 1.000453]\n",
      "3273 [D loss: 1.003003] [G loss: 1.000262]\n",
      "3274 [D loss: 1.002982] [G loss: 0.999975]\n",
      "3275 [D loss: 1.002938] [G loss: 0.999937]\n",
      "3276 [D loss: 1.002793] [G loss: 1.000244]\n",
      "3277 [D loss: 1.002788] [G loss: 1.000141]\n",
      "3278 [D loss: 1.002837] [G loss: 1.000204]\n",
      "3279 [D loss: 1.002837] [G loss: 1.000132]\n",
      "3280 [D loss: 1.002904] [G loss: 1.000151]\n",
      "3281 [D loss: 1.002965] [G loss: 1.000084]\n",
      "3282 [D loss: 1.003021] [G loss: 0.999896]\n",
      "3283 [D loss: 1.002917] [G loss: 1.000182]\n",
      "3284 [D loss: 1.003009] [G loss: 0.999860]\n",
      "3285 [D loss: 1.003097] [G loss: 1.000204]\n",
      "3286 [D loss: 1.002848] [G loss: 1.000339]\n",
      "3287 [D loss: 1.003013] [G loss: 1.000358]\n",
      "3288 [D loss: 1.003144] [G loss: 1.000156]\n",
      "3289 [D loss: 1.003026] [G loss: 1.000158]\n",
      "3290 [D loss: 1.002955] [G loss: 1.000335]\n",
      "3291 [D loss: 1.002954] [G loss: 1.000031]\n",
      "3292 [D loss: 1.002990] [G loss: 1.000373]\n",
      "3293 [D loss: 1.003063] [G loss: 1.000199]\n",
      "3294 [D loss: 1.003193] [G loss: 1.000127]\n",
      "3295 [D loss: 1.002976] [G loss: 1.000530]\n",
      "3296 [D loss: 1.002923] [G loss: 1.000264]\n",
      "3297 [D loss: 1.003015] [G loss: 1.000192]\n",
      "3298 [D loss: 1.003116] [G loss: 1.000231]\n",
      "3299 [D loss: 1.003050] [G loss: 1.000253]\n",
      "3300 [D loss: 1.002942] [G loss: 1.000056]\n",
      "3301 [D loss: 1.003117] [G loss: 1.000112]\n",
      "3302 [D loss: 1.002924] [G loss: 1.000200]\n",
      "3303 [D loss: 1.002680] [G loss: 1.000134]\n",
      "3304 [D loss: 1.002919] [G loss: 1.000171]\n",
      "3305 [D loss: 1.002973] [G loss: 1.000408]\n",
      "3306 [D loss: 1.003061] [G loss: 1.000377]\n",
      "3307 [D loss: 1.003087] [G loss: 1.000498]\n",
      "3308 [D loss: 1.003092] [G loss: 0.999960]\n",
      "3309 [D loss: 1.003161] [G loss: 1.000168]\n",
      "3310 [D loss: 1.003019] [G loss: 1.000115]\n",
      "3311 [D loss: 1.003043] [G loss: 1.000113]\n",
      "3312 [D loss: 1.002868] [G loss: 1.000015]\n",
      "3313 [D loss: 1.003166] [G loss: 1.000270]\n",
      "3314 [D loss: 1.002991] [G loss: 1.000315]\n",
      "3315 [D loss: 1.003044] [G loss: 1.000282]\n",
      "3316 [D loss: 1.002713] [G loss: 1.000210]\n",
      "3317 [D loss: 1.003011] [G loss: 1.000175]\n",
      "3318 [D loss: 1.002940] [G loss: 1.000036]\n",
      "3319 [D loss: 1.003178] [G loss: 1.000105]\n",
      "3320 [D loss: 1.002993] [G loss: 1.000229]\n",
      "3321 [D loss: 1.002904] [G loss: 0.999964]\n",
      "3322 [D loss: 1.002775] [G loss: 1.000214]\n",
      "3323 [D loss: 1.003066] [G loss: 1.000459]\n",
      "3324 [D loss: 1.003076] [G loss: 1.000090]\n",
      "3325 [D loss: 1.002892] [G loss: 1.000044]\n",
      "3326 [D loss: 1.003092] [G loss: 1.000058]\n",
      "3327 [D loss: 1.002969] [G loss: 1.000233]\n",
      "3328 [D loss: 1.002989] [G loss: 1.000279]\n",
      "3329 [D loss: 1.003022] [G loss: 1.000127]\n",
      "3330 [D loss: 1.003020] [G loss: 1.000223]\n",
      "3331 [D loss: 1.002784] [G loss: 0.999997]\n",
      "3332 [D loss: 1.003174] [G loss: 1.000402]\n",
      "3333 [D loss: 1.002966] [G loss: 1.000233]\n",
      "3334 [D loss: 1.002886] [G loss: 1.000200]\n",
      "3335 [D loss: 1.002851] [G loss: 1.000179]\n",
      "3336 [D loss: 1.003099] [G loss: 1.000097]\n",
      "3337 [D loss: 1.002938] [G loss: 1.000377]\n",
      "3338 [D loss: 1.003203] [G loss: 0.999987]\n",
      "3339 [D loss: 1.002992] [G loss: 1.000159]\n",
      "3340 [D loss: 1.002931] [G loss: 1.000121]\n",
      "3341 [D loss: 1.002933] [G loss: 1.000164]\n",
      "3342 [D loss: 1.002901] [G loss: 1.000157]\n",
      "3343 [D loss: 1.003035] [G loss: 1.000205]\n",
      "3344 [D loss: 1.003045] [G loss: 1.000267]\n",
      "3345 [D loss: 1.003111] [G loss: 1.000306]\n",
      "3346 [D loss: 1.002965] [G loss: 1.000178]\n",
      "3347 [D loss: 1.002832] [G loss: 1.000132]\n",
      "3348 [D loss: 1.002930] [G loss: 0.999980]\n",
      "3349 [D loss: 1.002996] [G loss: 1.000273]\n",
      "3350 [D loss: 1.002950] [G loss: 1.000143]\n",
      "3351 [D loss: 1.002961] [G loss: 1.000024]\n",
      "3352 [D loss: 1.003092] [G loss: 1.000058]\n",
      "3353 [D loss: 1.002962] [G loss: 1.000172]\n",
      "3354 [D loss: 1.003017] [G loss: 1.000198]\n",
      "3355 [D loss: 1.002839] [G loss: 1.000179]\n",
      "3356 [D loss: 1.003096] [G loss: 1.000070]\n",
      "3357 [D loss: 1.002977] [G loss: 1.000077]\n",
      "3358 [D loss: 1.002919] [G loss: 1.000240]\n",
      "3359 [D loss: 1.003129] [G loss: 1.000143]\n",
      "3360 [D loss: 1.002927] [G loss: 1.000256]\n",
      "3361 [D loss: 1.003003] [G loss: 1.000272]\n",
      "3362 [D loss: 1.002711] [G loss: 1.000338]\n",
      "3363 [D loss: 1.002985] [G loss: 1.000046]\n",
      "3364 [D loss: 1.002952] [G loss: 1.000273]\n",
      "3365 [D loss: 1.003010] [G loss: 1.000228]\n",
      "3366 [D loss: 1.003129] [G loss: 1.000044]\n",
      "3367 [D loss: 1.003071] [G loss: 1.000279]\n",
      "3368 [D loss: 1.002891] [G loss: 1.000284]\n",
      "3369 [D loss: 1.003157] [G loss: 1.000150]\n",
      "3370 [D loss: 1.003003] [G loss: 1.000474]\n",
      "3371 [D loss: 1.002903] [G loss: 1.000056]\n",
      "3372 [D loss: 1.002967] [G loss: 1.000011]\n",
      "3373 [D loss: 1.003157] [G loss: 1.000146]\n",
      "3374 [D loss: 1.003248] [G loss: 1.000269]\n",
      "3375 [D loss: 1.003099] [G loss: 0.999908]\n",
      "3376 [D loss: 1.002940] [G loss: 1.000137]\n",
      "3377 [D loss: 1.002936] [G loss: 1.000181]\n",
      "3378 [D loss: 1.002999] [G loss: 1.000130]\n",
      "3379 [D loss: 1.002874] [G loss: 1.000037]\n",
      "3380 [D loss: 1.003247] [G loss: 1.000108]\n",
      "3381 [D loss: 1.003053] [G loss: 1.000077]\n",
      "3382 [D loss: 1.003042] [G loss: 1.000020]\n",
      "3383 [D loss: 1.002871] [G loss: 1.000020]\n",
      "3384 [D loss: 1.002984] [G loss: 1.000047]\n",
      "3385 [D loss: 1.002949] [G loss: 1.000074]\n",
      "3386 [D loss: 1.002986] [G loss: 1.000039]\n",
      "3387 [D loss: 1.002952] [G loss: 1.000299]\n",
      "3388 [D loss: 1.003041] [G loss: 1.000384]\n",
      "3389 [D loss: 1.002811] [G loss: 1.000327]\n",
      "3390 [D loss: 1.002957] [G loss: 1.000175]\n",
      "3391 [D loss: 1.002962] [G loss: 1.000330]\n",
      "3392 [D loss: 1.003118] [G loss: 1.000164]\n",
      "3393 [D loss: 1.002866] [G loss: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3394 [D loss: 1.003066] [G loss: 1.000247]\n",
      "3395 [D loss: 1.003105] [G loss: 1.000222]\n",
      "3396 [D loss: 1.003028] [G loss: 1.000013]\n",
      "3397 [D loss: 1.002976] [G loss: 1.000070]\n",
      "3398 [D loss: 1.003100] [G loss: 1.000100]\n",
      "3399 [D loss: 1.002946] [G loss: 0.999907]\n",
      "3400 [D loss: 1.003008] [G loss: 1.000116]\n",
      "3401 [D loss: 1.002870] [G loss: 1.000140]\n",
      "3402 [D loss: 1.003033] [G loss: 1.000420]\n",
      "3403 [D loss: 1.002964] [G loss: 0.999797]\n",
      "3404 [D loss: 1.003132] [G loss: 1.000150]\n",
      "3405 [D loss: 1.002982] [G loss: 1.000261]\n",
      "3406 [D loss: 1.003073] [G loss: 1.000111]\n",
      "3407 [D loss: 1.003017] [G loss: 1.000436]\n",
      "3408 [D loss: 1.002876] [G loss: 1.000250]\n",
      "3409 [D loss: 1.002931] [G loss: 1.000211]\n",
      "3410 [D loss: 1.002978] [G loss: 1.000439]\n",
      "3411 [D loss: 1.002799] [G loss: 1.000465]\n",
      "3412 [D loss: 1.002889] [G loss: 1.000112]\n",
      "3413 [D loss: 1.003053] [G loss: 1.000336]\n",
      "3414 [D loss: 1.002876] [G loss: 1.000236]\n",
      "3415 [D loss: 1.002864] [G loss: 1.000165]\n",
      "3416 [D loss: 1.002847] [G loss: 1.000078]\n",
      "3417 [D loss: 1.003016] [G loss: 0.999988]\n",
      "3418 [D loss: 1.002975] [G loss: 1.000134]\n",
      "3419 [D loss: 1.002814] [G loss: 1.000217]\n",
      "3420 [D loss: 1.002875] [G loss: 1.000178]\n",
      "3421 [D loss: 1.002989] [G loss: 0.999925]\n",
      "3422 [D loss: 1.002955] [G loss: 1.000207]\n",
      "3423 [D loss: 1.002863] [G loss: 1.000234]\n",
      "3424 [D loss: 1.002935] [G loss: 1.000373]\n",
      "3425 [D loss: 1.002967] [G loss: 1.000190]\n",
      "3426 [D loss: 1.002884] [G loss: 1.000253]\n",
      "3427 [D loss: 1.002859] [G loss: 1.000414]\n",
      "3428 [D loss: 1.002866] [G loss: 1.000059]\n",
      "3429 [D loss: 1.003036] [G loss: 1.000101]\n",
      "3430 [D loss: 1.002978] [G loss: 1.000178]\n",
      "3431 [D loss: 1.002942] [G loss: 1.000027]\n",
      "3432 [D loss: 1.003041] [G loss: 1.000110]\n",
      "3433 [D loss: 1.003234] [G loss: 1.000118]\n",
      "3434 [D loss: 1.003208] [G loss: 1.000174]\n",
      "3435 [D loss: 1.002773] [G loss: 1.000024]\n",
      "3436 [D loss: 1.002935] [G loss: 1.000220]\n",
      "3437 [D loss: 1.003001] [G loss: 1.000143]\n",
      "3438 [D loss: 1.002945] [G loss: 1.000277]\n",
      "3439 [D loss: 1.003092] [G loss: 1.000248]\n",
      "3440 [D loss: 1.002942] [G loss: 1.000162]\n",
      "3441 [D loss: 1.002778] [G loss: 1.000046]\n",
      "3442 [D loss: 1.003055] [G loss: 1.000134]\n",
      "3443 [D loss: 1.002828] [G loss: 1.000126]\n",
      "3444 [D loss: 1.003013] [G loss: 0.999952]\n",
      "3445 [D loss: 1.002995] [G loss: 1.000147]\n",
      "3446 [D loss: 1.003053] [G loss: 1.000279]\n",
      "3447 [D loss: 1.003048] [G loss: 1.000309]\n",
      "3448 [D loss: 1.003032] [G loss: 1.000408]\n",
      "3449 [D loss: 1.003112] [G loss: 1.000264]\n",
      "3450 [D loss: 1.003027] [G loss: 1.000228]\n",
      "3451 [D loss: 1.002913] [G loss: 1.000225]\n",
      "3452 [D loss: 1.002989] [G loss: 1.000217]\n",
      "3453 [D loss: 1.002962] [G loss: 1.000227]\n",
      "3454 [D loss: 1.002930] [G loss: 1.000132]\n",
      "3455 [D loss: 1.002842] [G loss: 1.000029]\n",
      "3456 [D loss: 1.002962] [G loss: 1.000218]\n",
      "3457 [D loss: 1.003133] [G loss: 1.000143]\n",
      "3458 [D loss: 1.002866] [G loss: 1.000019]\n",
      "3459 [D loss: 1.002843] [G loss: 1.000429]\n",
      "3460 [D loss: 1.002884] [G loss: 0.999950]\n",
      "3461 [D loss: 1.002900] [G loss: 1.000083]\n",
      "3462 [D loss: 1.003103] [G loss: 1.000083]\n",
      "3463 [D loss: 1.002876] [G loss: 1.000037]\n",
      "3464 [D loss: 1.003018] [G loss: 1.000186]\n",
      "3465 [D loss: 1.002944] [G loss: 1.000096]\n",
      "3466 [D loss: 1.002984] [G loss: 1.000418]\n",
      "3467 [D loss: 1.003067] [G loss: 1.000099]\n",
      "3468 [D loss: 1.002739] [G loss: 1.000317]\n",
      "3469 [D loss: 1.003172] [G loss: 0.999966]\n",
      "3470 [D loss: 1.003100] [G loss: 1.000126]\n",
      "3471 [D loss: 1.002937] [G loss: 1.000413]\n",
      "3472 [D loss: 1.003016] [G loss: 1.000099]\n",
      "3473 [D loss: 1.003038] [G loss: 1.000238]\n",
      "3474 [D loss: 1.002987] [G loss: 1.000138]\n",
      "3475 [D loss: 1.003099] [G loss: 1.000122]\n",
      "3476 [D loss: 1.003015] [G loss: 1.000204]\n",
      "3477 [D loss: 1.003032] [G loss: 1.000036]\n",
      "3478 [D loss: 1.003121] [G loss: 1.000000]\n",
      "3479 [D loss: 1.003045] [G loss: 1.000326]\n",
      "3480 [D loss: 1.003027] [G loss: 1.000214]\n",
      "3481 [D loss: 1.003047] [G loss: 0.999776]\n",
      "3482 [D loss: 1.002850] [G loss: 1.000261]\n",
      "3483 [D loss: 1.003067] [G loss: 0.999910]\n",
      "3484 [D loss: 1.002955] [G loss: 0.999963]\n",
      "3485 [D loss: 1.002877] [G loss: 1.000163]\n",
      "3486 [D loss: 1.003052] [G loss: 1.000476]\n",
      "3487 [D loss: 1.002981] [G loss: 1.000259]\n",
      "3488 [D loss: 1.002818] [G loss: 1.000051]\n",
      "3489 [D loss: 1.002904] [G loss: 1.000144]\n",
      "3490 [D loss: 1.003071] [G loss: 1.000097]\n",
      "3491 [D loss: 1.003074] [G loss: 1.000191]\n",
      "3492 [D loss: 1.002767] [G loss: 1.000249]\n",
      "3493 [D loss: 1.002930] [G loss: 1.000470]\n",
      "3494 [D loss: 1.002860] [G loss: 1.000181]\n",
      "3495 [D loss: 1.003102] [G loss: 1.000353]\n",
      "3496 [D loss: 1.003123] [G loss: 1.000068]\n",
      "3497 [D loss: 1.003028] [G loss: 1.000172]\n",
      "3498 [D loss: 1.003023] [G loss: 1.000247]\n",
      "3499 [D loss: 1.002970] [G loss: 1.000027]\n",
      "3500 [D loss: 1.002938] [G loss: 1.000506]\n",
      "3501 [D loss: 1.003112] [G loss: 1.000316]\n",
      "3502 [D loss: 1.003088] [G loss: 0.999957]\n",
      "3503 [D loss: 1.002889] [G loss: 0.999977]\n",
      "3504 [D loss: 1.003027] [G loss: 1.000272]\n",
      "3505 [D loss: 1.002848] [G loss: 1.000355]\n",
      "3506 [D loss: 1.003088] [G loss: 1.000022]\n",
      "3507 [D loss: 1.002929] [G loss: 1.000299]\n",
      "3508 [D loss: 1.003121] [G loss: 1.000242]\n",
      "3509 [D loss: 1.002859] [G loss: 0.999977]\n",
      "3510 [D loss: 1.003133] [G loss: 1.000136]\n",
      "3511 [D loss: 1.002990] [G loss: 1.000396]\n",
      "3512 [D loss: 1.002856] [G loss: 1.000066]\n",
      "3513 [D loss: 1.003024] [G loss: 1.000162]\n",
      "3514 [D loss: 1.002954] [G loss: 1.000280]\n",
      "3515 [D loss: 1.003036] [G loss: 1.000147]\n",
      "3516 [D loss: 1.002934] [G loss: 1.000084]\n",
      "3517 [D loss: 1.002974] [G loss: 1.000257]\n",
      "3518 [D loss: 1.003037] [G loss: 1.000098]\n",
      "3519 [D loss: 1.002940] [G loss: 1.000274]\n",
      "3520 [D loss: 1.003065] [G loss: 1.000222]\n",
      "3521 [D loss: 1.003061] [G loss: 1.000165]\n",
      "3522 [D loss: 1.002880] [G loss: 1.000336]\n",
      "3523 [D loss: 1.002991] [G loss: 1.000193]\n",
      "3524 [D loss: 1.002696] [G loss: 1.000216]\n",
      "3525 [D loss: 1.003020] [G loss: 1.000096]\n",
      "3526 [D loss: 1.003091] [G loss: 1.000103]\n",
      "3527 [D loss: 1.003273] [G loss: 1.000166]\n",
      "3528 [D loss: 1.002936] [G loss: 1.000033]\n",
      "3529 [D loss: 1.003065] [G loss: 0.999815]\n",
      "3530 [D loss: 1.003003] [G loss: 1.000201]\n",
      "3531 [D loss: 1.003003] [G loss: 1.000194]\n",
      "3532 [D loss: 1.002882] [G loss: 1.000066]\n",
      "3533 [D loss: 1.003254] [G loss: 1.000118]\n",
      "3534 [D loss: 1.002993] [G loss: 1.000338]\n",
      "3535 [D loss: 1.003066] [G loss: 1.000023]\n",
      "3536 [D loss: 1.003192] [G loss: 0.999893]\n",
      "3537 [D loss: 1.003066] [G loss: 1.000239]\n",
      "3538 [D loss: 1.003061] [G loss: 1.000151]\n",
      "3539 [D loss: 1.002980] [G loss: 1.000189]\n",
      "3540 [D loss: 1.003029] [G loss: 1.000234]\n",
      "3541 [D loss: 1.002929] [G loss: 1.000594]\n",
      "3542 [D loss: 1.002917] [G loss: 1.000211]\n",
      "3543 [D loss: 1.003119] [G loss: 0.999949]\n",
      "3544 [D loss: 1.003169] [G loss: 1.000415]\n",
      "3545 [D loss: 1.002979] [G loss: 1.000040]\n",
      "3546 [D loss: 1.002947] [G loss: 1.000102]\n",
      "3547 [D loss: 1.003121] [G loss: 1.000178]\n",
      "3548 [D loss: 1.002960] [G loss: 0.999958]\n",
      "3549 [D loss: 1.002895] [G loss: 1.000338]\n",
      "3550 [D loss: 1.002962] [G loss: 1.000176]\n",
      "3551 [D loss: 1.003038] [G loss: 1.000184]\n",
      "3552 [D loss: 1.003060] [G loss: 1.000217]\n",
      "3553 [D loss: 1.002907] [G loss: 1.000246]\n",
      "3554 [D loss: 1.003020] [G loss: 1.000180]\n",
      "3555 [D loss: 1.003066] [G loss: 0.999982]\n",
      "3556 [D loss: 1.002872] [G loss: 1.000012]\n",
      "3557 [D loss: 1.002901] [G loss: 1.000036]\n",
      "3558 [D loss: 1.003079] [G loss: 1.000089]\n",
      "3559 [D loss: 1.003018] [G loss: 1.000069]\n",
      "3560 [D loss: 1.002932] [G loss: 1.000198]\n",
      "3561 [D loss: 1.003023] [G loss: 1.000026]\n",
      "3562 [D loss: 1.003115] [G loss: 1.000173]\n",
      "3563 [D loss: 1.003096] [G loss: 1.000191]\n",
      "3564 [D loss: 1.003092] [G loss: 1.000258]\n",
      "3565 [D loss: 1.003037] [G loss: 1.000169]\n",
      "3566 [D loss: 1.002670] [G loss: 1.000195]\n",
      "3567 [D loss: 1.002991] [G loss: 1.000117]\n",
      "3568 [D loss: 1.002898] [G loss: 1.000030]\n",
      "3569 [D loss: 1.002881] [G loss: 1.000322]\n",
      "3570 [D loss: 1.003140] [G loss: 1.000040]\n",
      "3571 [D loss: 1.002836] [G loss: 1.000134]\n",
      "3572 [D loss: 1.002960] [G loss: 1.000057]\n",
      "3573 [D loss: 1.002828] [G loss: 1.000082]\n",
      "3574 [D loss: 1.003013] [G loss: 1.000334]\n",
      "3575 [D loss: 1.002888] [G loss: 1.000061]\n",
      "3576 [D loss: 1.003141] [G loss: 1.000200]\n",
      "3577 [D loss: 1.002950] [G loss: 1.000044]\n",
      "3578 [D loss: 1.002965] [G loss: 0.999917]\n",
      "3579 [D loss: 1.003079] [G loss: 0.999862]\n",
      "3580 [D loss: 1.002834] [G loss: 1.000230]\n",
      "3581 [D loss: 1.003176] [G loss: 1.000080]\n",
      "3582 [D loss: 1.003067] [G loss: 1.000179]\n",
      "3583 [D loss: 1.002902] [G loss: 1.000316]\n",
      "3584 [D loss: 1.002962] [G loss: 0.999993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3585 [D loss: 1.002775] [G loss: 1.000412]\n",
      "3586 [D loss: 1.002952] [G loss: 1.000005]\n",
      "3587 [D loss: 1.003047] [G loss: 0.999941]\n",
      "3588 [D loss: 1.002889] [G loss: 1.000326]\n",
      "3589 [D loss: 1.003022] [G loss: 1.000262]\n",
      "3590 [D loss: 1.003091] [G loss: 1.000089]\n",
      "3591 [D loss: 1.003053] [G loss: 1.000224]\n",
      "3592 [D loss: 1.002639] [G loss: 0.999821]\n",
      "3593 [D loss: 1.002774] [G loss: 1.000074]\n",
      "3594 [D loss: 1.002765] [G loss: 1.000119]\n",
      "3595 [D loss: 1.003086] [G loss: 1.000177]\n",
      "3596 [D loss: 1.003251] [G loss: 0.999881]\n",
      "3597 [D loss: 1.003103] [G loss: 1.000093]\n",
      "3598 [D loss: 1.002986] [G loss: 1.000254]\n",
      "3599 [D loss: 1.002902] [G loss: 1.000019]\n",
      "3600 [D loss: 1.003007] [G loss: 1.000129]\n",
      "3601 [D loss: 1.003088] [G loss: 0.999916]\n",
      "3602 [D loss: 1.002886] [G loss: 1.000030]\n",
      "3603 [D loss: 1.003197] [G loss: 1.000147]\n",
      "3604 [D loss: 1.002882] [G loss: 1.000325]\n",
      "3605 [D loss: 1.003267] [G loss: 1.000069]\n",
      "3606 [D loss: 1.002878] [G loss: 1.000012]\n",
      "3607 [D loss: 1.002923] [G loss: 1.000071]\n",
      "3608 [D loss: 1.003031] [G loss: 1.000056]\n",
      "3609 [D loss: 1.002848] [G loss: 1.000126]\n",
      "3610 [D loss: 1.002918] [G loss: 1.000041]\n",
      "3611 [D loss: 1.002900] [G loss: 1.000037]\n",
      "3612 [D loss: 1.003202] [G loss: 1.000038]\n",
      "3613 [D loss: 1.003071] [G loss: 1.000028]\n",
      "3614 [D loss: 1.003103] [G loss: 1.000128]\n",
      "3615 [D loss: 1.002703] [G loss: 1.000170]\n",
      "3616 [D loss: 1.003023] [G loss: 1.000116]\n",
      "3617 [D loss: 1.003065] [G loss: 0.999865]\n",
      "3618 [D loss: 1.003208] [G loss: 1.000256]\n",
      "3619 [D loss: 1.003079] [G loss: 0.999989]\n",
      "3620 [D loss: 1.002938] [G loss: 1.000134]\n",
      "3621 [D loss: 1.002852] [G loss: 1.000153]\n",
      "3622 [D loss: 1.003192] [G loss: 1.000159]\n",
      "3623 [D loss: 1.003020] [G loss: 1.000219]\n",
      "3624 [D loss: 1.003176] [G loss: 1.000078]\n",
      "3625 [D loss: 1.003006] [G loss: 1.000142]\n",
      "3626 [D loss: 1.003054] [G loss: 1.000070]\n",
      "3627 [D loss: 1.002802] [G loss: 1.000082]\n",
      "3628 [D loss: 1.003024] [G loss: 0.999873]\n",
      "3629 [D loss: 1.002984] [G loss: 1.000082]\n",
      "3630 [D loss: 1.002779] [G loss: 1.000049]\n",
      "3631 [D loss: 1.003011] [G loss: 1.000148]\n",
      "3632 [D loss: 1.002918] [G loss: 1.000079]\n",
      "3633 [D loss: 1.002816] [G loss: 0.999972]\n",
      "3634 [D loss: 1.003117] [G loss: 1.000283]\n",
      "3635 [D loss: 1.003030] [G loss: 1.000014]\n",
      "3636 [D loss: 1.003159] [G loss: 0.999987]\n",
      "3637 [D loss: 1.002915] [G loss: 1.000042]\n",
      "3638 [D loss: 1.003030] [G loss: 1.000069]\n",
      "3639 [D loss: 1.002843] [G loss: 1.000084]\n",
      "3640 [D loss: 1.003127] [G loss: 0.999942]\n",
      "3641 [D loss: 1.002987] [G loss: 1.000117]\n",
      "3642 [D loss: 1.003002] [G loss: 1.000234]\n",
      "3643 [D loss: 1.003144] [G loss: 0.999860]\n",
      "3644 [D loss: 1.002853] [G loss: 1.000023]\n",
      "3645 [D loss: 1.003076] [G loss: 0.999876]\n",
      "3646 [D loss: 1.002819] [G loss: 1.000145]\n",
      "3647 [D loss: 1.002990] [G loss: 0.999919]\n",
      "3648 [D loss: 1.002811] [G loss: 1.000189]\n",
      "3649 [D loss: 1.002951] [G loss: 0.999979]\n",
      "3650 [D loss: 1.002947] [G loss: 1.000077]\n",
      "3651 [D loss: 1.002999] [G loss: 1.000278]\n",
      "3652 [D loss: 1.003154] [G loss: 0.999958]\n",
      "3653 [D loss: 1.002909] [G loss: 1.000274]\n",
      "3654 [D loss: 1.003041] [G loss: 0.999892]\n",
      "3655 [D loss: 1.002872] [G loss: 1.000164]\n",
      "3656 [D loss: 1.002987] [G loss: 1.000090]\n",
      "3657 [D loss: 1.003091] [G loss: 1.000148]\n",
      "3658 [D loss: 1.002938] [G loss: 1.000440]\n",
      "3659 [D loss: 1.003111] [G loss: 0.999881]\n",
      "3660 [D loss: 1.002945] [G loss: 1.000167]\n",
      "3661 [D loss: 1.003010] [G loss: 1.000009]\n",
      "3662 [D loss: 1.003021] [G loss: 0.999817]\n",
      "3663 [D loss: 1.003025] [G loss: 0.999920]\n",
      "3664 [D loss: 1.003002] [G loss: 0.999979]\n",
      "3665 [D loss: 1.003085] [G loss: 1.000184]\n",
      "3666 [D loss: 1.002832] [G loss: 1.000028]\n",
      "3667 [D loss: 1.002978] [G loss: 0.999971]\n",
      "3668 [D loss: 1.002861] [G loss: 1.000002]\n",
      "3669 [D loss: 1.003055] [G loss: 0.999960]\n",
      "3670 [D loss: 1.002897] [G loss: 1.000344]\n",
      "3671 [D loss: 1.002884] [G loss: 1.000068]\n",
      "3672 [D loss: 1.003015] [G loss: 1.000398]\n",
      "3673 [D loss: 1.002921] [G loss: 1.000149]\n",
      "3674 [D loss: 1.003346] [G loss: 0.999945]\n",
      "3675 [D loss: 1.002834] [G loss: 1.000138]\n",
      "3676 [D loss: 1.003160] [G loss: 1.000204]\n",
      "3677 [D loss: 1.003108] [G loss: 1.000078]\n",
      "3678 [D loss: 1.002975] [G loss: 1.000331]\n",
      "3679 [D loss: 1.003184] [G loss: 1.000037]\n",
      "3680 [D loss: 1.003057] [G loss: 1.000135]\n",
      "3681 [D loss: 1.003085] [G loss: 1.000099]\n",
      "3682 [D loss: 1.002974] [G loss: 1.000107]\n",
      "3683 [D loss: 1.002950] [G loss: 1.000246]\n",
      "3684 [D loss: 1.003046] [G loss: 0.999933]\n",
      "3685 [D loss: 1.002986] [G loss: 0.999903]\n",
      "3686 [D loss: 1.002882] [G loss: 1.000055]\n",
      "3687 [D loss: 1.002977] [G loss: 1.000380]\n",
      "3688 [D loss: 1.002972] [G loss: 1.000093]\n",
      "3689 [D loss: 1.002980] [G loss: 1.000063]\n",
      "3690 [D loss: 1.002956] [G loss: 1.000150]\n",
      "3691 [D loss: 1.003034] [G loss: 1.000195]\n",
      "3692 [D loss: 1.002859] [G loss: 1.000312]\n",
      "3693 [D loss: 1.002960] [G loss: 1.000124]\n",
      "3694 [D loss: 1.003277] [G loss: 1.000120]\n",
      "3695 [D loss: 1.002676] [G loss: 1.000158]\n",
      "3696 [D loss: 1.003177] [G loss: 1.000284]\n",
      "3697 [D loss: 1.002817] [G loss: 1.000479]\n",
      "3698 [D loss: 1.002753] [G loss: 1.000313]\n",
      "3699 [D loss: 1.002969] [G loss: 1.000060]\n",
      "3700 [D loss: 1.002791] [G loss: 1.000296]\n",
      "3701 [D loss: 1.002892] [G loss: 1.000245]\n",
      "3702 [D loss: 1.002925] [G loss: 0.999923]\n",
      "3703 [D loss: 1.002963] [G loss: 1.000310]\n",
      "3704 [D loss: 1.002977] [G loss: 1.000101]\n",
      "3705 [D loss: 1.003080] [G loss: 1.000129]\n",
      "3706 [D loss: 1.003044] [G loss: 1.000286]\n",
      "3707 [D loss: 1.003109] [G loss: 1.000230]\n",
      "3708 [D loss: 1.003028] [G loss: 1.000237]\n",
      "3709 [D loss: 1.002976] [G loss: 1.000136]\n",
      "3710 [D loss: 1.003003] [G loss: 1.000412]\n",
      "3711 [D loss: 1.002867] [G loss: 1.000072]\n",
      "3712 [D loss: 1.002968] [G loss: 0.999929]\n",
      "3713 [D loss: 1.003047] [G loss: 1.000429]\n",
      "3714 [D loss: 1.002806] [G loss: 0.999998]\n",
      "3715 [D loss: 1.003074] [G loss: 1.000007]\n",
      "3716 [D loss: 1.003029] [G loss: 1.000193]\n",
      "3717 [D loss: 1.002829] [G loss: 1.000050]\n",
      "3718 [D loss: 1.003063] [G loss: 1.000087]\n",
      "3719 [D loss: 1.002906] [G loss: 1.000299]\n",
      "3720 [D loss: 1.002913] [G loss: 1.000125]\n",
      "3721 [D loss: 1.002838] [G loss: 1.000101]\n",
      "3722 [D loss: 1.003043] [G loss: 1.000065]\n",
      "3723 [D loss: 1.003194] [G loss: 1.000096]\n",
      "3724 [D loss: 1.002982] [G loss: 1.000171]\n",
      "3725 [D loss: 1.003145] [G loss: 1.000177]\n",
      "3726 [D loss: 1.003043] [G loss: 0.999884]\n",
      "3727 [D loss: 1.002952] [G loss: 1.000268]\n",
      "3728 [D loss: 1.002916] [G loss: 1.000143]\n",
      "3729 [D loss: 1.002943] [G loss: 1.000206]\n",
      "3730 [D loss: 1.003119] [G loss: 1.000084]\n",
      "3731 [D loss: 1.003107] [G loss: 0.999960]\n",
      "3732 [D loss: 1.003161] [G loss: 0.999981]\n",
      "3733 [D loss: 1.003003] [G loss: 1.000185]\n",
      "3734 [D loss: 1.002973] [G loss: 1.000107]\n",
      "3735 [D loss: 1.003058] [G loss: 1.000011]\n",
      "3736 [D loss: 1.002948] [G loss: 1.000286]\n",
      "3737 [D loss: 1.002957] [G loss: 1.000315]\n",
      "3738 [D loss: 1.002847] [G loss: 1.000151]\n",
      "3739 [D loss: 1.003077] [G loss: 1.000262]\n",
      "3740 [D loss: 1.002835] [G loss: 1.000267]\n",
      "3741 [D loss: 1.002859] [G loss: 1.000278]\n",
      "3742 [D loss: 1.003197] [G loss: 1.000260]\n",
      "3743 [D loss: 1.003172] [G loss: 0.999920]\n",
      "3744 [D loss: 1.003120] [G loss: 1.000083]\n",
      "3745 [D loss: 1.003037] [G loss: 1.000325]\n",
      "3746 [D loss: 1.002853] [G loss: 1.000155]\n",
      "3747 [D loss: 1.003003] [G loss: 1.000028]\n",
      "3748 [D loss: 1.002902] [G loss: 0.999933]\n",
      "3749 [D loss: 1.002999] [G loss: 1.000279]\n",
      "3750 [D loss: 1.002865] [G loss: 1.000044]\n",
      "3751 [D loss: 1.002884] [G loss: 1.000452]\n",
      "3752 [D loss: 1.002924] [G loss: 0.999962]\n",
      "3753 [D loss: 1.003166] [G loss: 0.999969]\n",
      "3754 [D loss: 1.002994] [G loss: 1.000293]\n",
      "3755 [D loss: 1.003127] [G loss: 1.000271]\n",
      "3756 [D loss: 1.002995] [G loss: 0.999946]\n",
      "3757 [D loss: 1.002980] [G loss: 1.000122]\n",
      "3758 [D loss: 1.002991] [G loss: 1.000423]\n",
      "3759 [D loss: 1.002940] [G loss: 0.999736]\n",
      "3760 [D loss: 1.003010] [G loss: 0.999919]\n",
      "3761 [D loss: 1.003077] [G loss: 1.000186]\n",
      "3762 [D loss: 1.003004] [G loss: 1.000255]\n",
      "3763 [D loss: 1.002819] [G loss: 1.000038]\n",
      "3764 [D loss: 1.002974] [G loss: 0.999986]\n",
      "3765 [D loss: 1.002834] [G loss: 1.000097]\n",
      "3766 [D loss: 1.002845] [G loss: 1.000165]\n",
      "3767 [D loss: 1.003030] [G loss: 1.000220]\n",
      "3768 [D loss: 1.003111] [G loss: 1.000261]\n",
      "3769 [D loss: 1.003012] [G loss: 1.000314]\n",
      "3770 [D loss: 1.003127] [G loss: 1.000185]\n",
      "3771 [D loss: 1.003104] [G loss: 1.000295]\n",
      "3772 [D loss: 1.003177] [G loss: 1.000151]\n",
      "3773 [D loss: 1.002633] [G loss: 1.000288]\n",
      "3774 [D loss: 1.002922] [G loss: 0.999861]\n",
      "3775 [D loss: 1.003175] [G loss: 1.000156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3776 [D loss: 1.003071] [G loss: 1.000209]\n",
      "3777 [D loss: 1.002934] [G loss: 1.000108]\n",
      "3778 [D loss: 1.002960] [G loss: 1.000075]\n",
      "3779 [D loss: 1.003078] [G loss: 1.000207]\n",
      "3780 [D loss: 1.003025] [G loss: 1.000212]\n",
      "3781 [D loss: 1.002907] [G loss: 1.000158]\n",
      "3782 [D loss: 1.003001] [G loss: 1.000029]\n",
      "3783 [D loss: 1.002995] [G loss: 1.000382]\n",
      "3784 [D loss: 1.002946] [G loss: 1.000029]\n",
      "3785 [D loss: 1.002949] [G loss: 1.000274]\n",
      "3786 [D loss: 1.003020] [G loss: 1.000029]\n",
      "3787 [D loss: 1.002992] [G loss: 1.000264]\n",
      "3788 [D loss: 1.002944] [G loss: 1.000408]\n",
      "3789 [D loss: 1.002927] [G loss: 0.999972]\n",
      "3790 [D loss: 1.003154] [G loss: 1.000209]\n",
      "3791 [D loss: 1.002923] [G loss: 1.000215]\n",
      "3792 [D loss: 1.002790] [G loss: 1.000048]\n",
      "3793 [D loss: 1.002980] [G loss: 1.000060]\n",
      "3794 [D loss: 1.003013] [G loss: 1.000191]\n",
      "3795 [D loss: 1.003264] [G loss: 1.000390]\n",
      "3796 [D loss: 1.003095] [G loss: 1.000323]\n",
      "3797 [D loss: 1.002873] [G loss: 0.999944]\n",
      "3798 [D loss: 1.002709] [G loss: 1.000162]\n",
      "3799 [D loss: 1.003075] [G loss: 1.000030]\n",
      "3800 [D loss: 1.002944] [G loss: 1.000208]\n",
      "3801 [D loss: 1.003065] [G loss: 1.000303]\n",
      "3802 [D loss: 1.002847] [G loss: 1.000169]\n",
      "3803 [D loss: 1.003143] [G loss: 1.000035]\n",
      "3804 [D loss: 1.002820] [G loss: 1.000182]\n",
      "3805 [D loss: 1.002867] [G loss: 1.000224]\n",
      "3806 [D loss: 1.003058] [G loss: 1.000251]\n",
      "3807 [D loss: 1.003071] [G loss: 1.000008]\n",
      "3808 [D loss: 1.003148] [G loss: 1.000155]\n",
      "3809 [D loss: 1.002808] [G loss: 1.000131]\n",
      "3810 [D loss: 1.002960] [G loss: 0.999880]\n",
      "3811 [D loss: 1.003071] [G loss: 0.999868]\n",
      "3812 [D loss: 1.003057] [G loss: 0.999952]\n",
      "3813 [D loss: 1.002869] [G loss: 1.000015]\n",
      "3814 [D loss: 1.003273] [G loss: 1.000279]\n",
      "3815 [D loss: 1.003048] [G loss: 1.000030]\n",
      "3816 [D loss: 1.002890] [G loss: 1.000040]\n",
      "3817 [D loss: 1.002984] [G loss: 1.000180]\n",
      "3818 [D loss: 1.003078] [G loss: 0.999970]\n",
      "3819 [D loss: 1.003131] [G loss: 1.000010]\n",
      "3820 [D loss: 1.003021] [G loss: 0.999967]\n",
      "3821 [D loss: 1.003205] [G loss: 1.000339]\n",
      "3822 [D loss: 1.003058] [G loss: 1.000159]\n",
      "3823 [D loss: 1.003022] [G loss: 1.000083]\n",
      "3824 [D loss: 1.002949] [G loss: 1.000034]\n",
      "3825 [D loss: 1.003008] [G loss: 1.000310]\n",
      "3826 [D loss: 1.002979] [G loss: 1.000209]\n",
      "3827 [D loss: 1.002975] [G loss: 1.000008]\n",
      "3828 [D loss: 1.003223] [G loss: 1.000199]\n",
      "3829 [D loss: 1.002982] [G loss: 1.000322]\n",
      "3830 [D loss: 1.003248] [G loss: 1.000060]\n",
      "3831 [D loss: 1.002976] [G loss: 1.000092]\n",
      "3832 [D loss: 1.003011] [G loss: 0.999767]\n",
      "3833 [D loss: 1.003022] [G loss: 1.000238]\n",
      "3834 [D loss: 1.002820] [G loss: 0.999993]\n",
      "3835 [D loss: 1.003006] [G loss: 1.000287]\n",
      "3836 [D loss: 1.003246] [G loss: 1.000117]\n",
      "3837 [D loss: 1.002979] [G loss: 1.000134]\n",
      "3838 [D loss: 1.003080] [G loss: 1.000033]\n",
      "3839 [D loss: 1.003014] [G loss: 0.999886]\n",
      "3840 [D loss: 1.003058] [G loss: 1.000318]\n",
      "3841 [D loss: 1.003027] [G loss: 1.000090]\n",
      "3842 [D loss: 1.002978] [G loss: 1.000153]\n",
      "3843 [D loss: 1.003005] [G loss: 1.000147]\n",
      "3844 [D loss: 1.003062] [G loss: 1.000142]\n",
      "3845 [D loss: 1.003035] [G loss: 1.000229]\n",
      "3846 [D loss: 1.003236] [G loss: 1.000198]\n",
      "3847 [D loss: 1.002805] [G loss: 1.000188]\n",
      "3848 [D loss: 1.003137] [G loss: 1.000237]\n",
      "3849 [D loss: 1.003030] [G loss: 1.000096]\n",
      "3850 [D loss: 1.003110] [G loss: 1.000213]\n",
      "3851 [D loss: 1.003013] [G loss: 1.000269]\n",
      "3852 [D loss: 1.002973] [G loss: 1.000284]\n",
      "3853 [D loss: 1.002970] [G loss: 1.000153]\n",
      "3854 [D loss: 1.003066] [G loss: 1.000089]\n",
      "3855 [D loss: 1.003001] [G loss: 1.000224]\n",
      "3856 [D loss: 1.002948] [G loss: 0.999985]\n",
      "3857 [D loss: 1.002927] [G loss: 1.000099]\n",
      "3858 [D loss: 1.002934] [G loss: 1.000159]\n",
      "3859 [D loss: 1.003085] [G loss: 1.000240]\n",
      "3860 [D loss: 1.002922] [G loss: 0.999984]\n",
      "3861 [D loss: 1.003175] [G loss: 1.000217]\n",
      "3862 [D loss: 1.003102] [G loss: 1.000265]\n",
      "3863 [D loss: 1.003046] [G loss: 1.000039]\n",
      "3864 [D loss: 1.003003] [G loss: 1.000140]\n",
      "3865 [D loss: 1.003193] [G loss: 0.999959]\n",
      "3866 [D loss: 1.002879] [G loss: 1.000181]\n",
      "3867 [D loss: 1.003216] [G loss: 0.999923]\n",
      "3868 [D loss: 1.003074] [G loss: 1.000132]\n",
      "3869 [D loss: 1.003046] [G loss: 1.000160]\n",
      "3870 [D loss: 1.003058] [G loss: 1.000102]\n",
      "3871 [D loss: 1.003016] [G loss: 0.999815]\n",
      "3872 [D loss: 1.002969] [G loss: 0.999858]\n",
      "3873 [D loss: 1.002715] [G loss: 1.000099]\n",
      "3874 [D loss: 1.002994] [G loss: 1.000140]\n",
      "3875 [D loss: 1.003101] [G loss: 1.000008]\n",
      "3876 [D loss: 1.002970] [G loss: 1.000220]\n",
      "3877 [D loss: 1.002903] [G loss: 1.000232]\n",
      "3878 [D loss: 1.002965] [G loss: 1.000222]\n",
      "3879 [D loss: 1.003137] [G loss: 0.999966]\n",
      "3880 [D loss: 1.003007] [G loss: 1.000325]\n",
      "3881 [D loss: 1.002897] [G loss: 1.000279]\n",
      "3882 [D loss: 1.002779] [G loss: 0.999827]\n",
      "3883 [D loss: 1.003171] [G loss: 1.000220]\n",
      "3884 [D loss: 1.003152] [G loss: 1.000225]\n",
      "3885 [D loss: 1.002920] [G loss: 1.000124]\n",
      "3886 [D loss: 1.003008] [G loss: 0.999987]\n",
      "3887 [D loss: 1.003020] [G loss: 1.000302]\n",
      "3888 [D loss: 1.002897] [G loss: 1.000144]\n",
      "3889 [D loss: 1.003015] [G loss: 1.000502]\n",
      "3890 [D loss: 1.002772] [G loss: 0.999932]\n",
      "3891 [D loss: 1.002895] [G loss: 1.000001]\n",
      "3892 [D loss: 1.003126] [G loss: 1.000102]\n",
      "3893 [D loss: 1.003092] [G loss: 1.000110]\n",
      "3894 [D loss: 1.002636] [G loss: 1.000164]\n",
      "3895 [D loss: 1.002646] [G loss: 1.000205]\n",
      "3896 [D loss: 1.003030] [G loss: 1.000097]\n",
      "3897 [D loss: 1.003013] [G loss: 1.000110]\n",
      "3898 [D loss: 1.002625] [G loss: 1.000162]\n",
      "3899 [D loss: 1.002861] [G loss: 1.000317]\n",
      "3900 [D loss: 1.003343] [G loss: 0.999897]\n",
      "3901 [D loss: 1.002906] [G loss: 1.000308]\n",
      "3902 [D loss: 1.002918] [G loss: 0.999997]\n",
      "3903 [D loss: 1.003053] [G loss: 1.000041]\n",
      "3904 [D loss: 1.002926] [G loss: 1.000127]\n",
      "3905 [D loss: 1.002866] [G loss: 1.000031]\n",
      "3906 [D loss: 1.002806] [G loss: 1.000214]\n",
      "3907 [D loss: 1.002838] [G loss: 1.000090]\n",
      "3908 [D loss: 1.003081] [G loss: 1.000048]\n",
      "3909 [D loss: 1.002943] [G loss: 1.000150]\n",
      "3910 [D loss: 1.003074] [G loss: 0.999945]\n",
      "3911 [D loss: 1.002952] [G loss: 1.000227]\n",
      "3912 [D loss: 1.002961] [G loss: 1.000097]\n",
      "3913 [D loss: 1.002941] [G loss: 1.000063]\n",
      "3914 [D loss: 1.003021] [G loss: 1.000025]\n",
      "3915 [D loss: 1.003169] [G loss: 1.000160]\n",
      "3916 [D loss: 1.003180] [G loss: 0.999963]\n",
      "3917 [D loss: 1.003299] [G loss: 0.999948]\n",
      "3918 [D loss: 1.002947] [G loss: 1.000344]\n",
      "3919 [D loss: 1.003004] [G loss: 1.000054]\n",
      "3920 [D loss: 1.003153] [G loss: 1.000349]\n",
      "3921 [D loss: 1.003068] [G loss: 0.999923]\n",
      "3922 [D loss: 1.002886] [G loss: 1.000006]\n",
      "3923 [D loss: 1.003100] [G loss: 1.000122]\n",
      "3924 [D loss: 1.003053] [G loss: 1.000232]\n",
      "3925 [D loss: 1.003002] [G loss: 1.000149]\n",
      "3926 [D loss: 1.002941] [G loss: 1.000279]\n",
      "3927 [D loss: 1.003004] [G loss: 1.000030]\n",
      "3928 [D loss: 1.003024] [G loss: 1.000008]\n",
      "3929 [D loss: 1.002998] [G loss: 1.000417]\n",
      "3930 [D loss: 1.002973] [G loss: 1.000105]\n",
      "3931 [D loss: 1.002872] [G loss: 1.000314]\n",
      "3932 [D loss: 1.003443] [G loss: 1.000139]\n",
      "3933 [D loss: 1.003034] [G loss: 1.000154]\n",
      "3934 [D loss: 1.003097] [G loss: 1.000183]\n",
      "3935 [D loss: 1.002968] [G loss: 1.000024]\n",
      "3936 [D loss: 1.002978] [G loss: 0.999959]\n",
      "3937 [D loss: 1.002905] [G loss: 0.999848]\n",
      "3938 [D loss: 1.003072] [G loss: 1.000106]\n",
      "3939 [D loss: 1.003206] [G loss: 1.000005]\n",
      "3940 [D loss: 1.002978] [G loss: 1.000277]\n",
      "3941 [D loss: 1.003302] [G loss: 1.000111]\n",
      "3942 [D loss: 1.002871] [G loss: 1.000185]\n",
      "3943 [D loss: 1.002739] [G loss: 1.000223]\n",
      "3944 [D loss: 1.003173] [G loss: 1.000246]\n",
      "3945 [D loss: 1.003009] [G loss: 1.000073]\n",
      "3946 [D loss: 1.002798] [G loss: 0.999808]\n",
      "3947 [D loss: 1.002943] [G loss: 0.999899]\n",
      "3948 [D loss: 1.002965] [G loss: 1.000157]\n",
      "3949 [D loss: 1.003060] [G loss: 1.000214]\n",
      "3950 [D loss: 1.003004] [G loss: 0.999963]\n",
      "3951 [D loss: 1.003199] [G loss: 0.999953]\n",
      "3952 [D loss: 1.002987] [G loss: 1.000096]\n",
      "3953 [D loss: 1.003249] [G loss: 1.000159]\n",
      "3954 [D loss: 1.002986] [G loss: 0.999561]\n",
      "3955 [D loss: 1.003161] [G loss: 1.000131]\n",
      "3956 [D loss: 1.003100] [G loss: 1.000063]\n",
      "3957 [D loss: 1.003038] [G loss: 1.000237]\n",
      "3958 [D loss: 1.002870] [G loss: 1.000115]\n",
      "3959 [D loss: 1.002843] [G loss: 1.000276]\n",
      "3960 [D loss: 1.002928] [G loss: 1.000032]\n",
      "3961 [D loss: 1.002861] [G loss: 1.000048]\n",
      "3962 [D loss: 1.003102] [G loss: 0.999993]\n",
      "3963 [D loss: 1.003106] [G loss: 1.000373]\n",
      "3964 [D loss: 1.002886] [G loss: 1.000244]\n",
      "3965 [D loss: 1.003083] [G loss: 0.999975]\n",
      "3966 [D loss: 1.003039] [G loss: 1.000188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3967 [D loss: 1.003037] [G loss: 1.000008]\n",
      "3968 [D loss: 1.003188] [G loss: 1.000082]\n",
      "3969 [D loss: 1.003134] [G loss: 1.000270]\n",
      "3970 [D loss: 1.002846] [G loss: 1.000236]\n",
      "3971 [D loss: 1.002927] [G loss: 1.000407]\n",
      "3972 [D loss: 1.003089] [G loss: 1.000047]\n",
      "3973 [D loss: 1.003140] [G loss: 1.000082]\n",
      "3974 [D loss: 1.003019] [G loss: 1.000140]\n",
      "3975 [D loss: 1.003109] [G loss: 1.000295]\n",
      "3976 [D loss: 1.003047] [G loss: 1.000211]\n",
      "3977 [D loss: 1.002971] [G loss: 0.999886]\n",
      "3978 [D loss: 1.003148] [G loss: 1.000141]\n",
      "3979 [D loss: 1.002928] [G loss: 1.000102]\n",
      "3980 [D loss: 1.002917] [G loss: 1.000028]\n",
      "3981 [D loss: 1.002794] [G loss: 1.000112]\n",
      "3982 [D loss: 1.003006] [G loss: 0.999841]\n",
      "3983 [D loss: 1.003068] [G loss: 0.999824]\n",
      "3984 [D loss: 1.003207] [G loss: 1.000174]\n",
      "3985 [D loss: 1.003097] [G loss: 1.000092]\n",
      "3986 [D loss: 1.003161] [G loss: 1.000266]\n",
      "3987 [D loss: 1.002997] [G loss: 0.999939]\n",
      "3988 [D loss: 1.003169] [G loss: 0.999994]\n",
      "3989 [D loss: 1.002938] [G loss: 0.999981]\n",
      "3990 [D loss: 1.003067] [G loss: 1.000401]\n",
      "3991 [D loss: 1.002826] [G loss: 1.000244]\n",
      "3992 [D loss: 1.003068] [G loss: 0.999944]\n",
      "3993 [D loss: 1.002862] [G loss: 1.000120]\n",
      "3994 [D loss: 1.003117] [G loss: 0.999980]\n",
      "3995 [D loss: 1.003112] [G loss: 1.000004]\n",
      "3996 [D loss: 1.002937] [G loss: 1.000265]\n",
      "3997 [D loss: 1.002774] [G loss: 1.000137]\n",
      "3998 [D loss: 1.002920] [G loss: 1.000118]\n",
      "3999 [D loss: 1.002825] [G loss: 1.000325]\n",
      "4000 [D loss: 1.002889] [G loss: 1.000187]\n",
      "(14461, 768)\n",
      "4001 [D loss: 1.002951] [G loss: 1.000296]\n",
      "4002 [D loss: 1.003011] [G loss: 1.000023]\n",
      "4003 [D loss: 1.003037] [G loss: 1.000117]\n",
      "4004 [D loss: 1.002930] [G loss: 0.999963]\n",
      "4005 [D loss: 1.002993] [G loss: 1.000067]\n",
      "4006 [D loss: 1.003117] [G loss: 0.999988]\n",
      "4007 [D loss: 1.002993] [G loss: 0.999884]\n",
      "4008 [D loss: 1.003037] [G loss: 1.000133]\n",
      "4009 [D loss: 1.002971] [G loss: 1.000044]\n",
      "4010 [D loss: 1.003063] [G loss: 1.000128]\n",
      "4011 [D loss: 1.003068] [G loss: 1.000272]\n",
      "4012 [D loss: 1.003019] [G loss: 1.000061]\n",
      "4013 [D loss: 1.003136] [G loss: 0.999971]\n",
      "4014 [D loss: 1.002889] [G loss: 1.000201]\n",
      "4015 [D loss: 1.003028] [G loss: 1.000186]\n",
      "4016 [D loss: 1.002862] [G loss: 1.000126]\n",
      "4017 [D loss: 1.003146] [G loss: 1.000275]\n",
      "4018 [D loss: 1.003014] [G loss: 1.000197]\n",
      "4019 [D loss: 1.003085] [G loss: 1.000016]\n",
      "4020 [D loss: 1.002957] [G loss: 1.000186]\n",
      "4021 [D loss: 1.002855] [G loss: 1.000156]\n",
      "4022 [D loss: 1.003060] [G loss: 0.999900]\n",
      "4023 [D loss: 1.003199] [G loss: 1.000044]\n",
      "4024 [D loss: 1.002983] [G loss: 1.000160]\n",
      "4025 [D loss: 1.003011] [G loss: 1.000129]\n",
      "4026 [D loss: 1.003224] [G loss: 1.000260]\n",
      "4027 [D loss: 1.003193] [G loss: 0.999902]\n",
      "4028 [D loss: 1.003013] [G loss: 0.999956]\n",
      "4029 [D loss: 1.002806] [G loss: 0.999952]\n",
      "4030 [D loss: 1.002999] [G loss: 1.000296]\n",
      "4031 [D loss: 1.002958] [G loss: 0.999871]\n",
      "4032 [D loss: 1.002843] [G loss: 0.999786]\n",
      "4033 [D loss: 1.002818] [G loss: 1.000183]\n",
      "4034 [D loss: 1.003074] [G loss: 0.999914]\n",
      "4035 [D loss: 1.002880] [G loss: 1.000024]\n",
      "4036 [D loss: 1.003038] [G loss: 1.000063]\n",
      "4037 [D loss: 1.002868] [G loss: 0.999805]\n",
      "4038 [D loss: 1.003129] [G loss: 0.999971]\n",
      "4039 [D loss: 1.003154] [G loss: 1.000250]\n",
      "4040 [D loss: 1.003093] [G loss: 1.000165]\n",
      "4041 [D loss: 1.002886] [G loss: 1.000282]\n",
      "4042 [D loss: 1.003003] [G loss: 1.000370]\n",
      "4043 [D loss: 1.002931] [G loss: 0.999912]\n",
      "4044 [D loss: 1.003146] [G loss: 1.000120]\n",
      "4045 [D loss: 1.002866] [G loss: 0.999766]\n",
      "4046 [D loss: 1.003054] [G loss: 1.000193]\n",
      "4047 [D loss: 1.002840] [G loss: 1.000260]\n",
      "4048 [D loss: 1.003103] [G loss: 1.000277]\n",
      "4049 [D loss: 1.002943] [G loss: 0.999944]\n",
      "4050 [D loss: 1.002944] [G loss: 1.000213]\n",
      "4051 [D loss: 1.002707] [G loss: 0.999890]\n",
      "4052 [D loss: 1.002838] [G loss: 1.000023]\n",
      "4053 [D loss: 1.003032] [G loss: 1.000094]\n",
      "4054 [D loss: 1.003025] [G loss: 1.000230]\n",
      "4055 [D loss: 1.003108] [G loss: 0.999984]\n",
      "4056 [D loss: 1.002986] [G loss: 1.000052]\n",
      "4057 [D loss: 1.003022] [G loss: 1.000133]\n",
      "4058 [D loss: 1.002883] [G loss: 1.000269]\n",
      "4059 [D loss: 1.002920] [G loss: 1.000155]\n",
      "4060 [D loss: 1.002827] [G loss: 1.000078]\n",
      "4061 [D loss: 1.002970] [G loss: 1.000489]\n",
      "4062 [D loss: 1.002943] [G loss: 1.000039]\n",
      "4063 [D loss: 1.003020] [G loss: 0.999852]\n",
      "4064 [D loss: 1.003151] [G loss: 1.000282]\n",
      "4065 [D loss: 1.002831] [G loss: 1.000233]\n",
      "4066 [D loss: 1.003048] [G loss: 1.000541]\n",
      "4067 [D loss: 1.003112] [G loss: 1.000413]\n",
      "4068 [D loss: 1.003104] [G loss: 1.000041]\n",
      "4069 [D loss: 1.003127] [G loss: 1.000518]\n",
      "4070 [D loss: 1.003184] [G loss: 1.000126]\n",
      "4071 [D loss: 1.003017] [G loss: 1.000068]\n",
      "4072 [D loss: 1.002957] [G loss: 0.999983]\n",
      "4073 [D loss: 1.002733] [G loss: 1.000151]\n",
      "4074 [D loss: 1.003039] [G loss: 1.000079]\n",
      "4075 [D loss: 1.002959] [G loss: 1.000124]\n",
      "4076 [D loss: 1.003166] [G loss: 1.000199]\n",
      "4077 [D loss: 1.003211] [G loss: 1.000189]\n",
      "4078 [D loss: 1.003105] [G loss: 1.000237]\n",
      "4079 [D loss: 1.003115] [G loss: 0.999802]\n",
      "4080 [D loss: 1.003010] [G loss: 1.000275]\n",
      "4081 [D loss: 1.002918] [G loss: 1.000155]\n",
      "4082 [D loss: 1.003160] [G loss: 1.000230]\n",
      "4083 [D loss: 1.003041] [G loss: 0.999962]\n",
      "4084 [D loss: 1.003073] [G loss: 1.000229]\n",
      "4085 [D loss: 1.003009] [G loss: 1.000029]\n",
      "4086 [D loss: 1.003299] [G loss: 1.000001]\n",
      "4087 [D loss: 1.003172] [G loss: 1.000141]\n",
      "4088 [D loss: 1.003091] [G loss: 0.999787]\n",
      "4089 [D loss: 1.003092] [G loss: 0.999992]\n",
      "4090 [D loss: 1.002900] [G loss: 1.000079]\n",
      "4091 [D loss: 1.002972] [G loss: 1.000274]\n",
      "4092 [D loss: 1.003024] [G loss: 1.000305]\n",
      "4093 [D loss: 1.002851] [G loss: 1.000050]\n",
      "4094 [D loss: 1.003183] [G loss: 1.000188]\n",
      "4095 [D loss: 1.002973] [G loss: 0.999923]\n",
      "4096 [D loss: 1.003050] [G loss: 1.000014]\n",
      "4097 [D loss: 1.003003] [G loss: 1.000208]\n",
      "4098 [D loss: 1.003135] [G loss: 1.000131]\n",
      "4099 [D loss: 1.002964] [G loss: 0.999967]\n",
      "4100 [D loss: 1.002874] [G loss: 1.000044]\n",
      "4101 [D loss: 1.003185] [G loss: 1.000254]\n",
      "4102 [D loss: 1.002922] [G loss: 1.000063]\n",
      "4103 [D loss: 1.003044] [G loss: 1.000142]\n",
      "4104 [D loss: 1.003096] [G loss: 1.000225]\n",
      "4105 [D loss: 1.003150] [G loss: 1.000252]\n",
      "4106 [D loss: 1.003123] [G loss: 1.000164]\n",
      "4107 [D loss: 1.003075] [G loss: 1.000063]\n",
      "4108 [D loss: 1.003132] [G loss: 1.000053]\n",
      "4109 [D loss: 1.002964] [G loss: 1.000234]\n",
      "4110 [D loss: 1.003248] [G loss: 1.000141]\n",
      "4111 [D loss: 1.002953] [G loss: 1.000162]\n",
      "4112 [D loss: 1.002908] [G loss: 0.999994]\n",
      "4113 [D loss: 1.002993] [G loss: 0.999991]\n",
      "4114 [D loss: 1.003081] [G loss: 1.000219]\n",
      "4115 [D loss: 1.003095] [G loss: 0.999988]\n",
      "4116 [D loss: 1.002988] [G loss: 1.000051]\n",
      "4117 [D loss: 1.003011] [G loss: 1.000174]\n",
      "4118 [D loss: 1.003040] [G loss: 0.999727]\n",
      "4119 [D loss: 1.003105] [G loss: 1.000321]\n",
      "4120 [D loss: 1.003063] [G loss: 1.000328]\n",
      "4121 [D loss: 1.003098] [G loss: 1.000086]\n",
      "4122 [D loss: 1.003067] [G loss: 1.000052]\n",
      "4123 [D loss: 1.002927] [G loss: 1.000137]\n",
      "4124 [D loss: 1.002903] [G loss: 1.000135]\n",
      "4125 [D loss: 1.002960] [G loss: 1.000175]\n",
      "4126 [D loss: 1.002992] [G loss: 1.000169]\n",
      "4127 [D loss: 1.002927] [G loss: 0.999907]\n",
      "4128 [D loss: 1.002973] [G loss: 0.999972]\n",
      "4129 [D loss: 1.003005] [G loss: 0.999973]\n",
      "4130 [D loss: 1.003076] [G loss: 1.000041]\n",
      "4131 [D loss: 1.002892] [G loss: 1.000470]\n",
      "4132 [D loss: 1.003059] [G loss: 1.000250]\n",
      "4133 [D loss: 1.003077] [G loss: 1.000179]\n",
      "4134 [D loss: 1.003021] [G loss: 1.000141]\n",
      "4135 [D loss: 1.002843] [G loss: 1.000060]\n",
      "4136 [D loss: 1.003004] [G loss: 1.000074]\n",
      "4137 [D loss: 1.002922] [G loss: 1.000057]\n",
      "4138 [D loss: 1.002997] [G loss: 1.000140]\n",
      "4139 [D loss: 1.003050] [G loss: 1.000309]\n",
      "4140 [D loss: 1.003139] [G loss: 1.000415]\n",
      "4141 [D loss: 1.002850] [G loss: 1.000079]\n",
      "4142 [D loss: 1.003047] [G loss: 1.000087]\n",
      "4143 [D loss: 1.002799] [G loss: 1.000172]\n",
      "4144 [D loss: 1.003190] [G loss: 0.999932]\n",
      "4145 [D loss: 1.003112] [G loss: 1.000378]\n",
      "4146 [D loss: 1.003167] [G loss: 0.999858]\n",
      "4147 [D loss: 1.003165] [G loss: 1.000065]\n",
      "4148 [D loss: 1.003127] [G loss: 0.999834]\n",
      "4149 [D loss: 1.002992] [G loss: 1.000132]\n",
      "4150 [D loss: 1.003237] [G loss: 1.000076]\n",
      "4151 [D loss: 1.002867] [G loss: 1.000060]\n",
      "4152 [D loss: 1.002858] [G loss: 1.000009]\n",
      "4153 [D loss: 1.002892] [G loss: 1.000134]\n",
      "4154 [D loss: 1.002987] [G loss: 0.999929]\n",
      "4155 [D loss: 1.003069] [G loss: 0.999937]\n",
      "4156 [D loss: 1.003020] [G loss: 0.999800]\n",
      "4157 [D loss: 1.003160] [G loss: 0.999911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4158 [D loss: 1.002981] [G loss: 0.999949]\n",
      "4159 [D loss: 1.003035] [G loss: 1.000408]\n",
      "4160 [D loss: 1.002807] [G loss: 1.000002]\n",
      "4161 [D loss: 1.003015] [G loss: 1.000347]\n",
      "4162 [D loss: 1.002852] [G loss: 1.000130]\n",
      "4163 [D loss: 1.003008] [G loss: 1.000078]\n",
      "4164 [D loss: 1.002961] [G loss: 1.000091]\n",
      "4165 [D loss: 1.002848] [G loss: 0.999870]\n",
      "4166 [D loss: 1.003185] [G loss: 1.000229]\n",
      "4167 [D loss: 1.003169] [G loss: 1.000183]\n",
      "4168 [D loss: 1.003175] [G loss: 1.000328]\n",
      "4169 [D loss: 1.003020] [G loss: 1.000324]\n",
      "4170 [D loss: 1.003052] [G loss: 1.000248]\n",
      "4171 [D loss: 1.002976] [G loss: 0.999930]\n",
      "4172 [D loss: 1.003276] [G loss: 0.999896]\n",
      "4173 [D loss: 1.002872] [G loss: 1.000090]\n",
      "4174 [D loss: 1.002950] [G loss: 1.000192]\n",
      "4175 [D loss: 1.003115] [G loss: 1.000047]\n",
      "4176 [D loss: 1.002798] [G loss: 1.000332]\n",
      "4177 [D loss: 1.002959] [G loss: 1.000176]\n",
      "4178 [D loss: 1.002819] [G loss: 1.000100]\n",
      "4179 [D loss: 1.003029] [G loss: 1.000111]\n",
      "4180 [D loss: 1.002901] [G loss: 1.000101]\n",
      "4181 [D loss: 1.003242] [G loss: 1.000148]\n",
      "4182 [D loss: 1.002960] [G loss: 1.000262]\n",
      "4183 [D loss: 1.003081] [G loss: 1.000270]\n",
      "4184 [D loss: 1.003095] [G loss: 1.000315]\n",
      "4185 [D loss: 1.003015] [G loss: 0.999966]\n",
      "4186 [D loss: 1.002854] [G loss: 1.000085]\n",
      "4187 [D loss: 1.002945] [G loss: 0.999947]\n",
      "4188 [D loss: 1.002868] [G loss: 1.000212]\n",
      "4189 [D loss: 1.002917] [G loss: 0.999806]\n",
      "4190 [D loss: 1.003084] [G loss: 1.000009]\n",
      "4191 [D loss: 1.002825] [G loss: 1.000004]\n",
      "4192 [D loss: 1.003192] [G loss: 1.000148]\n",
      "4193 [D loss: 1.002944] [G loss: 1.000172]\n",
      "4194 [D loss: 1.003073] [G loss: 0.999935]\n",
      "4195 [D loss: 1.002902] [G loss: 1.000205]\n",
      "4196 [D loss: 1.003171] [G loss: 1.000117]\n",
      "4197 [D loss: 1.002843] [G loss: 0.999984]\n",
      "4198 [D loss: 1.002914] [G loss: 1.000104]\n",
      "4199 [D loss: 1.002918] [G loss: 1.000267]\n",
      "4200 [D loss: 1.002879] [G loss: 1.000003]\n",
      "4201 [D loss: 1.003014] [G loss: 1.000013]\n",
      "4202 [D loss: 1.002886] [G loss: 0.999770]\n",
      "4203 [D loss: 1.003026] [G loss: 1.000244]\n",
      "4204 [D loss: 1.002974] [G loss: 1.000218]\n",
      "4205 [D loss: 1.002894] [G loss: 1.000186]\n",
      "4206 [D loss: 1.002783] [G loss: 1.000227]\n",
      "4207 [D loss: 1.003124] [G loss: 1.000113]\n",
      "4208 [D loss: 1.003030] [G loss: 0.999966]\n",
      "4209 [D loss: 1.002917] [G loss: 1.000134]\n",
      "4210 [D loss: 1.003090] [G loss: 0.999967]\n",
      "4211 [D loss: 1.002984] [G loss: 1.000356]\n",
      "4212 [D loss: 1.002991] [G loss: 1.000003]\n",
      "4213 [D loss: 1.002776] [G loss: 1.000326]\n",
      "4214 [D loss: 1.003060] [G loss: 1.000034]\n",
      "4215 [D loss: 1.003095] [G loss: 1.000082]\n",
      "4216 [D loss: 1.003177] [G loss: 0.999920]\n",
      "4217 [D loss: 1.002984] [G loss: 1.000253]\n",
      "4218 [D loss: 1.003062] [G loss: 0.999986]\n",
      "4219 [D loss: 1.003159] [G loss: 1.000142]\n",
      "4220 [D loss: 1.002979] [G loss: 1.000323]\n",
      "4221 [D loss: 1.003158] [G loss: 1.000037]\n",
      "4222 [D loss: 1.002968] [G loss: 1.000134]\n",
      "4223 [D loss: 1.003116] [G loss: 1.000018]\n",
      "4224 [D loss: 1.002943] [G loss: 1.000216]\n",
      "4225 [D loss: 1.002957] [G loss: 0.999953]\n",
      "4226 [D loss: 1.002975] [G loss: 1.000359]\n",
      "4227 [D loss: 1.003059] [G loss: 0.999920]\n",
      "4228 [D loss: 1.003083] [G loss: 1.000110]\n",
      "4229 [D loss: 1.003109] [G loss: 1.000116]\n",
      "4230 [D loss: 1.003171] [G loss: 1.000095]\n",
      "4231 [D loss: 1.003051] [G loss: 1.000203]\n",
      "4232 [D loss: 1.003205] [G loss: 1.000096]\n",
      "4233 [D loss: 1.003205] [G loss: 1.000006]\n",
      "4234 [D loss: 1.003002] [G loss: 0.999944]\n",
      "4235 [D loss: 1.003060] [G loss: 1.000114]\n",
      "4236 [D loss: 1.003046] [G loss: 0.999920]\n",
      "4237 [D loss: 1.003215] [G loss: 1.000027]\n",
      "4238 [D loss: 1.003071] [G loss: 0.999997]\n",
      "4239 [D loss: 1.003043] [G loss: 0.999957]\n",
      "4240 [D loss: 1.003052] [G loss: 1.000237]\n",
      "4241 [D loss: 1.003011] [G loss: 1.000187]\n",
      "4242 [D loss: 1.002995] [G loss: 0.999679]\n",
      "4243 [D loss: 1.002912] [G loss: 0.999999]\n",
      "4244 [D loss: 1.002959] [G loss: 1.000167]\n",
      "4245 [D loss: 1.003081] [G loss: 1.000508]\n",
      "4246 [D loss: 1.002966] [G loss: 0.999986]\n",
      "4247 [D loss: 1.003077] [G loss: 1.000083]\n",
      "4248 [D loss: 1.003027] [G loss: 1.000377]\n",
      "4249 [D loss: 1.003046] [G loss: 1.000042]\n",
      "4250 [D loss: 1.002766] [G loss: 1.000064]\n",
      "4251 [D loss: 1.003066] [G loss: 1.000093]\n",
      "4252 [D loss: 1.003154] [G loss: 1.000247]\n",
      "4253 [D loss: 1.002988] [G loss: 1.000098]\n",
      "4254 [D loss: 1.003104] [G loss: 1.000158]\n",
      "4255 [D loss: 1.002905] [G loss: 0.999792]\n",
      "4256 [D loss: 1.003199] [G loss: 0.999875]\n",
      "4257 [D loss: 1.002829] [G loss: 1.000099]\n",
      "4258 [D loss: 1.003163] [G loss: 1.000072]\n",
      "4259 [D loss: 1.002936] [G loss: 1.000173]\n",
      "4260 [D loss: 1.002813] [G loss: 1.000191]\n",
      "4261 [D loss: 1.003141] [G loss: 0.999975]\n",
      "4262 [D loss: 1.003010] [G loss: 1.000411]\n",
      "4263 [D loss: 1.002969] [G loss: 0.999950]\n",
      "4264 [D loss: 1.003039] [G loss: 1.000245]\n",
      "4265 [D loss: 1.002971] [G loss: 0.999960]\n",
      "4266 [D loss: 1.002982] [G loss: 1.000114]\n",
      "4267 [D loss: 1.002971] [G loss: 1.000061]\n",
      "4268 [D loss: 1.002868] [G loss: 1.000059]\n",
      "4269 [D loss: 1.003135] [G loss: 1.000137]\n",
      "4270 [D loss: 1.003175] [G loss: 1.000057]\n",
      "4271 [D loss: 1.003044] [G loss: 0.999858]\n",
      "4272 [D loss: 1.002845] [G loss: 0.999826]\n",
      "4273 [D loss: 1.002948] [G loss: 1.000192]\n",
      "4274 [D loss: 1.003152] [G loss: 0.999954]\n",
      "4275 [D loss: 1.003134] [G loss: 0.999990]\n",
      "4276 [D loss: 1.003072] [G loss: 1.000206]\n",
      "4277 [D loss: 1.002956] [G loss: 1.000267]\n",
      "4278 [D loss: 1.003027] [G loss: 0.999886]\n",
      "4279 [D loss: 1.003115] [G loss: 1.000214]\n",
      "4280 [D loss: 1.003259] [G loss: 0.999970]\n",
      "4281 [D loss: 1.003023] [G loss: 0.999956]\n",
      "4282 [D loss: 1.002918] [G loss: 0.999995]\n",
      "4283 [D loss: 1.003223] [G loss: 1.000171]\n",
      "4284 [D loss: 1.003126] [G loss: 0.999863]\n",
      "4285 [D loss: 1.003058] [G loss: 1.000010]\n",
      "4286 [D loss: 1.002919] [G loss: 1.000038]\n",
      "4287 [D loss: 1.002908] [G loss: 0.999901]\n",
      "4288 [D loss: 1.003169] [G loss: 1.000152]\n",
      "4289 [D loss: 1.003006] [G loss: 1.000146]\n",
      "4290 [D loss: 1.002922] [G loss: 1.000029]\n",
      "4291 [D loss: 1.003055] [G loss: 1.000272]\n",
      "4292 [D loss: 1.002946] [G loss: 1.000144]\n",
      "4293 [D loss: 1.003318] [G loss: 1.000043]\n",
      "4294 [D loss: 1.002948] [G loss: 0.999936]\n",
      "4295 [D loss: 1.003079] [G loss: 1.000193]\n",
      "4296 [D loss: 1.003060] [G loss: 1.000080]\n",
      "4297 [D loss: 1.003075] [G loss: 1.000005]\n",
      "4298 [D loss: 1.002942] [G loss: 1.000055]\n",
      "4299 [D loss: 1.002991] [G loss: 0.999968]\n",
      "4300 [D loss: 1.002963] [G loss: 1.000110]\n",
      "4301 [D loss: 1.003119] [G loss: 1.000182]\n",
      "4302 [D loss: 1.002969] [G loss: 1.000411]\n",
      "4303 [D loss: 1.003065] [G loss: 0.999919]\n",
      "4304 [D loss: 1.002927] [G loss: 1.000056]\n",
      "4305 [D loss: 1.003134] [G loss: 0.999983]\n",
      "4306 [D loss: 1.003250] [G loss: 1.000082]\n",
      "4307 [D loss: 1.003132] [G loss: 1.000045]\n",
      "4308 [D loss: 1.003278] [G loss: 1.000164]\n",
      "4309 [D loss: 1.003220] [G loss: 1.000054]\n",
      "4310 [D loss: 1.002943] [G loss: 0.999871]\n",
      "4311 [D loss: 1.003295] [G loss: 1.000127]\n",
      "4312 [D loss: 1.003179] [G loss: 1.000339]\n",
      "4313 [D loss: 1.003034] [G loss: 1.000183]\n",
      "4314 [D loss: 1.003089] [G loss: 1.000110]\n",
      "4315 [D loss: 1.003024] [G loss: 1.000271]\n",
      "4316 [D loss: 1.003055] [G loss: 1.000225]\n",
      "4317 [D loss: 1.003053] [G loss: 1.000025]\n",
      "4318 [D loss: 1.002917] [G loss: 1.000137]\n",
      "4319 [D loss: 1.003130] [G loss: 0.999970]\n",
      "4320 [D loss: 1.003120] [G loss: 0.999997]\n",
      "4321 [D loss: 1.003208] [G loss: 0.999964]\n",
      "4322 [D loss: 1.003119] [G loss: 1.000049]\n",
      "4323 [D loss: 1.002974] [G loss: 1.000357]\n",
      "4324 [D loss: 1.002955] [G loss: 0.999790]\n",
      "4325 [D loss: 1.002972] [G loss: 1.000252]\n",
      "4326 [D loss: 1.003003] [G loss: 1.000120]\n",
      "4327 [D loss: 1.003205] [G loss: 1.000365]\n",
      "4328 [D loss: 1.002922] [G loss: 1.000058]\n",
      "4329 [D loss: 1.002961] [G loss: 1.000009]\n",
      "4330 [D loss: 1.003101] [G loss: 1.000031]\n",
      "4331 [D loss: 1.003051] [G loss: 1.000157]\n",
      "4332 [D loss: 1.002911] [G loss: 1.000033]\n",
      "4333 [D loss: 1.003217] [G loss: 1.000169]\n",
      "4334 [D loss: 1.003129] [G loss: 0.999956]\n",
      "4335 [D loss: 1.002842] [G loss: 0.999968]\n",
      "4336 [D loss: 1.002988] [G loss: 1.000126]\n",
      "4337 [D loss: 1.003203] [G loss: 1.000059]\n",
      "4338 [D loss: 1.002914] [G loss: 0.999979]\n",
      "4339 [D loss: 1.003201] [G loss: 1.000167]\n",
      "4340 [D loss: 1.002932] [G loss: 1.000160]\n",
      "4341 [D loss: 1.003274] [G loss: 1.000252]\n",
      "4342 [D loss: 1.003065] [G loss: 1.000086]\n",
      "4343 [D loss: 1.002850] [G loss: 1.000385]\n",
      "4344 [D loss: 1.003396] [G loss: 1.000107]\n",
      "4345 [D loss: 1.002999] [G loss: 1.000056]\n",
      "4346 [D loss: 1.003011] [G loss: 1.000122]\n",
      "4347 [D loss: 1.003013] [G loss: 0.999954]\n",
      "4348 [D loss: 1.003028] [G loss: 1.000035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4349 [D loss: 1.003285] [G loss: 0.999998]\n",
      "4350 [D loss: 1.003012] [G loss: 0.999973]\n",
      "4351 [D loss: 1.003006] [G loss: 1.000041]\n",
      "4352 [D loss: 1.002963] [G loss: 1.000121]\n",
      "4353 [D loss: 1.003197] [G loss: 1.000193]\n",
      "4354 [D loss: 1.003054] [G loss: 1.000093]\n",
      "4355 [D loss: 1.003104] [G loss: 1.000267]\n",
      "4356 [D loss: 1.002835] [G loss: 1.000170]\n",
      "4357 [D loss: 1.003105] [G loss: 1.000208]\n",
      "4358 [D loss: 1.003020] [G loss: 1.000210]\n",
      "4359 [D loss: 1.003046] [G loss: 1.000234]\n",
      "4360 [D loss: 1.003070] [G loss: 1.000100]\n",
      "4361 [D loss: 1.003126] [G loss: 1.000264]\n",
      "4362 [D loss: 1.002972] [G loss: 1.000238]\n",
      "4363 [D loss: 1.003073] [G loss: 1.000018]\n",
      "4364 [D loss: 1.002712] [G loss: 0.999750]\n",
      "4365 [D loss: 1.003091] [G loss: 1.000139]\n",
      "4366 [D loss: 1.003118] [G loss: 1.000122]\n",
      "4367 [D loss: 1.002896] [G loss: 1.000226]\n",
      "4368 [D loss: 1.002932] [G loss: 1.000202]\n",
      "4369 [D loss: 1.002918] [G loss: 1.000112]\n",
      "4370 [D loss: 1.003030] [G loss: 1.000037]\n",
      "4371 [D loss: 1.002862] [G loss: 0.999796]\n",
      "4372 [D loss: 1.002949] [G loss: 1.000064]\n",
      "4373 [D loss: 1.002945] [G loss: 1.000106]\n",
      "4374 [D loss: 1.002953] [G loss: 0.999932]\n",
      "4375 [D loss: 1.002903] [G loss: 1.000401]\n",
      "4376 [D loss: 1.003015] [G loss: 0.999971]\n",
      "4377 [D loss: 1.003116] [G loss: 0.999878]\n",
      "4378 [D loss: 1.003275] [G loss: 1.000047]\n",
      "4379 [D loss: 1.002913] [G loss: 0.999965]\n",
      "4380 [D loss: 1.002857] [G loss: 1.000042]\n",
      "4381 [D loss: 1.003011] [G loss: 1.000229]\n",
      "4382 [D loss: 1.003029] [G loss: 1.000119]\n",
      "4383 [D loss: 1.003033] [G loss: 1.000295]\n",
      "4384 [D loss: 1.002863] [G loss: 1.000109]\n",
      "4385 [D loss: 1.002957] [G loss: 1.000074]\n",
      "4386 [D loss: 1.003231] [G loss: 1.000322]\n",
      "4387 [D loss: 1.003127] [G loss: 0.999937]\n",
      "4388 [D loss: 1.002916] [G loss: 0.999942]\n",
      "4389 [D loss: 1.002862] [G loss: 1.000103]\n",
      "4390 [D loss: 1.002834] [G loss: 0.999885]\n",
      "4391 [D loss: 1.003220] [G loss: 1.000066]\n",
      "4392 [D loss: 1.002971] [G loss: 1.000167]\n",
      "4393 [D loss: 1.002958] [G loss: 1.000168]\n",
      "4394 [D loss: 1.003185] [G loss: 1.000129]\n",
      "4395 [D loss: 1.002966] [G loss: 0.999821]\n",
      "4396 [D loss: 1.003131] [G loss: 0.999831]\n",
      "4397 [D loss: 1.003083] [G loss: 1.000170]\n",
      "4398 [D loss: 1.003059] [G loss: 0.999924]\n",
      "4399 [D loss: 1.003257] [G loss: 0.999991]\n",
      "4400 [D loss: 1.002901] [G loss: 1.000253]\n",
      "4401 [D loss: 1.002848] [G loss: 1.000078]\n",
      "4402 [D loss: 1.002967] [G loss: 1.000238]\n",
      "4403 [D loss: 1.003221] [G loss: 0.999934]\n",
      "4404 [D loss: 1.002940] [G loss: 1.000064]\n",
      "4405 [D loss: 1.003223] [G loss: 1.000017]\n",
      "4406 [D loss: 1.002877] [G loss: 1.000182]\n",
      "4407 [D loss: 1.002958] [G loss: 1.000102]\n",
      "4408 [D loss: 1.003092] [G loss: 1.000312]\n",
      "4409 [D loss: 1.002979] [G loss: 1.000517]\n",
      "4410 [D loss: 1.002793] [G loss: 1.000300]\n",
      "4411 [D loss: 1.003097] [G loss: 1.000170]\n",
      "4412 [D loss: 1.002981] [G loss: 1.000098]\n",
      "4413 [D loss: 1.002930] [G loss: 1.000208]\n",
      "4414 [D loss: 1.003027] [G loss: 1.000333]\n",
      "4415 [D loss: 1.003026] [G loss: 0.999971]\n",
      "4416 [D loss: 1.002785] [G loss: 1.000177]\n",
      "4417 [D loss: 1.003016] [G loss: 1.000293]\n",
      "4418 [D loss: 1.002876] [G loss: 1.000192]\n",
      "4419 [D loss: 1.003060] [G loss: 1.000100]\n",
      "4420 [D loss: 1.002914] [G loss: 0.999997]\n",
      "4421 [D loss: 1.002811] [G loss: 0.999916]\n",
      "4422 [D loss: 1.003133] [G loss: 1.000041]\n",
      "4423 [D loss: 1.003021] [G loss: 1.000263]\n",
      "4424 [D loss: 1.002956] [G loss: 1.000293]\n",
      "4425 [D loss: 1.002966] [G loss: 0.999898]\n",
      "4426 [D loss: 1.002919] [G loss: 0.999874]\n",
      "4427 [D loss: 1.003106] [G loss: 0.999890]\n",
      "4428 [D loss: 1.003095] [G loss: 1.000050]\n",
      "4429 [D loss: 1.003176] [G loss: 1.000195]\n",
      "4430 [D loss: 1.002779] [G loss: 1.000077]\n",
      "4431 [D loss: 1.003144] [G loss: 1.000165]\n",
      "4432 [D loss: 1.002839] [G loss: 1.000066]\n",
      "4433 [D loss: 1.003081] [G loss: 1.000237]\n",
      "4434 [D loss: 1.003197] [G loss: 1.000130]\n",
      "4435 [D loss: 1.002857] [G loss: 1.000292]\n",
      "4436 [D loss: 1.003061] [G loss: 1.000044]\n",
      "4437 [D loss: 1.002856] [G loss: 1.000428]\n",
      "4438 [D loss: 1.002942] [G loss: 0.999690]\n",
      "4439 [D loss: 1.002908] [G loss: 1.000020]\n",
      "4440 [D loss: 1.002980] [G loss: 1.000072]\n",
      "4441 [D loss: 1.003115] [G loss: 1.000269]\n",
      "4442 [D loss: 1.002896] [G loss: 1.000083]\n",
      "4443 [D loss: 1.003075] [G loss: 1.000190]\n",
      "4444 [D loss: 1.002911] [G loss: 1.000274]\n",
      "4445 [D loss: 1.003047] [G loss: 1.000129]\n",
      "4446 [D loss: 1.003218] [G loss: 1.000213]\n",
      "4447 [D loss: 1.003125] [G loss: 1.000127]\n",
      "4448 [D loss: 1.002805] [G loss: 1.000062]\n",
      "4449 [D loss: 1.003200] [G loss: 0.999992]\n",
      "4450 [D loss: 1.002915] [G loss: 0.999794]\n",
      "4451 [D loss: 1.002967] [G loss: 1.000044]\n",
      "4452 [D loss: 1.002999] [G loss: 0.999935]\n",
      "4453 [D loss: 1.003016] [G loss: 1.000047]\n",
      "4454 [D loss: 1.003119] [G loss: 0.999885]\n",
      "4455 [D loss: 1.002916] [G loss: 0.999888]\n",
      "4456 [D loss: 1.003076] [G loss: 1.000271]\n",
      "4457 [D loss: 1.003047] [G loss: 1.000240]\n",
      "4458 [D loss: 1.002821] [G loss: 1.000047]\n",
      "4459 [D loss: 1.003187] [G loss: 0.999937]\n",
      "4460 [D loss: 1.002972] [G loss: 1.000275]\n",
      "4461 [D loss: 1.003084] [G loss: 1.000221]\n",
      "4462 [D loss: 1.003053] [G loss: 1.000277]\n",
      "4463 [D loss: 1.003289] [G loss: 1.000284]\n",
      "4464 [D loss: 1.002948] [G loss: 0.999795]\n",
      "4465 [D loss: 1.003030] [G loss: 1.000138]\n",
      "4466 [D loss: 1.003041] [G loss: 1.000132]\n",
      "4467 [D loss: 1.003185] [G loss: 1.000150]\n",
      "4468 [D loss: 1.003082] [G loss: 0.999982]\n",
      "4469 [D loss: 1.002992] [G loss: 0.999894]\n",
      "4470 [D loss: 1.002976] [G loss: 1.000294]\n",
      "4471 [D loss: 1.003071] [G loss: 1.000211]\n",
      "4472 [D loss: 1.002963] [G loss: 1.000289]\n",
      "4473 [D loss: 1.002901] [G loss: 1.000161]\n",
      "4474 [D loss: 1.002991] [G loss: 1.000176]\n",
      "4475 [D loss: 1.003099] [G loss: 1.000289]\n",
      "4476 [D loss: 1.003091] [G loss: 1.000047]\n",
      "4477 [D loss: 1.002932] [G loss: 1.000063]\n",
      "4478 [D loss: 1.002832] [G loss: 1.000193]\n",
      "4479 [D loss: 1.003224] [G loss: 0.999914]\n",
      "4480 [D loss: 1.002909] [G loss: 0.999865]\n",
      "4481 [D loss: 1.002986] [G loss: 1.000082]\n",
      "4482 [D loss: 1.003135] [G loss: 1.000098]\n",
      "4483 [D loss: 1.002763] [G loss: 1.000077]\n",
      "4484 [D loss: 1.003053] [G loss: 1.000044]\n",
      "4485 [D loss: 1.002928] [G loss: 1.000140]\n",
      "4486 [D loss: 1.002724] [G loss: 0.999995]\n",
      "4487 [D loss: 1.003090] [G loss: 1.000118]\n",
      "4488 [D loss: 1.003167] [G loss: 1.000109]\n",
      "4489 [D loss: 1.003048] [G loss: 1.000037]\n",
      "4490 [D loss: 1.003082] [G loss: 1.000016]\n",
      "4491 [D loss: 1.003234] [G loss: 1.000109]\n",
      "4492 [D loss: 1.002895] [G loss: 0.999814]\n",
      "4493 [D loss: 1.002952] [G loss: 1.000220]\n",
      "4494 [D loss: 1.002996] [G loss: 1.000031]\n",
      "4495 [D loss: 1.002872] [G loss: 1.000128]\n",
      "4496 [D loss: 1.003119] [G loss: 0.999826]\n",
      "4497 [D loss: 1.003090] [G loss: 1.000080]\n",
      "4498 [D loss: 1.002784] [G loss: 0.999915]\n",
      "4499 [D loss: 1.003060] [G loss: 1.000118]\n",
      "4500 [D loss: 1.003072] [G loss: 0.999791]\n",
      "4501 [D loss: 1.002890] [G loss: 1.000232]\n",
      "4502 [D loss: 1.002865] [G loss: 1.000191]\n",
      "4503 [D loss: 1.003092] [G loss: 1.000170]\n",
      "4504 [D loss: 1.002806] [G loss: 1.000278]\n",
      "4505 [D loss: 1.002978] [G loss: 1.000309]\n",
      "4506 [D loss: 1.002917] [G loss: 1.000297]\n",
      "4507 [D loss: 1.003153] [G loss: 1.000296]\n",
      "4508 [D loss: 1.003246] [G loss: 1.000272]\n",
      "4509 [D loss: 1.002859] [G loss: 1.000228]\n",
      "4510 [D loss: 1.003066] [G loss: 0.999863]\n",
      "4511 [D loss: 1.003000] [G loss: 0.999907]\n",
      "4512 [D loss: 1.002966] [G loss: 0.999795]\n",
      "4513 [D loss: 1.003024] [G loss: 1.000062]\n",
      "4514 [D loss: 1.003018] [G loss: 1.000185]\n",
      "4515 [D loss: 1.002981] [G loss: 1.000188]\n",
      "4516 [D loss: 1.003350] [G loss: 1.000208]\n",
      "4517 [D loss: 1.003157] [G loss: 0.999912]\n",
      "4518 [D loss: 1.003199] [G loss: 1.000126]\n",
      "4519 [D loss: 1.003051] [G loss: 1.000170]\n",
      "4520 [D loss: 1.002950] [G loss: 1.000194]\n",
      "4521 [D loss: 1.003437] [G loss: 1.000025]\n",
      "4522 [D loss: 1.002868] [G loss: 1.000159]\n",
      "4523 [D loss: 1.003060] [G loss: 0.999947]\n",
      "4524 [D loss: 1.003126] [G loss: 1.000033]\n",
      "4525 [D loss: 1.003266] [G loss: 0.999942]\n",
      "4526 [D loss: 1.003083] [G loss: 1.000138]\n",
      "4527 [D loss: 1.002974] [G loss: 1.000152]\n",
      "4528 [D loss: 1.002991] [G loss: 1.000084]\n",
      "4529 [D loss: 1.002979] [G loss: 1.000291]\n",
      "4530 [D loss: 1.003230] [G loss: 1.000117]\n",
      "4531 [D loss: 1.003031] [G loss: 1.000138]\n",
      "4532 [D loss: 1.002973] [G loss: 0.999974]\n",
      "4533 [D loss: 1.002751] [G loss: 0.999953]\n",
      "4534 [D loss: 1.003039] [G loss: 0.999815]\n",
      "4535 [D loss: 1.003047] [G loss: 1.000003]\n",
      "4536 [D loss: 1.003203] [G loss: 1.000062]\n",
      "4537 [D loss: 1.003058] [G loss: 0.999916]\n",
      "4538 [D loss: 1.002951] [G loss: 1.000289]\n",
      "4539 [D loss: 1.002991] [G loss: 1.000083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4540 [D loss: 1.003142] [G loss: 1.000062]\n",
      "4541 [D loss: 1.003064] [G loss: 0.999943]\n",
      "4542 [D loss: 1.003058] [G loss: 1.000116]\n",
      "4543 [D loss: 1.002845] [G loss: 1.000327]\n",
      "4544 [D loss: 1.002684] [G loss: 1.000006]\n",
      "4545 [D loss: 1.002998] [G loss: 0.999945]\n",
      "4546 [D loss: 1.003047] [G loss: 1.000111]\n",
      "4547 [D loss: 1.002906] [G loss: 1.000041]\n",
      "4548 [D loss: 1.003148] [G loss: 0.999867]\n",
      "4549 [D loss: 1.003085] [G loss: 1.000041]\n",
      "4550 [D loss: 1.002994] [G loss: 1.000008]\n",
      "4551 [D loss: 1.003006] [G loss: 1.000001]\n",
      "4552 [D loss: 1.002995] [G loss: 0.999869]\n",
      "4553 [D loss: 1.003219] [G loss: 1.000031]\n",
      "4554 [D loss: 1.003318] [G loss: 1.000090]\n",
      "4555 [D loss: 1.002988] [G loss: 0.999910]\n",
      "4556 [D loss: 1.002922] [G loss: 1.000060]\n",
      "4557 [D loss: 1.002825] [G loss: 0.999845]\n",
      "4558 [D loss: 1.003072] [G loss: 1.000090]\n",
      "4559 [D loss: 1.003014] [G loss: 1.000362]\n",
      "4560 [D loss: 1.003021] [G loss: 1.000298]\n",
      "4561 [D loss: 1.002810] [G loss: 1.000195]\n",
      "4562 [D loss: 1.002906] [G loss: 1.000205]\n",
      "4563 [D loss: 1.003178] [G loss: 1.000114]\n",
      "4564 [D loss: 1.002946] [G loss: 1.000162]\n",
      "4565 [D loss: 1.003048] [G loss: 1.000312]\n",
      "4566 [D loss: 1.002894] [G loss: 1.000100]\n",
      "4567 [D loss: 1.002912] [G loss: 1.000168]\n",
      "4568 [D loss: 1.002971] [G loss: 1.000087]\n",
      "4569 [D loss: 1.002944] [G loss: 1.000331]\n",
      "4570 [D loss: 1.003118] [G loss: 0.999942]\n",
      "4571 [D loss: 1.002936] [G loss: 0.999841]\n",
      "4572 [D loss: 1.003064] [G loss: 1.000081]\n",
      "4573 [D loss: 1.002986] [G loss: 0.999947]\n",
      "4574 [D loss: 1.003072] [G loss: 1.000033]\n",
      "4575 [D loss: 1.003181] [G loss: 1.000224]\n",
      "4576 [D loss: 1.003032] [G loss: 0.999954]\n",
      "4577 [D loss: 1.003132] [G loss: 1.000021]\n",
      "4578 [D loss: 1.003020] [G loss: 1.000303]\n",
      "4579 [D loss: 1.003040] [G loss: 0.999904]\n",
      "4580 [D loss: 1.003203] [G loss: 1.000157]\n",
      "4581 [D loss: 1.003257] [G loss: 1.000134]\n",
      "4582 [D loss: 1.003230] [G loss: 1.000048]\n",
      "4583 [D loss: 1.003349] [G loss: 0.999982]\n",
      "4584 [D loss: 1.002956] [G loss: 1.000196]\n",
      "4585 [D loss: 1.002984] [G loss: 1.000010]\n",
      "4586 [D loss: 1.003015] [G loss: 1.000224]\n",
      "4587 [D loss: 1.002842] [G loss: 0.999887]\n",
      "4588 [D loss: 1.003093] [G loss: 1.000414]\n",
      "4589 [D loss: 1.003124] [G loss: 1.000048]\n",
      "4590 [D loss: 1.002912] [G loss: 1.000174]\n",
      "4591 [D loss: 1.002952] [G loss: 1.000027]\n",
      "4592 [D loss: 1.003025] [G loss: 0.999853]\n",
      "4593 [D loss: 1.002906] [G loss: 1.000041]\n",
      "4594 [D loss: 1.002949] [G loss: 1.000134]\n",
      "4595 [D loss: 1.003208] [G loss: 0.999832]\n",
      "4596 [D loss: 1.002930] [G loss: 0.999960]\n",
      "4597 [D loss: 1.003206] [G loss: 1.000048]\n",
      "4598 [D loss: 1.002965] [G loss: 0.999914]\n",
      "4599 [D loss: 1.003126] [G loss: 1.000262]\n",
      "4600 [D loss: 1.003138] [G loss: 1.000161]\n",
      "4601 [D loss: 1.003005] [G loss: 0.999805]\n",
      "4602 [D loss: 1.003044] [G loss: 1.000102]\n",
      "4603 [D loss: 1.003192] [G loss: 0.999961]\n",
      "4604 [D loss: 1.003261] [G loss: 1.000021]\n",
      "4605 [D loss: 1.002993] [G loss: 1.000101]\n",
      "4606 [D loss: 1.003181] [G loss: 1.000070]\n",
      "4607 [D loss: 1.002870] [G loss: 1.000168]\n",
      "4608 [D loss: 1.003231] [G loss: 1.000198]\n",
      "4609 [D loss: 1.003200] [G loss: 0.999962]\n",
      "4610 [D loss: 1.003054] [G loss: 1.000398]\n",
      "4611 [D loss: 1.003009] [G loss: 1.000010]\n",
      "4612 [D loss: 1.002968] [G loss: 1.000151]\n",
      "4613 [D loss: 1.003142] [G loss: 1.000173]\n",
      "4614 [D loss: 1.003177] [G loss: 0.999969]\n",
      "4615 [D loss: 1.003182] [G loss: 1.000199]\n",
      "4616 [D loss: 1.003070] [G loss: 1.000184]\n",
      "4617 [D loss: 1.003243] [G loss: 0.999912]\n",
      "4618 [D loss: 1.002905] [G loss: 1.000135]\n",
      "4619 [D loss: 1.003073] [G loss: 1.000166]\n",
      "4620 [D loss: 1.003003] [G loss: 1.000074]\n",
      "4621 [D loss: 1.002985] [G loss: 1.000057]\n",
      "4622 [D loss: 1.002849] [G loss: 1.000172]\n",
      "4623 [D loss: 1.003109] [G loss: 1.000178]\n",
      "4624 [D loss: 1.002891] [G loss: 0.999996]\n",
      "4625 [D loss: 1.003200] [G loss: 1.000061]\n",
      "4626 [D loss: 1.003108] [G loss: 1.000104]\n",
      "4627 [D loss: 1.002865] [G loss: 1.000063]\n",
      "4628 [D loss: 1.003026] [G loss: 1.000366]\n",
      "4629 [D loss: 1.003083] [G loss: 1.000275]\n",
      "4630 [D loss: 1.003022] [G loss: 0.999857]\n",
      "4631 [D loss: 1.003097] [G loss: 1.000043]\n",
      "4632 [D loss: 1.003107] [G loss: 0.999842]\n",
      "4633 [D loss: 1.003094] [G loss: 0.999979]\n",
      "4634 [D loss: 1.002913] [G loss: 1.000113]\n",
      "4635 [D loss: 1.003115] [G loss: 1.000128]\n",
      "4636 [D loss: 1.002875] [G loss: 1.000049]\n",
      "4637 [D loss: 1.002999] [G loss: 1.000328]\n",
      "4638 [D loss: 1.003115] [G loss: 1.000432]\n",
      "4639 [D loss: 1.002946] [G loss: 1.000187]\n",
      "4640 [D loss: 1.002989] [G loss: 0.999942]\n",
      "4641 [D loss: 1.002995] [G loss: 1.000526]\n",
      "4642 [D loss: 1.002980] [G loss: 0.999977]\n",
      "4643 [D loss: 1.002932] [G loss: 1.000399]\n",
      "4644 [D loss: 1.003278] [G loss: 1.000010]\n",
      "4645 [D loss: 1.003080] [G loss: 1.000035]\n",
      "4646 [D loss: 1.003103] [G loss: 1.000147]\n",
      "4647 [D loss: 1.003106] [G loss: 1.000063]\n",
      "4648 [D loss: 1.002938] [G loss: 1.000049]\n",
      "4649 [D loss: 1.003243] [G loss: 1.000017]\n",
      "4650 [D loss: 1.003160] [G loss: 1.000365]\n",
      "4651 [D loss: 1.003139] [G loss: 0.999981]\n",
      "4652 [D loss: 1.002969] [G loss: 1.000133]\n",
      "4653 [D loss: 1.002868] [G loss: 1.000088]\n",
      "4654 [D loss: 1.003256] [G loss: 1.000164]\n",
      "4655 [D loss: 1.002980] [G loss: 1.000120]\n",
      "4656 [D loss: 1.003171] [G loss: 0.999869]\n",
      "4657 [D loss: 1.002890] [G loss: 1.000063]\n",
      "4658 [D loss: 1.002993] [G loss: 1.000177]\n",
      "4659 [D loss: 1.002953] [G loss: 1.000163]\n",
      "4660 [D loss: 1.002954] [G loss: 1.000059]\n",
      "4661 [D loss: 1.002957] [G loss: 1.000198]\n",
      "4662 [D loss: 1.002993] [G loss: 1.000172]\n",
      "4663 [D loss: 1.003042] [G loss: 1.000132]\n",
      "4664 [D loss: 1.002936] [G loss: 0.999947]\n",
      "4665 [D loss: 1.002892] [G loss: 1.000232]\n",
      "4666 [D loss: 1.003262] [G loss: 0.999811]\n",
      "4667 [D loss: 1.002850] [G loss: 0.999928]\n",
      "4668 [D loss: 1.003223] [G loss: 1.000094]\n",
      "4669 [D loss: 1.002940] [G loss: 1.000095]\n",
      "4670 [D loss: 1.003211] [G loss: 1.000429]\n",
      "4671 [D loss: 1.002987] [G loss: 1.000025]\n",
      "4672 [D loss: 1.003321] [G loss: 1.000237]\n",
      "4673 [D loss: 1.003111] [G loss: 1.000222]\n",
      "4674 [D loss: 1.003024] [G loss: 1.000171]\n",
      "4675 [D loss: 1.003044] [G loss: 0.999818]\n",
      "4676 [D loss: 1.003073] [G loss: 1.000187]\n",
      "4677 [D loss: 1.003153] [G loss: 0.999936]\n",
      "4678 [D loss: 1.003078] [G loss: 0.999793]\n",
      "4679 [D loss: 1.002918] [G loss: 1.000106]\n",
      "4680 [D loss: 1.003074] [G loss: 1.000067]\n",
      "4681 [D loss: 1.003282] [G loss: 1.000253]\n",
      "4682 [D loss: 1.003106] [G loss: 0.999839]\n",
      "4683 [D loss: 1.003252] [G loss: 1.000100]\n",
      "4684 [D loss: 1.003070] [G loss: 1.000074]\n",
      "4685 [D loss: 1.002904] [G loss: 0.999867]\n",
      "4686 [D loss: 1.002836] [G loss: 0.999983]\n",
      "4687 [D loss: 1.003076] [G loss: 0.999763]\n",
      "4688 [D loss: 1.003108] [G loss: 0.999948]\n",
      "4689 [D loss: 1.003052] [G loss: 1.000196]\n",
      "4690 [D loss: 1.003181] [G loss: 1.000173]\n",
      "4691 [D loss: 1.003045] [G loss: 0.999884]\n",
      "4692 [D loss: 1.002911] [G loss: 0.999881]\n",
      "4693 [D loss: 1.003060] [G loss: 1.000050]\n",
      "4694 [D loss: 1.002957] [G loss: 0.999751]\n",
      "4695 [D loss: 1.003027] [G loss: 1.000071]\n",
      "4696 [D loss: 1.002955] [G loss: 0.999883]\n",
      "4697 [D loss: 1.002880] [G loss: 1.000108]\n",
      "4698 [D loss: 1.002938] [G loss: 1.000071]\n",
      "4699 [D loss: 1.003136] [G loss: 1.000069]\n",
      "4700 [D loss: 1.003221] [G loss: 1.000194]\n",
      "4701 [D loss: 1.003024] [G loss: 1.000189]\n",
      "4702 [D loss: 1.003027] [G loss: 1.000025]\n",
      "4703 [D loss: 1.003065] [G loss: 1.000000]\n",
      "4704 [D loss: 1.003284] [G loss: 1.000202]\n",
      "4705 [D loss: 1.003050] [G loss: 1.000208]\n",
      "4706 [D loss: 1.003140] [G loss: 1.000177]\n",
      "4707 [D loss: 1.003166] [G loss: 1.000094]\n",
      "4708 [D loss: 1.002947] [G loss: 1.000156]\n",
      "4709 [D loss: 1.003059] [G loss: 1.000035]\n",
      "4710 [D loss: 1.003124] [G loss: 1.000003]\n",
      "4711 [D loss: 1.002884] [G loss: 1.000081]\n",
      "4712 [D loss: 1.002758] [G loss: 1.000046]\n",
      "4713 [D loss: 1.003337] [G loss: 1.000208]\n",
      "4714 [D loss: 1.002885] [G loss: 1.000001]\n",
      "4715 [D loss: 1.003155] [G loss: 0.999917]\n",
      "4716 [D loss: 1.002846] [G loss: 0.999997]\n",
      "4717 [D loss: 1.003055] [G loss: 1.000405]\n",
      "4718 [D loss: 1.003055] [G loss: 1.000008]\n",
      "4719 [D loss: 1.003131] [G loss: 1.000107]\n",
      "4720 [D loss: 1.003026] [G loss: 1.000115]\n",
      "4721 [D loss: 1.003044] [G loss: 1.000179]\n",
      "4722 [D loss: 1.002979] [G loss: 1.000038]\n",
      "4723 [D loss: 1.002970] [G loss: 1.000162]\n",
      "4724 [D loss: 1.003094] [G loss: 1.000040]\n",
      "4725 [D loss: 1.003052] [G loss: 0.999842]\n",
      "4726 [D loss: 1.003025] [G loss: 1.000164]\n",
      "4727 [D loss: 1.002950] [G loss: 1.000046]\n",
      "4728 [D loss: 1.003278] [G loss: 1.000412]\n",
      "4729 [D loss: 1.002991] [G loss: 1.000167]\n",
      "4730 [D loss: 1.002985] [G loss: 1.000268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4731 [D loss: 1.003070] [G loss: 1.000154]\n",
      "4732 [D loss: 1.003291] [G loss: 0.999987]\n",
      "4733 [D loss: 1.003187] [G loss: 0.999798]\n",
      "4734 [D loss: 1.003062] [G loss: 1.000029]\n",
      "4735 [D loss: 1.003122] [G loss: 0.999806]\n",
      "4736 [D loss: 1.003168] [G loss: 1.000046]\n",
      "4737 [D loss: 1.003117] [G loss: 1.000021]\n",
      "4738 [D loss: 1.003075] [G loss: 0.999967]\n",
      "4739 [D loss: 1.002856] [G loss: 1.000097]\n",
      "4740 [D loss: 1.003172] [G loss: 1.000322]\n",
      "4741 [D loss: 1.003153] [G loss: 1.000064]\n",
      "4742 [D loss: 1.003165] [G loss: 1.000254]\n",
      "4743 [D loss: 1.003093] [G loss: 1.000031]\n",
      "4744 [D loss: 1.003023] [G loss: 0.999907]\n",
      "4745 [D loss: 1.002860] [G loss: 1.000300]\n",
      "4746 [D loss: 1.003114] [G loss: 1.000139]\n",
      "4747 [D loss: 1.002893] [G loss: 1.000302]\n",
      "4748 [D loss: 1.003052] [G loss: 1.000313]\n",
      "4749 [D loss: 1.003009] [G loss: 1.000199]\n",
      "4750 [D loss: 1.002919] [G loss: 1.000218]\n",
      "4751 [D loss: 1.003024] [G loss: 1.000188]\n",
      "4752 [D loss: 1.003250] [G loss: 0.999929]\n",
      "4753 [D loss: 1.003049] [G loss: 1.000149]\n",
      "4754 [D loss: 1.002995] [G loss: 1.000126]\n",
      "4755 [D loss: 1.002860] [G loss: 1.000350]\n",
      "4756 [D loss: 1.003103] [G loss: 1.000193]\n",
      "4757 [D loss: 1.003031] [G loss: 1.000230]\n",
      "4758 [D loss: 1.002803] [G loss: 1.000044]\n",
      "4759 [D loss: 1.003014] [G loss: 1.000427]\n",
      "4760 [D loss: 1.003050] [G loss: 1.000172]\n",
      "4761 [D loss: 1.002978] [G loss: 1.000254]\n",
      "4762 [D loss: 1.002735] [G loss: 1.000049]\n",
      "4763 [D loss: 1.003318] [G loss: 1.000094]\n",
      "4764 [D loss: 1.002859] [G loss: 1.000247]\n",
      "4765 [D loss: 1.003186] [G loss: 1.000237]\n",
      "4766 [D loss: 1.002976] [G loss: 1.000285]\n",
      "4767 [D loss: 1.003057] [G loss: 0.999985]\n",
      "4768 [D loss: 1.003020] [G loss: 1.000047]\n",
      "4769 [D loss: 1.003098] [G loss: 1.000006]\n",
      "4770 [D loss: 1.003007] [G loss: 1.000074]\n",
      "4771 [D loss: 1.002915] [G loss: 1.000135]\n",
      "4772 [D loss: 1.003109] [G loss: 1.000150]\n",
      "4773 [D loss: 1.002890] [G loss: 1.000330]\n",
      "4774 [D loss: 1.002919] [G loss: 1.000336]\n",
      "4775 [D loss: 1.003377] [G loss: 1.000121]\n",
      "4776 [D loss: 1.002821] [G loss: 1.000278]\n",
      "4777 [D loss: 1.003080] [G loss: 1.000214]\n",
      "4778 [D loss: 1.003250] [G loss: 1.000135]\n",
      "4779 [D loss: 1.003133] [G loss: 0.999973]\n",
      "4780 [D loss: 1.003145] [G loss: 1.000092]\n",
      "4781 [D loss: 1.002849] [G loss: 1.000289]\n",
      "4782 [D loss: 1.003057] [G loss: 1.000034]\n",
      "4783 [D loss: 1.002971] [G loss: 1.000390]\n",
      "4784 [D loss: 1.003069] [G loss: 1.000509]\n",
      "4785 [D loss: 1.003070] [G loss: 0.999989]\n",
      "4786 [D loss: 1.003064] [G loss: 0.999803]\n",
      "4787 [D loss: 1.003404] [G loss: 1.000171]\n",
      "4788 [D loss: 1.003039] [G loss: 1.000361]\n",
      "4789 [D loss: 1.003186] [G loss: 1.000124]\n",
      "4790 [D loss: 1.003251] [G loss: 0.999766]\n",
      "4791 [D loss: 1.002928] [G loss: 1.000265]\n",
      "4792 [D loss: 1.002958] [G loss: 1.000292]\n",
      "4793 [D loss: 1.003060] [G loss: 1.000035]\n",
      "4794 [D loss: 1.003027] [G loss: 0.999931]\n",
      "4795 [D loss: 1.003211] [G loss: 1.000127]\n",
      "4796 [D loss: 1.002976] [G loss: 0.999966]\n",
      "4797 [D loss: 1.003026] [G loss: 1.000124]\n",
      "4798 [D loss: 1.002936] [G loss: 1.000344]\n",
      "4799 [D loss: 1.002914] [G loss: 1.000026]\n",
      "4800 [D loss: 1.002998] [G loss: 1.000133]\n",
      "4801 [D loss: 1.003117] [G loss: 1.000298]\n",
      "4802 [D loss: 1.002943] [G loss: 1.000062]\n",
      "4803 [D loss: 1.002930] [G loss: 1.000015]\n",
      "4804 [D loss: 1.003027] [G loss: 0.999878]\n",
      "4805 [D loss: 1.003098] [G loss: 1.000101]\n",
      "4806 [D loss: 1.003194] [G loss: 1.000178]\n",
      "4807 [D loss: 1.003031] [G loss: 1.000217]\n",
      "4808 [D loss: 1.002816] [G loss: 1.000178]\n",
      "4809 [D loss: 1.002903] [G loss: 0.999904]\n",
      "4810 [D loss: 1.002974] [G loss: 0.999898]\n",
      "4811 [D loss: 1.003209] [G loss: 1.000050]\n",
      "4812 [D loss: 1.003085] [G loss: 1.000093]\n",
      "4813 [D loss: 1.003133] [G loss: 0.999993]\n",
      "4814 [D loss: 1.003152] [G loss: 1.000114]\n",
      "4815 [D loss: 1.003183] [G loss: 1.000039]\n",
      "4816 [D loss: 1.003137] [G loss: 1.000255]\n",
      "4817 [D loss: 1.003228] [G loss: 1.000095]\n",
      "4818 [D loss: 1.003252] [G loss: 1.000001]\n",
      "4819 [D loss: 1.003144] [G loss: 0.999976]\n",
      "4820 [D loss: 1.003145] [G loss: 1.000426]\n",
      "4821 [D loss: 1.003079] [G loss: 1.000083]\n",
      "4822 [D loss: 1.003030] [G loss: 1.000174]\n",
      "4823 [D loss: 1.002888] [G loss: 1.000169]\n",
      "4824 [D loss: 1.003031] [G loss: 0.999962]\n",
      "4825 [D loss: 1.003148] [G loss: 1.000089]\n",
      "4826 [D loss: 1.002962] [G loss: 1.000262]\n",
      "4827 [D loss: 1.003134] [G loss: 0.999990]\n",
      "4828 [D loss: 1.002989] [G loss: 1.000079]\n",
      "4829 [D loss: 1.003089] [G loss: 1.000049]\n",
      "4830 [D loss: 1.002850] [G loss: 1.000282]\n",
      "4831 [D loss: 1.002754] [G loss: 0.999933]\n",
      "4832 [D loss: 1.002976] [G loss: 1.000188]\n",
      "4833 [D loss: 1.003123] [G loss: 1.000008]\n",
      "4834 [D loss: 1.003023] [G loss: 1.000305]\n",
      "4835 [D loss: 1.002952] [G loss: 1.000333]\n",
      "4836 [D loss: 1.003022] [G loss: 0.999887]\n",
      "4837 [D loss: 1.002807] [G loss: 0.999938]\n",
      "4838 [D loss: 1.002803] [G loss: 1.000051]\n",
      "4839 [D loss: 1.003172] [G loss: 1.000013]\n",
      "4840 [D loss: 1.002986] [G loss: 0.999872]\n",
      "4841 [D loss: 1.003117] [G loss: 0.999954]\n",
      "4842 [D loss: 1.003205] [G loss: 0.999943]\n",
      "4843 [D loss: 1.003209] [G loss: 1.000154]\n",
      "4844 [D loss: 1.002773] [G loss: 1.000329]\n",
      "4845 [D loss: 1.002899] [G loss: 1.000014]\n",
      "4846 [D loss: 1.003109] [G loss: 1.000054]\n",
      "4847 [D loss: 1.003040] [G loss: 1.000035]\n",
      "4848 [D loss: 1.003165] [G loss: 0.999973]\n",
      "4849 [D loss: 1.002985] [G loss: 1.000066]\n",
      "4850 [D loss: 1.002936] [G loss: 1.000110]\n",
      "4851 [D loss: 1.003181] [G loss: 1.000081]\n",
      "4852 [D loss: 1.003127] [G loss: 1.000269]\n",
      "4853 [D loss: 1.002966] [G loss: 1.000078]\n",
      "4854 [D loss: 1.003040] [G loss: 1.000242]\n",
      "4855 [D loss: 1.002992] [G loss: 1.000346]\n",
      "4856 [D loss: 1.002932] [G loss: 0.999970]\n",
      "4857 [D loss: 1.003084] [G loss: 1.000168]\n",
      "4858 [D loss: 1.003014] [G loss: 0.999936]\n",
      "4859 [D loss: 1.003210] [G loss: 1.000039]\n",
      "4860 [D loss: 1.002988] [G loss: 1.000044]\n",
      "4861 [D loss: 1.003018] [G loss: 1.000202]\n",
      "4862 [D loss: 1.003257] [G loss: 0.999968]\n",
      "4863 [D loss: 1.002928] [G loss: 1.000213]\n",
      "4864 [D loss: 1.003115] [G loss: 1.000300]\n",
      "4865 [D loss: 1.003030] [G loss: 0.999854]\n",
      "4866 [D loss: 1.003078] [G loss: 1.000361]\n",
      "4867 [D loss: 1.002870] [G loss: 1.000091]\n",
      "4868 [D loss: 1.003106] [G loss: 1.000240]\n",
      "4869 [D loss: 1.002941] [G loss: 1.000333]\n",
      "4870 [D loss: 1.003013] [G loss: 0.999797]\n",
      "4871 [D loss: 1.003064] [G loss: 1.000347]\n",
      "4872 [D loss: 1.003008] [G loss: 1.000046]\n",
      "4873 [D loss: 1.003068] [G loss: 1.000015]\n",
      "4874 [D loss: 1.002907] [G loss: 1.000145]\n",
      "4875 [D loss: 1.003231] [G loss: 1.000322]\n",
      "4876 [D loss: 1.003021] [G loss: 1.000086]\n",
      "4877 [D loss: 1.003093] [G loss: 1.000101]\n",
      "4878 [D loss: 1.003122] [G loss: 1.000246]\n",
      "4879 [D loss: 1.002930] [G loss: 1.000158]\n",
      "4880 [D loss: 1.003215] [G loss: 1.000050]\n",
      "4881 [D loss: 1.003275] [G loss: 1.000201]\n",
      "4882 [D loss: 1.003135] [G loss: 1.000147]\n",
      "4883 [D loss: 1.003116] [G loss: 1.000350]\n",
      "4884 [D loss: 1.003061] [G loss: 1.000099]\n",
      "4885 [D loss: 1.003166] [G loss: 1.000115]\n",
      "4886 [D loss: 1.003073] [G loss: 1.000098]\n",
      "4887 [D loss: 1.003039] [G loss: 0.999995]\n",
      "4888 [D loss: 1.003330] [G loss: 1.000185]\n",
      "4889 [D loss: 1.003052] [G loss: 1.000118]\n",
      "4890 [D loss: 1.003202] [G loss: 1.000156]\n",
      "4891 [D loss: 1.003235] [G loss: 1.000417]\n",
      "4892 [D loss: 1.003137] [G loss: 1.000040]\n",
      "4893 [D loss: 1.002942] [G loss: 1.000064]\n",
      "4894 [D loss: 1.003143] [G loss: 1.000281]\n",
      "4895 [D loss: 1.003104] [G loss: 1.000211]\n",
      "4896 [D loss: 1.003151] [G loss: 0.999927]\n",
      "4897 [D loss: 1.003107] [G loss: 1.000104]\n",
      "4898 [D loss: 1.003132] [G loss: 1.000178]\n",
      "4899 [D loss: 1.002993] [G loss: 1.000087]\n",
      "4900 [D loss: 1.002759] [G loss: 1.000416]\n",
      "4901 [D loss: 1.003040] [G loss: 1.000277]\n",
      "4902 [D loss: 1.003243] [G loss: 1.000296]\n",
      "4903 [D loss: 1.003178] [G loss: 0.999876]\n",
      "4904 [D loss: 1.003025] [G loss: 1.000353]\n",
      "4905 [D loss: 1.003245] [G loss: 1.000206]\n",
      "4906 [D loss: 1.003024] [G loss: 1.000082]\n",
      "4907 [D loss: 1.003047] [G loss: 1.000151]\n",
      "4908 [D loss: 1.003163] [G loss: 1.000204]\n",
      "4909 [D loss: 1.003216] [G loss: 1.000446]\n",
      "4910 [D loss: 1.003301] [G loss: 0.999982]\n",
      "4911 [D loss: 1.002879] [G loss: 1.000152]\n",
      "4912 [D loss: 1.002994] [G loss: 0.999960]\n",
      "4913 [D loss: 1.002871] [G loss: 1.000161]\n",
      "4914 [D loss: 1.003001] [G loss: 1.000041]\n",
      "4915 [D loss: 1.002967] [G loss: 1.000348]\n",
      "4916 [D loss: 1.003216] [G loss: 1.000292]\n",
      "4917 [D loss: 1.002873] [G loss: 1.000149]\n",
      "4918 [D loss: 1.003023] [G loss: 1.000104]\n",
      "4919 [D loss: 1.003067] [G loss: 1.000252]\n",
      "4920 [D loss: 1.003216] [G loss: 1.000077]\n",
      "4921 [D loss: 1.003052] [G loss: 1.000125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4922 [D loss: 1.002910] [G loss: 0.999991]\n",
      "4923 [D loss: 1.003058] [G loss: 1.000237]\n",
      "4924 [D loss: 1.003283] [G loss: 1.000322]\n",
      "4925 [D loss: 1.003168] [G loss: 1.000161]\n",
      "4926 [D loss: 1.002839] [G loss: 1.000440]\n",
      "4927 [D loss: 1.002998] [G loss: 1.000200]\n",
      "4928 [D loss: 1.003213] [G loss: 1.000166]\n",
      "4929 [D loss: 1.003238] [G loss: 1.000330]\n",
      "4930 [D loss: 1.003182] [G loss: 0.999988]\n",
      "4931 [D loss: 1.003173] [G loss: 1.000261]\n",
      "4932 [D loss: 1.003074] [G loss: 0.999728]\n",
      "4933 [D loss: 1.003034] [G loss: 1.000219]\n",
      "4934 [D loss: 1.003169] [G loss: 1.000185]\n",
      "4935 [D loss: 1.003164] [G loss: 1.000368]\n",
      "4936 [D loss: 1.003107] [G loss: 1.000199]\n",
      "4937 [D loss: 1.003142] [G loss: 1.000245]\n",
      "4938 [D loss: 1.002938] [G loss: 1.000136]\n",
      "4939 [D loss: 1.003233] [G loss: 1.000124]\n",
      "4940 [D loss: 1.003086] [G loss: 1.000310]\n",
      "4941 [D loss: 1.002976] [G loss: 1.000220]\n",
      "4942 [D loss: 1.002989] [G loss: 1.000281]\n",
      "4943 [D loss: 1.003143] [G loss: 1.000131]\n",
      "4944 [D loss: 1.003200] [G loss: 1.000049]\n",
      "4945 [D loss: 1.002943] [G loss: 1.000326]\n",
      "4946 [D loss: 1.003088] [G loss: 0.999974]\n",
      "4947 [D loss: 1.003160] [G loss: 1.000399]\n",
      "4948 [D loss: 1.002996] [G loss: 1.000177]\n",
      "4949 [D loss: 1.003116] [G loss: 1.000203]\n",
      "4950 [D loss: 1.003021] [G loss: 0.999754]\n",
      "4951 [D loss: 1.002927] [G loss: 1.000082]\n",
      "4952 [D loss: 1.003253] [G loss: 0.999898]\n",
      "4953 [D loss: 1.003192] [G loss: 1.000173]\n",
      "4954 [D loss: 1.003011] [G loss: 1.000099]\n",
      "4955 [D loss: 1.003038] [G loss: 0.999816]\n",
      "4956 [D loss: 1.003092] [G loss: 1.000167]\n",
      "4957 [D loss: 1.002983] [G loss: 1.000044]\n",
      "4958 [D loss: 1.003110] [G loss: 1.000222]\n",
      "4959 [D loss: 1.003357] [G loss: 0.999894]\n",
      "4960 [D loss: 1.003212] [G loss: 0.999794]\n",
      "4961 [D loss: 1.003189] [G loss: 1.000331]\n",
      "4962 [D loss: 1.003013] [G loss: 0.999962]\n",
      "4963 [D loss: 1.003166] [G loss: 1.000104]\n",
      "4964 [D loss: 1.002981] [G loss: 1.000258]\n",
      "4965 [D loss: 1.002844] [G loss: 1.000160]\n",
      "4966 [D loss: 1.002889] [G loss: 1.000215]\n",
      "4967 [D loss: 1.003073] [G loss: 1.000331]\n",
      "4968 [D loss: 1.003042] [G loss: 1.000217]\n",
      "4969 [D loss: 1.003082] [G loss: 1.000289]\n",
      "4970 [D loss: 1.003141] [G loss: 1.000266]\n",
      "4971 [D loss: 1.003098] [G loss: 1.000195]\n",
      "4972 [D loss: 1.002885] [G loss: 1.000160]\n",
      "4973 [D loss: 1.003080] [G loss: 1.000296]\n",
      "4974 [D loss: 1.003107] [G loss: 1.000176]\n",
      "4975 [D loss: 1.003005] [G loss: 1.000155]\n",
      "4976 [D loss: 1.003085] [G loss: 0.999932]\n",
      "4977 [D loss: 1.003136] [G loss: 1.000023]\n",
      "4978 [D loss: 1.003234] [G loss: 1.000105]\n",
      "4979 [D loss: 1.003044] [G loss: 0.999927]\n",
      "4980 [D loss: 1.003066] [G loss: 0.999875]\n",
      "4981 [D loss: 1.002989] [G loss: 0.999764]\n",
      "4982 [D loss: 1.003111] [G loss: 0.999913]\n",
      "4983 [D loss: 1.003053] [G loss: 1.000268]\n",
      "4984 [D loss: 1.003186] [G loss: 1.000095]\n",
      "4985 [D loss: 1.002897] [G loss: 1.000035]\n",
      "4986 [D loss: 1.003168] [G loss: 0.999993]\n",
      "4987 [D loss: 1.002999] [G loss: 1.000125]\n",
      "4988 [D loss: 1.003257] [G loss: 1.000243]\n",
      "4989 [D loss: 1.003132] [G loss: 1.000061]\n",
      "4990 [D loss: 1.003167] [G loss: 1.000276]\n",
      "4991 [D loss: 1.003014] [G loss: 1.000254]\n",
      "4992 [D loss: 1.003096] [G loss: 1.000150]\n",
      "4993 [D loss: 1.003095] [G loss: 1.000330]\n",
      "4994 [D loss: 1.003343] [G loss: 1.000272]\n",
      "4995 [D loss: 1.003179] [G loss: 1.000120]\n",
      "4996 [D loss: 1.003213] [G loss: 1.000281]\n",
      "4997 [D loss: 1.003166] [G loss: 1.000144]\n",
      "4998 [D loss: 1.003115] [G loss: 1.000157]\n",
      "4999 [D loss: 1.002950] [G loss: 1.000247]\n",
      "5000 [D loss: 1.002839] [G loss: 1.000206]\n",
      "5001 [D loss: 1.003184] [G loss: 1.000092]\n",
      "5002 [D loss: 1.003100] [G loss: 1.000158]\n",
      "5003 [D loss: 1.002957] [G loss: 1.000225]\n",
      "5004 [D loss: 1.003065] [G loss: 1.000267]\n",
      "5005 [D loss: 1.002927] [G loss: 0.999999]\n",
      "5006 [D loss: 1.003243] [G loss: 0.999979]\n",
      "5007 [D loss: 1.002884] [G loss: 1.000185]\n",
      "5008 [D loss: 1.002909] [G loss: 1.000490]\n",
      "5009 [D loss: 1.003040] [G loss: 1.000302]\n",
      "5010 [D loss: 1.003042] [G loss: 0.999958]\n",
      "5011 [D loss: 1.002943] [G loss: 1.000210]\n",
      "5012 [D loss: 1.002836] [G loss: 1.000417]\n",
      "5013 [D loss: 1.003030] [G loss: 1.000255]\n",
      "5014 [D loss: 1.003088] [G loss: 1.000089]\n",
      "5015 [D loss: 1.003116] [G loss: 1.000067]\n",
      "5016 [D loss: 1.003096] [G loss: 1.000351]\n",
      "5017 [D loss: 1.003087] [G loss: 1.000359]\n",
      "5018 [D loss: 1.002935] [G loss: 1.000334]\n",
      "5019 [D loss: 1.002994] [G loss: 1.000133]\n",
      "5020 [D loss: 1.002858] [G loss: 1.000233]\n",
      "5021 [D loss: 1.003021] [G loss: 1.000252]\n",
      "5022 [D loss: 1.002973] [G loss: 1.000106]\n",
      "5023 [D loss: 1.003188] [G loss: 1.000194]\n",
      "5024 [D loss: 1.002849] [G loss: 1.000028]\n",
      "5025 [D loss: 1.003282] [G loss: 1.000277]\n",
      "5026 [D loss: 1.003075] [G loss: 1.000216]\n",
      "5027 [D loss: 1.003198] [G loss: 1.000155]\n",
      "5028 [D loss: 1.003172] [G loss: 1.000053]\n",
      "5029 [D loss: 1.003023] [G loss: 1.000119]\n",
      "5030 [D loss: 1.003238] [G loss: 1.000360]\n",
      "5031 [D loss: 1.002968] [G loss: 1.000160]\n",
      "5032 [D loss: 1.003118] [G loss: 1.000505]\n",
      "5033 [D loss: 1.003043] [G loss: 1.000177]\n",
      "5034 [D loss: 1.003058] [G loss: 1.000164]\n",
      "5035 [D loss: 1.003069] [G loss: 1.000202]\n",
      "5036 [D loss: 1.003059] [G loss: 1.000193]\n",
      "5037 [D loss: 1.003115] [G loss: 1.000111]\n",
      "5038 [D loss: 1.003055] [G loss: 1.000294]\n",
      "5039 [D loss: 1.003191] [G loss: 0.999927]\n",
      "5040 [D loss: 1.003160] [G loss: 0.999949]\n",
      "5041 [D loss: 1.003169] [G loss: 1.000132]\n",
      "5042 [D loss: 1.003100] [G loss: 0.999978]\n",
      "5043 [D loss: 1.003208] [G loss: 1.000098]\n",
      "5044 [D loss: 1.002870] [G loss: 1.000264]\n",
      "5045 [D loss: 1.003228] [G loss: 1.000133]\n",
      "5046 [D loss: 1.002937] [G loss: 1.000130]\n",
      "5047 [D loss: 1.003362] [G loss: 1.000131]\n",
      "5048 [D loss: 1.003239] [G loss: 1.000478]\n",
      "5049 [D loss: 1.002926] [G loss: 0.999860]\n",
      "5050 [D loss: 1.003026] [G loss: 1.000233]\n",
      "5051 [D loss: 1.003218] [G loss: 1.000051]\n",
      "5052 [D loss: 1.002855] [G loss: 1.000153]\n",
      "5053 [D loss: 1.003017] [G loss: 1.000209]\n",
      "5054 [D loss: 1.003236] [G loss: 1.000400]\n",
      "5055 [D loss: 1.002972] [G loss: 1.000248]\n",
      "5056 [D loss: 1.003162] [G loss: 1.000135]\n",
      "5057 [D loss: 1.003250] [G loss: 1.000286]\n",
      "5058 [D loss: 1.003141] [G loss: 1.000267]\n",
      "5059 [D loss: 1.003138] [G loss: 0.999810]\n",
      "5060 [D loss: 1.003034] [G loss: 1.000101]\n",
      "5061 [D loss: 1.003245] [G loss: 1.000278]\n",
      "5062 [D loss: 1.003063] [G loss: 1.000218]\n",
      "5063 [D loss: 1.003226] [G loss: 0.999931]\n",
      "5064 [D loss: 1.003125] [G loss: 1.000266]\n",
      "5065 [D loss: 1.003206] [G loss: 1.000275]\n",
      "5066 [D loss: 1.003135] [G loss: 0.999911]\n",
      "5067 [D loss: 1.003018] [G loss: 1.000306]\n",
      "5068 [D loss: 1.003056] [G loss: 1.000337]\n",
      "5069 [D loss: 1.002956] [G loss: 0.999928]\n",
      "5070 [D loss: 1.003161] [G loss: 1.000258]\n",
      "5071 [D loss: 1.003144] [G loss: 1.000034]\n",
      "5072 [D loss: 1.003097] [G loss: 1.000014]\n",
      "5073 [D loss: 1.003284] [G loss: 1.000281]\n",
      "5074 [D loss: 1.002881] [G loss: 1.000083]\n",
      "5075 [D loss: 1.002856] [G loss: 1.000257]\n",
      "5076 [D loss: 1.003128] [G loss: 1.000115]\n",
      "5077 [D loss: 1.002907] [G loss: 1.000087]\n",
      "5078 [D loss: 1.003175] [G loss: 0.999971]\n",
      "5079 [D loss: 1.003101] [G loss: 1.000110]\n",
      "5080 [D loss: 1.003226] [G loss: 1.000143]\n",
      "5081 [D loss: 1.003028] [G loss: 1.000353]\n",
      "5082 [D loss: 1.002848] [G loss: 0.999967]\n",
      "5083 [D loss: 1.003223] [G loss: 1.000284]\n",
      "5084 [D loss: 1.003070] [G loss: 1.000355]\n",
      "5085 [D loss: 1.002959] [G loss: 1.000272]\n",
      "5086 [D loss: 1.003034] [G loss: 1.000096]\n",
      "5087 [D loss: 1.003066] [G loss: 1.000002]\n",
      "5088 [D loss: 1.003030] [G loss: 0.999921]\n",
      "5089 [D loss: 1.003070] [G loss: 1.000273]\n",
      "5090 [D loss: 1.002881] [G loss: 0.999980]\n",
      "5091 [D loss: 1.003081] [G loss: 1.000117]\n",
      "5092 [D loss: 1.003064] [G loss: 1.000116]\n",
      "5093 [D loss: 1.002980] [G loss: 0.999963]\n",
      "5094 [D loss: 1.003199] [G loss: 1.000383]\n",
      "5095 [D loss: 1.003031] [G loss: 0.999945]\n",
      "5096 [D loss: 1.003262] [G loss: 1.000055]\n",
      "5097 [D loss: 1.002941] [G loss: 1.000007]\n",
      "5098 [D loss: 1.003124] [G loss: 1.000134]\n",
      "5099 [D loss: 1.003205] [G loss: 0.999959]\n",
      "5100 [D loss: 1.003252] [G loss: 1.000105]\n",
      "5101 [D loss: 1.002923] [G loss: 1.000183]\n",
      "5102 [D loss: 1.002954] [G loss: 0.999932]\n",
      "5103 [D loss: 1.002975] [G loss: 1.000043]\n",
      "5104 [D loss: 1.002909] [G loss: 1.000168]\n",
      "5105 [D loss: 1.003102] [G loss: 1.000169]\n",
      "5106 [D loss: 1.002912] [G loss: 0.999996]\n",
      "5107 [D loss: 1.003211] [G loss: 0.999879]\n",
      "5108 [D loss: 1.003204] [G loss: 1.000024]\n",
      "5109 [D loss: 1.003098] [G loss: 1.000290]\n",
      "5110 [D loss: 1.003218] [G loss: 1.000338]\n",
      "5111 [D loss: 1.003069] [G loss: 1.000236]\n",
      "5112 [D loss: 1.003246] [G loss: 1.000008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5113 [D loss: 1.003235] [G loss: 1.000209]\n",
      "5114 [D loss: 1.002941] [G loss: 1.000064]\n",
      "5115 [D loss: 1.003084] [G loss: 1.000133]\n",
      "5116 [D loss: 1.003040] [G loss: 0.999972]\n",
      "5117 [D loss: 1.003315] [G loss: 1.000226]\n",
      "5118 [D loss: 1.003113] [G loss: 1.000025]\n",
      "5119 [D loss: 1.003037] [G loss: 1.000081]\n",
      "5120 [D loss: 1.002828] [G loss: 1.000346]\n",
      "5121 [D loss: 1.003121] [G loss: 1.000132]\n",
      "5122 [D loss: 1.003121] [G loss: 1.000033]\n",
      "5123 [D loss: 1.002943] [G loss: 1.000167]\n",
      "5124 [D loss: 1.003046] [G loss: 1.000150]\n",
      "5125 [D loss: 1.003090] [G loss: 0.999977]\n",
      "5126 [D loss: 1.002834] [G loss: 1.000189]\n",
      "5127 [D loss: 1.003044] [G loss: 1.000244]\n",
      "5128 [D loss: 1.003099] [G loss: 1.000170]\n",
      "5129 [D loss: 1.003121] [G loss: 1.000186]\n",
      "5130 [D loss: 1.003245] [G loss: 1.000262]\n",
      "5131 [D loss: 1.003144] [G loss: 1.000347]\n",
      "5132 [D loss: 1.003015] [G loss: 1.000329]\n",
      "5133 [D loss: 1.003034] [G loss: 1.000229]\n",
      "5134 [D loss: 1.003045] [G loss: 1.000188]\n",
      "5135 [D loss: 1.002750] [G loss: 1.000166]\n",
      "5136 [D loss: 1.003124] [G loss: 1.000328]\n",
      "5137 [D loss: 1.003028] [G loss: 1.000072]\n",
      "5138 [D loss: 1.003125] [G loss: 1.000194]\n",
      "5139 [D loss: 1.002993] [G loss: 1.000146]\n",
      "5140 [D loss: 1.003303] [G loss: 0.999885]\n",
      "5141 [D loss: 1.002970] [G loss: 1.000247]\n",
      "5142 [D loss: 1.002980] [G loss: 1.000228]\n",
      "5143 [D loss: 1.003159] [G loss: 1.000278]\n",
      "5144 [D loss: 1.003035] [G loss: 1.000370]\n",
      "5145 [D loss: 1.003114] [G loss: 1.000099]\n",
      "5146 [D loss: 1.002935] [G loss: 1.000168]\n",
      "5147 [D loss: 1.003015] [G loss: 1.000228]\n",
      "5148 [D loss: 1.003339] [G loss: 1.000226]\n",
      "5149 [D loss: 1.002968] [G loss: 0.999905]\n",
      "5150 [D loss: 1.002975] [G loss: 1.000403]\n",
      "5151 [D loss: 1.003307] [G loss: 1.000263]\n",
      "5152 [D loss: 1.002816] [G loss: 1.000062]\n",
      "5153 [D loss: 1.002985] [G loss: 1.000198]\n",
      "5154 [D loss: 1.002935] [G loss: 1.000187]\n",
      "5155 [D loss: 1.002900] [G loss: 1.000266]\n",
      "5156 [D loss: 1.003131] [G loss: 1.000157]\n",
      "5157 [D loss: 1.002730] [G loss: 1.000370]\n",
      "5158 [D loss: 1.003205] [G loss: 1.000300]\n",
      "5159 [D loss: 1.003026] [G loss: 1.000057]\n",
      "5160 [D loss: 1.003158] [G loss: 1.000201]\n",
      "5161 [D loss: 1.002979] [G loss: 1.000297]\n",
      "5162 [D loss: 1.002901] [G loss: 1.000059]\n",
      "5163 [D loss: 1.003205] [G loss: 1.000332]\n",
      "5164 [D loss: 1.003309] [G loss: 1.000145]\n",
      "5165 [D loss: 1.003259] [G loss: 1.000194]\n",
      "5166 [D loss: 1.003106] [G loss: 1.000196]\n",
      "5167 [D loss: 1.003075] [G loss: 1.000192]\n",
      "5168 [D loss: 1.003254] [G loss: 1.000352]\n",
      "5169 [D loss: 1.003167] [G loss: 1.000123]\n",
      "5170 [D loss: 1.002798] [G loss: 1.000313]\n",
      "5171 [D loss: 1.002939] [G loss: 1.000180]\n",
      "5172 [D loss: 1.003125] [G loss: 1.000120]\n",
      "5173 [D loss: 1.002893] [G loss: 1.000008]\n",
      "5174 [D loss: 1.003147] [G loss: 1.000174]\n",
      "5175 [D loss: 1.003197] [G loss: 1.000031]\n",
      "5176 [D loss: 1.003108] [G loss: 1.000127]\n",
      "5177 [D loss: 1.003029] [G loss: 1.000067]\n",
      "5178 [D loss: 1.003254] [G loss: 1.000103]\n",
      "5179 [D loss: 1.003233] [G loss: 1.000134]\n",
      "5180 [D loss: 1.002984] [G loss: 1.000352]\n",
      "5181 [D loss: 1.003022] [G loss: 1.000411]\n",
      "5182 [D loss: 1.003102] [G loss: 1.000348]\n",
      "5183 [D loss: 1.003294] [G loss: 1.000161]\n",
      "5184 [D loss: 1.003024] [G loss: 1.000304]\n",
      "5185 [D loss: 1.003478] [G loss: 1.000200]\n",
      "5186 [D loss: 1.003021] [G loss: 1.000377]\n",
      "5187 [D loss: 1.003118] [G loss: 1.000082]\n",
      "5188 [D loss: 1.003150] [G loss: 1.000220]\n",
      "5189 [D loss: 1.003320] [G loss: 1.000157]\n",
      "5190 [D loss: 1.003101] [G loss: 1.000282]\n",
      "5191 [D loss: 1.003132] [G loss: 1.000101]\n",
      "5192 [D loss: 1.003242] [G loss: 0.999995]\n",
      "5193 [D loss: 1.003219] [G loss: 0.999824]\n",
      "5194 [D loss: 1.003213] [G loss: 1.000051]\n",
      "5195 [D loss: 1.003093] [G loss: 1.000306]\n",
      "5196 [D loss: 1.003192] [G loss: 1.000072]\n",
      "5197 [D loss: 1.003117] [G loss: 1.000474]\n",
      "5198 [D loss: 1.002926] [G loss: 1.000041]\n",
      "5199 [D loss: 1.003062] [G loss: 1.000346]\n",
      "5200 [D loss: 1.003128] [G loss: 1.000034]\n",
      "5201 [D loss: 1.003301] [G loss: 0.999955]\n",
      "5202 [D loss: 1.003011] [G loss: 1.000345]\n",
      "5203 [D loss: 1.003001] [G loss: 1.000091]\n",
      "5204 [D loss: 1.003291] [G loss: 1.000251]\n",
      "5205 [D loss: 1.002910] [G loss: 1.000352]\n",
      "5206 [D loss: 1.003347] [G loss: 1.000383]\n",
      "5207 [D loss: 1.003104] [G loss: 1.000267]\n",
      "5208 [D loss: 1.003369] [G loss: 1.000168]\n",
      "5209 [D loss: 1.003033] [G loss: 1.000343]\n",
      "5210 [D loss: 1.002934] [G loss: 1.000037]\n",
      "5211 [D loss: 1.003047] [G loss: 1.000131]\n",
      "5212 [D loss: 1.003027] [G loss: 1.000240]\n",
      "5213 [D loss: 1.003132] [G loss: 1.000072]\n",
      "5214 [D loss: 1.003171] [G loss: 1.000019]\n",
      "5215 [D loss: 1.002872] [G loss: 1.000047]\n",
      "5216 [D loss: 1.002965] [G loss: 1.000081]\n",
      "5217 [D loss: 1.003261] [G loss: 1.000203]\n",
      "5218 [D loss: 1.003070] [G loss: 1.000155]\n",
      "5219 [D loss: 1.002849] [G loss: 1.000230]\n",
      "5220 [D loss: 1.002957] [G loss: 1.000317]\n",
      "5221 [D loss: 1.002893] [G loss: 1.000144]\n",
      "5222 [D loss: 1.002792] [G loss: 1.000233]\n",
      "5223 [D loss: 1.003148] [G loss: 1.000081]\n",
      "5224 [D loss: 1.003217] [G loss: 1.000224]\n",
      "5225 [D loss: 1.003119] [G loss: 0.999762]\n",
      "5226 [D loss: 1.003131] [G loss: 1.000377]\n",
      "5227 [D loss: 1.003025] [G loss: 1.000088]\n",
      "5228 [D loss: 1.002965] [G loss: 1.000279]\n",
      "5229 [D loss: 1.002988] [G loss: 0.999923]\n",
      "5230 [D loss: 1.003126] [G loss: 1.000042]\n",
      "5231 [D loss: 1.002855] [G loss: 0.999993]\n",
      "5232 [D loss: 1.003162] [G loss: 1.000020]\n",
      "5233 [D loss: 1.003123] [G loss: 0.999918]\n",
      "5234 [D loss: 1.003391] [G loss: 1.000204]\n",
      "5235 [D loss: 1.003164] [G loss: 1.000225]\n",
      "5236 [D loss: 1.003099] [G loss: 1.000181]\n",
      "5237 [D loss: 1.003112] [G loss: 1.000459]\n",
      "5238 [D loss: 1.003110] [G loss: 1.000110]\n",
      "5239 [D loss: 1.002946] [G loss: 1.000321]\n",
      "5240 [D loss: 1.003003] [G loss: 1.000035]\n",
      "5241 [D loss: 1.003106] [G loss: 0.999977]\n",
      "5242 [D loss: 1.003021] [G loss: 1.000090]\n",
      "5243 [D loss: 1.003089] [G loss: 1.000292]\n",
      "5244 [D loss: 1.003232] [G loss: 1.000192]\n",
      "5245 [D loss: 1.003051] [G loss: 1.000297]\n",
      "5246 [D loss: 1.003302] [G loss: 1.000154]\n",
      "5247 [D loss: 1.003055] [G loss: 1.000415]\n",
      "5248 [D loss: 1.003203] [G loss: 1.000178]\n",
      "5249 [D loss: 1.003070] [G loss: 1.000120]\n",
      "5250 [D loss: 1.002993] [G loss: 1.000426]\n",
      "5251 [D loss: 1.003210] [G loss: 1.000194]\n",
      "5252 [D loss: 1.003167] [G loss: 1.000095]\n",
      "5253 [D loss: 1.003226] [G loss: 0.999926]\n",
      "5254 [D loss: 1.003069] [G loss: 1.000275]\n",
      "5255 [D loss: 1.003185] [G loss: 1.000204]\n",
      "5256 [D loss: 1.003409] [G loss: 1.000175]\n",
      "5257 [D loss: 1.003040] [G loss: 1.000274]\n",
      "5258 [D loss: 1.003236] [G loss: 1.000344]\n",
      "5259 [D loss: 1.003188] [G loss: 1.000414]\n",
      "5260 [D loss: 1.003089] [G loss: 1.000076]\n",
      "5261 [D loss: 1.003086] [G loss: 1.000026]\n",
      "5262 [D loss: 1.003156] [G loss: 1.000072]\n",
      "5263 [D loss: 1.002986] [G loss: 1.000237]\n",
      "5264 [D loss: 1.003211] [G loss: 1.000173]\n",
      "5265 [D loss: 1.002839] [G loss: 1.000195]\n",
      "5266 [D loss: 1.003235] [G loss: 1.000220]\n",
      "5267 [D loss: 1.003296] [G loss: 1.000256]\n",
      "5268 [D loss: 1.003023] [G loss: 1.000409]\n",
      "5269 [D loss: 1.003295] [G loss: 1.000074]\n",
      "5270 [D loss: 1.003048] [G loss: 1.000227]\n",
      "5271 [D loss: 1.003107] [G loss: 1.000149]\n",
      "5272 [D loss: 1.003261] [G loss: 1.000221]\n",
      "5273 [D loss: 1.003129] [G loss: 1.000000]\n",
      "5274 [D loss: 1.003143] [G loss: 1.000195]\n",
      "5275 [D loss: 1.003053] [G loss: 0.999987]\n",
      "5276 [D loss: 1.002897] [G loss: 1.000284]\n",
      "5277 [D loss: 1.003085] [G loss: 1.000358]\n",
      "5278 [D loss: 1.003005] [G loss: 1.000355]\n",
      "5279 [D loss: 1.003240] [G loss: 1.000095]\n",
      "5280 [D loss: 1.003101] [G loss: 1.000096]\n",
      "5281 [D loss: 1.003132] [G loss: 1.000167]\n",
      "5282 [D loss: 1.003070] [G loss: 1.000051]\n",
      "5283 [D loss: 1.003161] [G loss: 1.000260]\n",
      "5284 [D loss: 1.003246] [G loss: 1.000211]\n",
      "5285 [D loss: 1.003148] [G loss: 1.000072]\n",
      "5286 [D loss: 1.003239] [G loss: 1.000173]\n",
      "5287 [D loss: 1.003143] [G loss: 1.000252]\n",
      "5288 [D loss: 1.003164] [G loss: 1.000344]\n",
      "5289 [D loss: 1.003047] [G loss: 1.000096]\n",
      "5290 [D loss: 1.003108] [G loss: 1.000014]\n",
      "5291 [D loss: 1.002961] [G loss: 1.000185]\n",
      "5292 [D loss: 1.002847] [G loss: 1.000393]\n",
      "5293 [D loss: 1.003000] [G loss: 1.000403]\n",
      "5294 [D loss: 1.003212] [G loss: 1.000267]\n",
      "5295 [D loss: 1.003222] [G loss: 1.000175]\n",
      "5296 [D loss: 1.002931] [G loss: 1.000141]\n",
      "5297 [D loss: 1.003128] [G loss: 1.000095]\n",
      "5298 [D loss: 1.003116] [G loss: 1.000340]\n",
      "5299 [D loss: 1.002995] [G loss: 1.000103]\n",
      "5300 [D loss: 1.002827] [G loss: 1.000127]\n",
      "5301 [D loss: 1.003100] [G loss: 1.000612]\n",
      "5302 [D loss: 1.003158] [G loss: 1.000101]\n",
      "5303 [D loss: 1.003035] [G loss: 1.000087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5304 [D loss: 1.003146] [G loss: 1.000155]\n",
      "5305 [D loss: 1.002803] [G loss: 1.000014]\n",
      "5306 [D loss: 1.003044] [G loss: 1.000113]\n",
      "5307 [D loss: 1.003059] [G loss: 0.999892]\n",
      "5308 [D loss: 1.003190] [G loss: 0.999847]\n",
      "5309 [D loss: 1.003101] [G loss: 1.000238]\n",
      "5310 [D loss: 1.003080] [G loss: 1.000231]\n",
      "5311 [D loss: 1.003207] [G loss: 1.000412]\n",
      "5312 [D loss: 1.003327] [G loss: 1.000154]\n",
      "5313 [D loss: 1.003138] [G loss: 1.000056]\n",
      "5314 [D loss: 1.003122] [G loss: 0.999994]\n",
      "5315 [D loss: 1.003095] [G loss: 0.999883]\n",
      "5316 [D loss: 1.002988] [G loss: 1.000229]\n",
      "5317 [D loss: 1.002979] [G loss: 1.000241]\n",
      "5318 [D loss: 1.002896] [G loss: 1.000194]\n",
      "5319 [D loss: 1.002844] [G loss: 1.000448]\n",
      "5320 [D loss: 1.003054] [G loss: 1.000038]\n",
      "5321 [D loss: 1.003257] [G loss: 1.000348]\n",
      "5322 [D loss: 1.003207] [G loss: 1.000242]\n",
      "5323 [D loss: 1.003118] [G loss: 0.999994]\n",
      "5324 [D loss: 1.003020] [G loss: 1.000274]\n",
      "5325 [D loss: 1.003228] [G loss: 0.999998]\n",
      "5326 [D loss: 1.003149] [G loss: 0.999981]\n",
      "5327 [D loss: 1.003197] [G loss: 1.000412]\n",
      "5328 [D loss: 1.003179] [G loss: 1.000135]\n",
      "5329 [D loss: 1.002870] [G loss: 1.000089]\n",
      "5330 [D loss: 1.003047] [G loss: 1.000378]\n",
      "5331 [D loss: 1.002939] [G loss: 1.000020]\n",
      "5332 [D loss: 1.003144] [G loss: 1.000391]\n",
      "5333 [D loss: 1.002977] [G loss: 1.000298]\n",
      "5334 [D loss: 1.003307] [G loss: 1.000102]\n",
      "5335 [D loss: 1.003145] [G loss: 1.000264]\n",
      "5336 [D loss: 1.002850] [G loss: 1.000273]\n",
      "5337 [D loss: 1.002947] [G loss: 1.000075]\n",
      "5338 [D loss: 1.003042] [G loss: 0.999913]\n",
      "5339 [D loss: 1.003277] [G loss: 1.000321]\n",
      "5340 [D loss: 1.003188] [G loss: 1.000212]\n",
      "5341 [D loss: 1.003012] [G loss: 1.000324]\n",
      "5342 [D loss: 1.003142] [G loss: 1.000394]\n",
      "5343 [D loss: 1.003026] [G loss: 1.000075]\n",
      "5344 [D loss: 1.003268] [G loss: 1.000237]\n",
      "5345 [D loss: 1.003021] [G loss: 1.000224]\n",
      "5346 [D loss: 1.003058] [G loss: 1.000370]\n",
      "5347 [D loss: 1.003337] [G loss: 1.000325]\n",
      "5348 [D loss: 1.003112] [G loss: 1.000137]\n",
      "5349 [D loss: 1.003088] [G loss: 1.000080]\n",
      "5350 [D loss: 1.003148] [G loss: 1.000249]\n",
      "5351 [D loss: 1.003075] [G loss: 1.000099]\n",
      "5352 [D loss: 1.003097] [G loss: 1.000134]\n",
      "5353 [D loss: 1.003137] [G loss: 1.000003]\n",
      "5354 [D loss: 1.003094] [G loss: 1.000178]\n",
      "5355 [D loss: 1.003206] [G loss: 1.000098]\n",
      "5356 [D loss: 1.003105] [G loss: 0.999898]\n",
      "5357 [D loss: 1.003219] [G loss: 1.000167]\n",
      "5358 [D loss: 1.003177] [G loss: 1.000183]\n",
      "5359 [D loss: 1.003077] [G loss: 1.000009]\n",
      "5360 [D loss: 1.003217] [G loss: 1.000160]\n",
      "5361 [D loss: 1.002959] [G loss: 1.000098]\n",
      "5362 [D loss: 1.003477] [G loss: 1.000325]\n",
      "5363 [D loss: 1.003045] [G loss: 1.000241]\n",
      "5364 [D loss: 1.003311] [G loss: 1.000199]\n",
      "5365 [D loss: 1.002761] [G loss: 1.000477]\n",
      "5366 [D loss: 1.003164] [G loss: 1.000243]\n",
      "5367 [D loss: 1.003099] [G loss: 1.000076]\n",
      "5368 [D loss: 1.003250] [G loss: 0.999662]\n",
      "5369 [D loss: 1.003158] [G loss: 1.000548]\n",
      "5370 [D loss: 1.003047] [G loss: 1.000166]\n",
      "5371 [D loss: 1.003073] [G loss: 1.000010]\n",
      "5372 [D loss: 1.003110] [G loss: 1.000381]\n",
      "5373 [D loss: 1.002978] [G loss: 1.000237]\n",
      "5374 [D loss: 1.003058] [G loss: 1.000320]\n",
      "5375 [D loss: 1.003065] [G loss: 1.000432]\n",
      "5376 [D loss: 1.002811] [G loss: 1.000075]\n",
      "5377 [D loss: 1.002972] [G loss: 1.000189]\n",
      "5378 [D loss: 1.003030] [G loss: 1.000179]\n",
      "5379 [D loss: 1.003333] [G loss: 1.000181]\n",
      "5380 [D loss: 1.003157] [G loss: 1.000229]\n",
      "5381 [D loss: 1.003303] [G loss: 1.000232]\n",
      "5382 [D loss: 1.003372] [G loss: 1.000131]\n",
      "5383 [D loss: 1.003205] [G loss: 1.000265]\n",
      "5384 [D loss: 1.002890] [G loss: 0.999902]\n",
      "5385 [D loss: 1.003101] [G loss: 1.000156]\n",
      "5386 [D loss: 1.003093] [G loss: 1.000120]\n",
      "5387 [D loss: 1.003098] [G loss: 0.999908]\n",
      "5388 [D loss: 1.002955] [G loss: 1.000045]\n",
      "5389 [D loss: 1.003029] [G loss: 1.000298]\n",
      "5390 [D loss: 1.003041] [G loss: 1.000328]\n",
      "5391 [D loss: 1.003246] [G loss: 1.000310]\n",
      "5392 [D loss: 1.003051] [G loss: 1.000083]\n",
      "5393 [D loss: 1.003298] [G loss: 1.000529]\n",
      "5394 [D loss: 1.003251] [G loss: 1.000294]\n",
      "5395 [D loss: 1.003361] [G loss: 1.000253]\n",
      "5396 [D loss: 1.002951] [G loss: 1.000585]\n",
      "5397 [D loss: 1.003079] [G loss: 1.000083]\n",
      "5398 [D loss: 1.003181] [G loss: 1.000302]\n",
      "5399 [D loss: 1.003035] [G loss: 0.999900]\n",
      "5400 [D loss: 1.003028] [G loss: 1.000089]\n",
      "5401 [D loss: 1.003041] [G loss: 1.000311]\n",
      "5402 [D loss: 1.003194] [G loss: 1.000221]\n",
      "5403 [D loss: 1.003074] [G loss: 1.000200]\n",
      "5404 [D loss: 1.003021] [G loss: 1.000098]\n",
      "5405 [D loss: 1.003414] [G loss: 1.000076]\n",
      "5406 [D loss: 1.003320] [G loss: 1.000134]\n",
      "5407 [D loss: 1.002885] [G loss: 1.000259]\n",
      "5408 [D loss: 1.003185] [G loss: 1.000310]\n",
      "5409 [D loss: 1.003157] [G loss: 1.000086]\n",
      "5410 [D loss: 1.003051] [G loss: 1.000305]\n",
      "5411 [D loss: 1.003142] [G loss: 1.000049]\n",
      "5412 [D loss: 1.003195] [G loss: 1.000229]\n",
      "5413 [D loss: 1.002933] [G loss: 1.000243]\n",
      "5414 [D loss: 1.003305] [G loss: 1.000139]\n",
      "5415 [D loss: 1.003088] [G loss: 1.000322]\n",
      "5416 [D loss: 1.003302] [G loss: 1.000197]\n",
      "5417 [D loss: 1.003217] [G loss: 1.000271]\n",
      "5418 [D loss: 1.003029] [G loss: 1.000146]\n",
      "5419 [D loss: 1.003043] [G loss: 1.000245]\n",
      "5420 [D loss: 1.003155] [G loss: 1.000240]\n",
      "5421 [D loss: 1.003118] [G loss: 1.000014]\n",
      "5422 [D loss: 1.003095] [G loss: 1.000251]\n",
      "5423 [D loss: 1.003225] [G loss: 0.999785]\n",
      "5424 [D loss: 1.003088] [G loss: 1.000098]\n",
      "5425 [D loss: 1.003066] [G loss: 1.000205]\n",
      "5426 [D loss: 1.002935] [G loss: 1.000224]\n",
      "5427 [D loss: 1.003077] [G loss: 1.000392]\n",
      "5428 [D loss: 1.003306] [G loss: 1.000065]\n",
      "5429 [D loss: 1.003155] [G loss: 1.000325]\n",
      "5430 [D loss: 1.003142] [G loss: 1.000204]\n",
      "5431 [D loss: 1.003208] [G loss: 1.000263]\n",
      "5432 [D loss: 1.003066] [G loss: 1.000229]\n",
      "5433 [D loss: 1.003229] [G loss: 0.999923]\n",
      "5434 [D loss: 1.002999] [G loss: 1.000197]\n",
      "5435 [D loss: 1.003048] [G loss: 1.000148]\n",
      "5436 [D loss: 1.003133] [G loss: 1.000294]\n",
      "5437 [D loss: 1.003155] [G loss: 1.000285]\n",
      "5438 [D loss: 1.003032] [G loss: 1.000165]\n",
      "5439 [D loss: 1.003115] [G loss: 1.000134]\n",
      "5440 [D loss: 1.003175] [G loss: 1.000529]\n",
      "5441 [D loss: 1.003414] [G loss: 1.000030]\n",
      "5442 [D loss: 1.003009] [G loss: 1.000151]\n",
      "5443 [D loss: 1.003207] [G loss: 1.000353]\n",
      "5444 [D loss: 1.002911] [G loss: 1.000192]\n",
      "5445 [D loss: 1.003157] [G loss: 1.000520]\n",
      "5446 [D loss: 1.002955] [G loss: 1.000084]\n",
      "5447 [D loss: 1.003109] [G loss: 1.000398]\n",
      "5448 [D loss: 1.003119] [G loss: 1.000211]\n",
      "5449 [D loss: 1.003219] [G loss: 1.000290]\n",
      "5450 [D loss: 1.002968] [G loss: 1.000217]\n",
      "5451 [D loss: 1.003371] [G loss: 1.000142]\n",
      "5452 [D loss: 1.003223] [G loss: 1.000028]\n",
      "5453 [D loss: 1.003250] [G loss: 1.000346]\n",
      "5454 [D loss: 1.002958] [G loss: 1.000233]\n",
      "5455 [D loss: 1.003197] [G loss: 0.999990]\n",
      "5456 [D loss: 1.003203] [G loss: 0.999812]\n",
      "5457 [D loss: 1.003121] [G loss: 1.000260]\n",
      "5458 [D loss: 1.002997] [G loss: 1.000332]\n",
      "5459 [D loss: 1.003236] [G loss: 1.000132]\n",
      "5460 [D loss: 1.003117] [G loss: 1.000342]\n",
      "5461 [D loss: 1.003139] [G loss: 0.999977]\n",
      "5462 [D loss: 1.003095] [G loss: 1.000195]\n",
      "5463 [D loss: 1.003144] [G loss: 1.000385]\n",
      "5464 [D loss: 1.003064] [G loss: 1.000154]\n",
      "5465 [D loss: 1.002955] [G loss: 1.000346]\n",
      "5466 [D loss: 1.003264] [G loss: 1.000265]\n",
      "5467 [D loss: 1.003069] [G loss: 1.000443]\n",
      "5468 [D loss: 1.003152] [G loss: 1.000108]\n",
      "5469 [D loss: 1.003161] [G loss: 1.000218]\n",
      "5470 [D loss: 1.003053] [G loss: 1.000290]\n",
      "5471 [D loss: 1.003024] [G loss: 1.000142]\n",
      "5472 [D loss: 1.003140] [G loss: 1.000303]\n",
      "5473 [D loss: 1.003227] [G loss: 1.000345]\n",
      "5474 [D loss: 1.003327] [G loss: 1.000315]\n",
      "5475 [D loss: 1.003173] [G loss: 1.000409]\n",
      "5476 [D loss: 1.003016] [G loss: 1.000294]\n",
      "5477 [D loss: 1.003448] [G loss: 1.000087]\n",
      "5478 [D loss: 1.003010] [G loss: 1.000398]\n",
      "5479 [D loss: 1.003008] [G loss: 1.000445]\n",
      "5480 [D loss: 1.003237] [G loss: 1.000375]\n",
      "5481 [D loss: 1.002995] [G loss: 1.000356]\n",
      "5482 [D loss: 1.003304] [G loss: 1.000301]\n",
      "5483 [D loss: 1.003291] [G loss: 1.000276]\n",
      "5484 [D loss: 1.003311] [G loss: 1.000302]\n",
      "5485 [D loss: 1.003021] [G loss: 1.000258]\n",
      "5486 [D loss: 1.002879] [G loss: 1.000595]\n",
      "5487 [D loss: 1.003242] [G loss: 1.000287]\n",
      "5488 [D loss: 1.002976] [G loss: 1.000229]\n",
      "5489 [D loss: 1.003219] [G loss: 1.000197]\n",
      "5490 [D loss: 1.003133] [G loss: 1.000266]\n",
      "5491 [D loss: 1.003069] [G loss: 1.000353]\n",
      "5492 [D loss: 1.003219] [G loss: 1.000259]\n",
      "5493 [D loss: 1.003091] [G loss: 1.000635]\n",
      "5494 [D loss: 1.003214] [G loss: 1.000143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5495 [D loss: 1.003279] [G loss: 1.000525]\n",
      "5496 [D loss: 1.003154] [G loss: 1.000280]\n",
      "5497 [D loss: 1.003225] [G loss: 1.000074]\n",
      "5498 [D loss: 1.003045] [G loss: 1.000462]\n",
      "5499 [D loss: 1.003024] [G loss: 1.000467]\n",
      "5500 [D loss: 1.003138] [G loss: 0.999999]\n",
      "5501 [D loss: 1.003124] [G loss: 1.000087]\n",
      "5502 [D loss: 1.003121] [G loss: 1.000112]\n",
      "5503 [D loss: 1.003073] [G loss: 1.000358]\n",
      "5504 [D loss: 1.003086] [G loss: 1.000343]\n",
      "5505 [D loss: 1.002739] [G loss: 1.000095]\n",
      "5506 [D loss: 1.002925] [G loss: 1.000349]\n",
      "5507 [D loss: 1.003337] [G loss: 1.000133]\n",
      "5508 [D loss: 1.003218] [G loss: 1.000149]\n",
      "5509 [D loss: 1.003095] [G loss: 1.000190]\n",
      "5510 [D loss: 1.003318] [G loss: 1.000181]\n",
      "5511 [D loss: 1.003089] [G loss: 1.000268]\n",
      "5512 [D loss: 1.003312] [G loss: 1.000189]\n",
      "5513 [D loss: 1.003040] [G loss: 1.000476]\n",
      "5514 [D loss: 1.003051] [G loss: 1.000312]\n",
      "5515 [D loss: 1.003355] [G loss: 1.000625]\n",
      "5516 [D loss: 1.003253] [G loss: 1.000356]\n",
      "5517 [D loss: 1.003096] [G loss: 1.000283]\n",
      "5518 [D loss: 1.003311] [G loss: 0.999958]\n",
      "5519 [D loss: 1.003164] [G loss: 1.000286]\n",
      "5520 [D loss: 1.003136] [G loss: 1.000332]\n",
      "5521 [D loss: 1.003430] [G loss: 1.000151]\n",
      "5522 [D loss: 1.003190] [G loss: 1.000278]\n",
      "5523 [D loss: 1.003027] [G loss: 1.000384]\n",
      "5524 [D loss: 1.003156] [G loss: 1.000276]\n",
      "5525 [D loss: 1.003157] [G loss: 1.000211]\n",
      "5526 [D loss: 1.003206] [G loss: 1.000326]\n",
      "5527 [D loss: 1.003054] [G loss: 1.000163]\n",
      "5528 [D loss: 1.003218] [G loss: 1.000316]\n",
      "5529 [D loss: 1.002882] [G loss: 1.000481]\n",
      "5530 [D loss: 1.003304] [G loss: 1.000542]\n",
      "5531 [D loss: 1.003207] [G loss: 1.000427]\n",
      "5532 [D loss: 1.003248] [G loss: 1.000185]\n",
      "5533 [D loss: 1.003056] [G loss: 1.000418]\n",
      "5534 [D loss: 1.002992] [G loss: 1.000181]\n",
      "5535 [D loss: 1.003216] [G loss: 1.000181]\n",
      "5536 [D loss: 1.003186] [G loss: 1.000455]\n",
      "5537 [D loss: 1.003224] [G loss: 1.000554]\n",
      "5538 [D loss: 1.003015] [G loss: 1.000274]\n",
      "5539 [D loss: 1.003084] [G loss: 1.000306]\n",
      "5540 [D loss: 1.003206] [G loss: 1.000299]\n",
      "5541 [D loss: 1.003205] [G loss: 1.000444]\n",
      "5542 [D loss: 1.003096] [G loss: 1.000402]\n",
      "5543 [D loss: 1.003071] [G loss: 1.000463]\n",
      "5544 [D loss: 1.003075] [G loss: 1.000430]\n",
      "5545 [D loss: 1.002920] [G loss: 1.000175]\n",
      "5546 [D loss: 1.003007] [G loss: 1.000419]\n",
      "5547 [D loss: 1.003068] [G loss: 1.000321]\n",
      "5548 [D loss: 1.002952] [G loss: 1.000301]\n",
      "5549 [D loss: 1.002951] [G loss: 1.000200]\n",
      "5550 [D loss: 1.003112] [G loss: 1.000477]\n",
      "5551 [D loss: 1.002992] [G loss: 1.000409]\n",
      "5552 [D loss: 1.003089] [G loss: 1.000240]\n",
      "5553 [D loss: 1.003213] [G loss: 1.000349]\n",
      "5554 [D loss: 1.003096] [G loss: 1.000280]\n",
      "5555 [D loss: 1.003201] [G loss: 1.000139]\n",
      "5556 [D loss: 1.003073] [G loss: 1.000268]\n",
      "5557 [D loss: 1.003085] [G loss: 1.000246]\n",
      "5558 [D loss: 1.003139] [G loss: 1.000484]\n",
      "5559 [D loss: 1.003075] [G loss: 1.000288]\n",
      "5560 [D loss: 1.003012] [G loss: 1.000691]\n",
      "5561 [D loss: 1.003153] [G loss: 1.000264]\n",
      "5562 [D loss: 1.003120] [G loss: 1.000255]\n",
      "5563 [D loss: 1.003194] [G loss: 1.000608]\n",
      "5564 [D loss: 1.003049] [G loss: 1.000514]\n",
      "5565 [D loss: 1.003345] [G loss: 1.000590]\n",
      "5566 [D loss: 1.003142] [G loss: 1.000328]\n",
      "5567 [D loss: 1.003004] [G loss: 1.000063]\n",
      "5568 [D loss: 1.003038] [G loss: 1.000266]\n",
      "5569 [D loss: 1.003108] [G loss: 1.000248]\n",
      "5570 [D loss: 1.002925] [G loss: 1.000554]\n",
      "5571 [D loss: 1.003315] [G loss: 1.000118]\n",
      "5572 [D loss: 1.003056] [G loss: 1.000336]\n",
      "5573 [D loss: 1.002937] [G loss: 1.000348]\n",
      "5574 [D loss: 1.003261] [G loss: 1.000445]\n",
      "5575 [D loss: 1.003032] [G loss: 1.000138]\n",
      "5576 [D loss: 1.002916] [G loss: 1.000409]\n",
      "5577 [D loss: 1.003151] [G loss: 1.000307]\n",
      "5578 [D loss: 1.003074] [G loss: 1.000583]\n",
      "5579 [D loss: 1.003064] [G loss: 1.000434]\n",
      "5580 [D loss: 1.003184] [G loss: 1.000269]\n",
      "5581 [D loss: 1.002977] [G loss: 1.000085]\n",
      "5582 [D loss: 1.003215] [G loss: 1.000275]\n",
      "5583 [D loss: 1.003268] [G loss: 1.000187]\n",
      "5584 [D loss: 1.003151] [G loss: 1.000374]\n",
      "5585 [D loss: 1.002962] [G loss: 1.000170]\n",
      "5586 [D loss: 1.003015] [G loss: 1.000382]\n",
      "5587 [D loss: 1.003040] [G loss: 1.000311]\n",
      "5588 [D loss: 1.003100] [G loss: 1.000076]\n",
      "5589 [D loss: 1.003119] [G loss: 1.000631]\n",
      "5590 [D loss: 1.003148] [G loss: 1.000347]\n",
      "5591 [D loss: 1.003055] [G loss: 1.000274]\n",
      "5592 [D loss: 1.003198] [G loss: 1.000276]\n",
      "5593 [D loss: 1.003058] [G loss: 1.000479]\n",
      "5594 [D loss: 1.003189] [G loss: 1.000363]\n",
      "5595 [D loss: 1.003200] [G loss: 1.000571]\n",
      "5596 [D loss: 1.003054] [G loss: 1.000316]\n",
      "5597 [D loss: 1.003153] [G loss: 1.000257]\n",
      "5598 [D loss: 1.003028] [G loss: 1.000392]\n",
      "5599 [D loss: 1.003293] [G loss: 1.000456]\n",
      "5600 [D loss: 1.002944] [G loss: 1.000418]\n",
      "5601 [D loss: 1.003042] [G loss: 1.000518]\n",
      "5602 [D loss: 1.003151] [G loss: 1.000290]\n",
      "5603 [D loss: 1.002997] [G loss: 1.000427]\n",
      "5604 [D loss: 1.002983] [G loss: 1.000292]\n",
      "5605 [D loss: 1.002930] [G loss: 1.000292]\n",
      "5606 [D loss: 1.003095] [G loss: 1.000307]\n",
      "5607 [D loss: 1.002831] [G loss: 1.000486]\n",
      "5608 [D loss: 1.003006] [G loss: 1.000201]\n",
      "5609 [D loss: 1.003292] [G loss: 1.000413]\n",
      "5610 [D loss: 1.002839] [G loss: 1.000313]\n",
      "5611 [D loss: 1.002880] [G loss: 1.000529]\n",
      "5612 [D loss: 1.002941] [G loss: 1.000657]\n",
      "5613 [D loss: 1.003095] [G loss: 1.000534]\n",
      "5614 [D loss: 1.002937] [G loss: 1.000548]\n",
      "5615 [D loss: 1.003223] [G loss: 1.000139]\n",
      "5616 [D loss: 1.003245] [G loss: 1.000043]\n",
      "5617 [D loss: 1.003348] [G loss: 1.000216]\n",
      "5618 [D loss: 1.003176] [G loss: 1.000474]\n",
      "5619 [D loss: 1.003158] [G loss: 1.000391]\n",
      "5620 [D loss: 1.003067] [G loss: 1.000213]\n",
      "5621 [D loss: 1.003102] [G loss: 1.000424]\n",
      "5622 [D loss: 1.003013] [G loss: 1.000451]\n",
      "5623 [D loss: 1.003224] [G loss: 1.000064]\n",
      "5624 [D loss: 1.003002] [G loss: 1.000554]\n",
      "5625 [D loss: 1.003054] [G loss: 1.000206]\n",
      "5626 [D loss: 1.003325] [G loss: 1.000424]\n",
      "5627 [D loss: 1.003107] [G loss: 1.000324]\n",
      "5628 [D loss: 1.003214] [G loss: 1.000271]\n",
      "5629 [D loss: 1.003071] [G loss: 1.000388]\n",
      "5630 [D loss: 1.003113] [G loss: 1.000442]\n",
      "5631 [D loss: 1.002954] [G loss: 1.000635]\n",
      "5632 [D loss: 1.003119] [G loss: 1.000329]\n",
      "5633 [D loss: 1.003314] [G loss: 1.000543]\n",
      "5634 [D loss: 1.003074] [G loss: 1.000289]\n",
      "5635 [D loss: 1.003000] [G loss: 1.000402]\n",
      "5636 [D loss: 1.003150] [G loss: 1.000373]\n",
      "5637 [D loss: 1.003089] [G loss: 1.000229]\n",
      "5638 [D loss: 1.003009] [G loss: 1.000320]\n",
      "5639 [D loss: 1.003017] [G loss: 1.000486]\n",
      "5640 [D loss: 1.003257] [G loss: 1.000390]\n",
      "5641 [D loss: 1.003187] [G loss: 1.000433]\n",
      "5642 [D loss: 1.003320] [G loss: 1.000464]\n",
      "5643 [D loss: 1.003113] [G loss: 1.000097]\n",
      "5644 [D loss: 1.003142] [G loss: 1.000316]\n",
      "5645 [D loss: 1.003245] [G loss: 1.000106]\n",
      "5646 [D loss: 1.003153] [G loss: 1.000305]\n",
      "5647 [D loss: 1.003023] [G loss: 1.000308]\n",
      "5648 [D loss: 1.003357] [G loss: 1.000588]\n",
      "5649 [D loss: 1.003118] [G loss: 1.000555]\n",
      "5650 [D loss: 1.003173] [G loss: 1.000527]\n",
      "5651 [D loss: 1.003219] [G loss: 1.000272]\n",
      "5652 [D loss: 1.003127] [G loss: 1.000275]\n",
      "5653 [D loss: 1.003040] [G loss: 1.000435]\n",
      "5654 [D loss: 1.003030] [G loss: 1.000209]\n",
      "5655 [D loss: 1.003066] [G loss: 1.000302]\n",
      "5656 [D loss: 1.003240] [G loss: 1.000349]\n",
      "5657 [D loss: 1.003125] [G loss: 1.000504]\n",
      "5658 [D loss: 1.003206] [G loss: 1.000303]\n",
      "5659 [D loss: 1.003347] [G loss: 1.000397]\n",
      "5660 [D loss: 1.003023] [G loss: 1.000578]\n",
      "5661 [D loss: 1.003289] [G loss: 1.000365]\n",
      "5662 [D loss: 1.003222] [G loss: 1.000194]\n",
      "5663 [D loss: 1.002947] [G loss: 1.000524]\n",
      "5664 [D loss: 1.003250] [G loss: 1.000376]\n",
      "5665 [D loss: 1.003272] [G loss: 1.000289]\n",
      "5666 [D loss: 1.003177] [G loss: 1.000372]\n",
      "5667 [D loss: 1.003098] [G loss: 1.000384]\n",
      "5668 [D loss: 1.003131] [G loss: 1.000468]\n",
      "5669 [D loss: 1.003122] [G loss: 1.000399]\n",
      "5670 [D loss: 1.003248] [G loss: 1.000336]\n",
      "5671 [D loss: 1.003208] [G loss: 1.000417]\n",
      "5672 [D loss: 1.003018] [G loss: 1.000299]\n",
      "5673 [D loss: 1.003019] [G loss: 1.000362]\n",
      "5674 [D loss: 1.002895] [G loss: 1.000381]\n",
      "5675 [D loss: 1.003249] [G loss: 1.000294]\n",
      "5676 [D loss: 1.002986] [G loss: 1.000443]\n",
      "5677 [D loss: 1.003344] [G loss: 1.000066]\n",
      "5678 [D loss: 1.003162] [G loss: 1.000398]\n",
      "5679 [D loss: 1.002961] [G loss: 1.000434]\n",
      "5680 [D loss: 1.003130] [G loss: 1.000301]\n",
      "5681 [D loss: 1.003216] [G loss: 1.000210]\n",
      "5682 [D loss: 1.003205] [G loss: 1.000391]\n",
      "5683 [D loss: 1.003194] [G loss: 1.000214]\n",
      "5684 [D loss: 1.003200] [G loss: 1.000321]\n",
      "5685 [D loss: 1.003142] [G loss: 1.000376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5686 [D loss: 1.003280] [G loss: 1.000301]\n",
      "5687 [D loss: 1.003193] [G loss: 1.000257]\n",
      "5688 [D loss: 1.003160] [G loss: 1.000141]\n",
      "5689 [D loss: 1.003104] [G loss: 1.000358]\n",
      "5690 [D loss: 1.003234] [G loss: 1.000349]\n",
      "5691 [D loss: 1.003006] [G loss: 1.000516]\n",
      "5692 [D loss: 1.003094] [G loss: 1.000593]\n",
      "5693 [D loss: 1.002928] [G loss: 1.000123]\n",
      "5694 [D loss: 1.003119] [G loss: 1.000214]\n",
      "5695 [D loss: 1.002933] [G loss: 1.000265]\n",
      "5696 [D loss: 1.003070] [G loss: 1.000062]\n",
      "5697 [D loss: 1.003120] [G loss: 1.000385]\n",
      "5698 [D loss: 1.003197] [G loss: 1.000536]\n",
      "5699 [D loss: 1.003060] [G loss: 1.000244]\n",
      "5700 [D loss: 1.003156] [G loss: 1.000518]\n",
      "5701 [D loss: 1.003353] [G loss: 1.000367]\n",
      "5702 [D loss: 1.003099] [G loss: 1.000235]\n",
      "5703 [D loss: 1.003029] [G loss: 1.000067]\n",
      "5704 [D loss: 1.002934] [G loss: 1.000221]\n",
      "5705 [D loss: 1.002947] [G loss: 1.000206]\n",
      "5706 [D loss: 1.003113] [G loss: 1.000262]\n",
      "5707 [D loss: 1.002991] [G loss: 1.000383]\n",
      "5708 [D loss: 1.003054] [G loss: 1.000375]\n",
      "5709 [D loss: 1.003167] [G loss: 1.000410]\n",
      "5710 [D loss: 1.003060] [G loss: 1.000379]\n",
      "5711 [D loss: 1.003188] [G loss: 1.000512]\n",
      "5712 [D loss: 1.003044] [G loss: 1.000434]\n",
      "5713 [D loss: 1.002889] [G loss: 1.000372]\n",
      "5714 [D loss: 1.003139] [G loss: 1.000660]\n",
      "5715 [D loss: 1.003210] [G loss: 1.000448]\n",
      "5716 [D loss: 1.003138] [G loss: 1.000503]\n",
      "5717 [D loss: 1.003001] [G loss: 1.000592]\n",
      "5718 [D loss: 1.003212] [G loss: 1.000380]\n",
      "5719 [D loss: 1.003216] [G loss: 1.000508]\n",
      "5720 [D loss: 1.003327] [G loss: 1.000430]\n",
      "5721 [D loss: 1.002986] [G loss: 1.000522]\n",
      "5722 [D loss: 1.003269] [G loss: 1.000598]\n",
      "5723 [D loss: 1.003378] [G loss: 1.000302]\n",
      "5724 [D loss: 1.003257] [G loss: 1.000157]\n",
      "5725 [D loss: 1.002963] [G loss: 1.000408]\n",
      "5726 [D loss: 1.003228] [G loss: 1.000145]\n",
      "5727 [D loss: 1.003184] [G loss: 1.000405]\n",
      "5728 [D loss: 1.002979] [G loss: 1.000225]\n",
      "5729 [D loss: 1.003153] [G loss: 1.000193]\n",
      "5730 [D loss: 1.003201] [G loss: 1.000239]\n",
      "5731 [D loss: 1.003214] [G loss: 1.000368]\n",
      "5732 [D loss: 1.003280] [G loss: 1.000273]\n",
      "5733 [D loss: 1.003213] [G loss: 1.000340]\n",
      "5734 [D loss: 1.003149] [G loss: 1.000146]\n",
      "5735 [D loss: 1.003049] [G loss: 1.000176]\n",
      "5736 [D loss: 1.003169] [G loss: 1.000236]\n",
      "5737 [D loss: 1.003174] [G loss: 1.000214]\n",
      "5738 [D loss: 1.002963] [G loss: 1.000433]\n",
      "5739 [D loss: 1.002879] [G loss: 1.000374]\n",
      "5740 [D loss: 1.003232] [G loss: 1.000488]\n",
      "5741 [D loss: 1.003182] [G loss: 1.000202]\n",
      "5742 [D loss: 1.003169] [G loss: 1.000109]\n",
      "5743 [D loss: 1.002997] [G loss: 1.000206]\n",
      "5744 [D loss: 1.003080] [G loss: 1.000397]\n",
      "5745 [D loss: 1.003105] [G loss: 1.000211]\n",
      "5746 [D loss: 1.003100] [G loss: 1.000658]\n",
      "5747 [D loss: 1.003116] [G loss: 1.000540]\n",
      "5748 [D loss: 1.002959] [G loss: 1.000507]\n",
      "5749 [D loss: 1.003116] [G loss: 1.000401]\n",
      "5750 [D loss: 1.003313] [G loss: 1.000296]\n",
      "5751 [D loss: 1.002872] [G loss: 1.000174]\n",
      "5752 [D loss: 1.003239] [G loss: 1.000478]\n",
      "5753 [D loss: 1.003001] [G loss: 1.000281]\n",
      "5754 [D loss: 1.002965] [G loss: 1.000199]\n",
      "5755 [D loss: 1.003017] [G loss: 1.000240]\n",
      "5756 [D loss: 1.003113] [G loss: 1.000266]\n",
      "5757 [D loss: 1.003070] [G loss: 1.000256]\n",
      "5758 [D loss: 1.002817] [G loss: 1.000399]\n",
      "5759 [D loss: 1.003018] [G loss: 1.000402]\n",
      "5760 [D loss: 1.003123] [G loss: 1.000532]\n",
      "5761 [D loss: 1.003097] [G loss: 1.000557]\n",
      "5762 [D loss: 1.002962] [G loss: 1.000444]\n",
      "5763 [D loss: 1.003112] [G loss: 1.000274]\n",
      "5764 [D loss: 1.003475] [G loss: 1.000163]\n",
      "5765 [D loss: 1.003233] [G loss: 1.000387]\n",
      "5766 [D loss: 1.002913] [G loss: 1.000236]\n",
      "5767 [D loss: 1.003055] [G loss: 1.000532]\n",
      "5768 [D loss: 1.003063] [G loss: 1.000543]\n",
      "5769 [D loss: 1.003319] [G loss: 1.000403]\n",
      "5770 [D loss: 1.002933] [G loss: 1.000418]\n",
      "5771 [D loss: 1.003101] [G loss: 1.000187]\n",
      "5772 [D loss: 1.003123] [G loss: 1.000434]\n",
      "5773 [D loss: 1.003004] [G loss: 1.000275]\n",
      "5774 [D loss: 1.003082] [G loss: 1.000305]\n",
      "5775 [D loss: 1.003002] [G loss: 1.000079]\n",
      "5776 [D loss: 1.003294] [G loss: 1.000359]\n",
      "5777 [D loss: 1.003268] [G loss: 1.000566]\n",
      "5778 [D loss: 1.003063] [G loss: 1.000589]\n",
      "5779 [D loss: 1.003371] [G loss: 1.000276]\n",
      "5780 [D loss: 1.003154] [G loss: 1.000585]\n",
      "5781 [D loss: 1.002958] [G loss: 1.000569]\n",
      "5782 [D loss: 1.002982] [G loss: 1.000471]\n",
      "5783 [D loss: 1.003239] [G loss: 1.000339]\n",
      "5784 [D loss: 1.003086] [G loss: 1.000459]\n",
      "5785 [D loss: 1.002953] [G loss: 1.000293]\n",
      "5786 [D loss: 1.002952] [G loss: 1.000410]\n",
      "5787 [D loss: 1.003116] [G loss: 1.000382]\n",
      "5788 [D loss: 1.003130] [G loss: 1.000433]\n",
      "5789 [D loss: 1.002938] [G loss: 1.000331]\n",
      "5790 [D loss: 1.002988] [G loss: 1.000117]\n",
      "5791 [D loss: 1.003168] [G loss: 1.000303]\n",
      "5792 [D loss: 1.003179] [G loss: 1.000413]\n",
      "5793 [D loss: 1.003215] [G loss: 1.000406]\n",
      "5794 [D loss: 1.003005] [G loss: 1.000517]\n",
      "5795 [D loss: 1.002857] [G loss: 1.000368]\n",
      "5796 [D loss: 1.002907] [G loss: 1.000347]\n",
      "5797 [D loss: 1.003144] [G loss: 1.000187]\n",
      "5798 [D loss: 1.003123] [G loss: 1.000395]\n",
      "5799 [D loss: 1.003235] [G loss: 1.000339]\n",
      "5800 [D loss: 1.002976] [G loss: 1.000282]\n",
      "5801 [D loss: 1.003491] [G loss: 1.000050]\n",
      "5802 [D loss: 1.003190] [G loss: 1.000404]\n",
      "5803 [D loss: 1.003141] [G loss: 1.000596]\n",
      "5804 [D loss: 1.003380] [G loss: 1.000271]\n",
      "5805 [D loss: 1.003206] [G loss: 1.000461]\n",
      "5806 [D loss: 1.003101] [G loss: 1.000615]\n",
      "5807 [D loss: 1.003157] [G loss: 1.000350]\n",
      "5808 [D loss: 1.003144] [G loss: 1.000126]\n",
      "5809 [D loss: 1.003047] [G loss: 1.000617]\n",
      "5810 [D loss: 1.003135] [G loss: 1.000167]\n",
      "5811 [D loss: 1.003119] [G loss: 1.000402]\n",
      "5812 [D loss: 1.003206] [G loss: 1.000379]\n",
      "5813 [D loss: 1.002766] [G loss: 1.000191]\n",
      "5814 [D loss: 1.003076] [G loss: 1.000383]\n",
      "5815 [D loss: 1.003204] [G loss: 1.000182]\n",
      "5816 [D loss: 1.003061] [G loss: 1.000456]\n",
      "5817 [D loss: 1.003123] [G loss: 1.000374]\n",
      "5818 [D loss: 1.003094] [G loss: 1.000033]\n",
      "5819 [D loss: 1.003194] [G loss: 1.000265]\n",
      "5820 [D loss: 1.003356] [G loss: 1.000319]\n",
      "5821 [D loss: 1.003060] [G loss: 1.000322]\n",
      "5822 [D loss: 1.003164] [G loss: 1.000297]\n",
      "5823 [D loss: 1.003153] [G loss: 1.000382]\n",
      "5824 [D loss: 1.003141] [G loss: 1.000318]\n",
      "5825 [D loss: 1.002994] [G loss: 1.000435]\n",
      "5826 [D loss: 1.003282] [G loss: 1.000456]\n",
      "5827 [D loss: 1.003032] [G loss: 1.000357]\n",
      "5828 [D loss: 1.003241] [G loss: 1.000170]\n",
      "5829 [D loss: 1.003118] [G loss: 1.000085]\n",
      "5830 [D loss: 1.003084] [G loss: 1.000027]\n",
      "5831 [D loss: 1.003227] [G loss: 1.000286]\n",
      "5832 [D loss: 1.002801] [G loss: 1.000179]\n",
      "5833 [D loss: 1.002939] [G loss: 1.000560]\n",
      "5834 [D loss: 1.003073] [G loss: 1.000517]\n",
      "5835 [D loss: 1.003089] [G loss: 1.000274]\n",
      "5836 [D loss: 1.003019] [G loss: 1.000180]\n",
      "5837 [D loss: 1.003280] [G loss: 1.000219]\n",
      "5838 [D loss: 1.003010] [G loss: 1.000502]\n",
      "5839 [D loss: 1.003103] [G loss: 1.000244]\n",
      "5840 [D loss: 1.003082] [G loss: 1.000412]\n",
      "5841 [D loss: 1.003072] [G loss: 1.000372]\n",
      "5842 [D loss: 1.003298] [G loss: 1.000251]\n",
      "5843 [D loss: 1.003251] [G loss: 0.999984]\n",
      "5844 [D loss: 1.003142] [G loss: 1.000234]\n",
      "5845 [D loss: 1.003112] [G loss: 1.000528]\n",
      "5846 [D loss: 1.003085] [G loss: 1.000658]\n",
      "5847 [D loss: 1.003027] [G loss: 1.000345]\n",
      "5848 [D loss: 1.003111] [G loss: 1.000582]\n",
      "5849 [D loss: 1.003016] [G loss: 1.000329]\n",
      "5850 [D loss: 1.003396] [G loss: 1.000544]\n",
      "5851 [D loss: 1.002995] [G loss: 1.000287]\n",
      "5852 [D loss: 1.003205] [G loss: 1.000226]\n",
      "5853 [D loss: 1.003101] [G loss: 1.000183]\n",
      "5854 [D loss: 1.003055] [G loss: 1.000378]\n",
      "5855 [D loss: 1.003094] [G loss: 1.000526]\n",
      "5856 [D loss: 1.003195] [G loss: 1.000499]\n",
      "5857 [D loss: 1.003116] [G loss: 1.000185]\n",
      "5858 [D loss: 1.003202] [G loss: 1.000602]\n",
      "5859 [D loss: 1.003249] [G loss: 1.000260]\n",
      "5860 [D loss: 1.003313] [G loss: 1.000577]\n",
      "5861 [D loss: 1.003320] [G loss: 1.000386]\n",
      "5862 [D loss: 1.003260] [G loss: 1.000860]\n",
      "5863 [D loss: 1.003162] [G loss: 1.000308]\n",
      "5864 [D loss: 1.003216] [G loss: 1.000651]\n",
      "5865 [D loss: 1.003155] [G loss: 1.000313]\n",
      "5866 [D loss: 1.003041] [G loss: 1.000336]\n",
      "5867 [D loss: 1.003131] [G loss: 1.000541]\n",
      "5868 [D loss: 1.003032] [G loss: 1.000412]\n",
      "5869 [D loss: 1.002982] [G loss: 1.000521]\n",
      "5870 [D loss: 1.003393] [G loss: 1.000298]\n",
      "5871 [D loss: 1.003231] [G loss: 1.000437]\n",
      "5872 [D loss: 1.003007] [G loss: 1.000201]\n",
      "5873 [D loss: 1.003121] [G loss: 1.000362]\n",
      "5874 [D loss: 1.002990] [G loss: 1.000577]\n",
      "5875 [D loss: 1.003311] [G loss: 1.000596]\n",
      "5876 [D loss: 1.003083] [G loss: 1.000321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5877 [D loss: 1.003211] [G loss: 1.000780]\n",
      "5878 [D loss: 1.003060] [G loss: 1.000266]\n",
      "5879 [D loss: 1.003060] [G loss: 1.000814]\n",
      "5880 [D loss: 1.003004] [G loss: 1.000488]\n",
      "5881 [D loss: 1.003198] [G loss: 1.000569]\n",
      "5882 [D loss: 1.003200] [G loss: 1.000392]\n",
      "5883 [D loss: 1.002711] [G loss: 1.000004]\n",
      "5884 [D loss: 1.003200] [G loss: 1.000473]\n",
      "5885 [D loss: 1.003217] [G loss: 1.000464]\n",
      "5886 [D loss: 1.002970] [G loss: 1.000533]\n",
      "5887 [D loss: 1.003106] [G loss: 1.000570]\n",
      "5888 [D loss: 1.003273] [G loss: 1.000297]\n",
      "5889 [D loss: 1.003240] [G loss: 1.000235]\n",
      "5890 [D loss: 1.002981] [G loss: 1.000481]\n",
      "5891 [D loss: 1.003154] [G loss: 1.000397]\n",
      "5892 [D loss: 1.003178] [G loss: 1.000397]\n",
      "5893 [D loss: 1.003098] [G loss: 1.000457]\n",
      "5894 [D loss: 1.003145] [G loss: 1.000603]\n",
      "5895 [D loss: 1.003171] [G loss: 1.000613]\n",
      "5896 [D loss: 1.003152] [G loss: 1.000491]\n",
      "5897 [D loss: 1.003218] [G loss: 1.000276]\n",
      "5898 [D loss: 1.002970] [G loss: 1.000268]\n",
      "5899 [D loss: 1.003033] [G loss: 1.000492]\n",
      "5900 [D loss: 1.003171] [G loss: 1.000596]\n",
      "5901 [D loss: 1.003215] [G loss: 1.000717]\n",
      "5902 [D loss: 1.003314] [G loss: 1.000560]\n",
      "5903 [D loss: 1.003250] [G loss: 1.000257]\n",
      "5904 [D loss: 1.003266] [G loss: 1.000520]\n",
      "5905 [D loss: 1.003403] [G loss: 1.000658]\n",
      "5906 [D loss: 1.003268] [G loss: 1.000340]\n",
      "5907 [D loss: 1.003137] [G loss: 1.000419]\n",
      "5908 [D loss: 1.003088] [G loss: 1.000436]\n",
      "5909 [D loss: 1.003026] [G loss: 1.000747]\n",
      "5910 [D loss: 1.003198] [G loss: 1.000441]\n",
      "5911 [D loss: 1.002830] [G loss: 1.000750]\n",
      "5912 [D loss: 1.003169] [G loss: 1.000518]\n",
      "5913 [D loss: 1.002877] [G loss: 1.000534]\n",
      "5914 [D loss: 1.003105] [G loss: 1.000608]\n",
      "5915 [D loss: 1.003225] [G loss: 1.000548]\n",
      "5916 [D loss: 1.003250] [G loss: 1.000393]\n",
      "5917 [D loss: 1.003142] [G loss: 1.000494]\n",
      "5918 [D loss: 1.002875] [G loss: 1.000697]\n",
      "5919 [D loss: 1.003143] [G loss: 1.000366]\n",
      "5920 [D loss: 1.003081] [G loss: 1.000490]\n",
      "5921 [D loss: 1.003150] [G loss: 1.000322]\n",
      "5922 [D loss: 1.003282] [G loss: 1.000487]\n",
      "5923 [D loss: 1.003163] [G loss: 1.000605]\n",
      "5924 [D loss: 1.003086] [G loss: 1.000235]\n",
      "5925 [D loss: 1.003091] [G loss: 1.000553]\n",
      "5926 [D loss: 1.003031] [G loss: 1.000567]\n",
      "5927 [D loss: 1.003160] [G loss: 1.000158]\n",
      "5928 [D loss: 1.002983] [G loss: 1.000289]\n",
      "5929 [D loss: 1.003187] [G loss: 1.000195]\n",
      "5930 [D loss: 1.003153] [G loss: 1.000653]\n",
      "5931 [D loss: 1.003171] [G loss: 1.000496]\n",
      "5932 [D loss: 1.003031] [G loss: 1.000765]\n",
      "5933 [D loss: 1.003033] [G loss: 1.000526]\n",
      "5934 [D loss: 1.003130] [G loss: 1.000386]\n",
      "5935 [D loss: 1.003096] [G loss: 1.000683]\n",
      "5936 [D loss: 1.003014] [G loss: 1.000529]\n",
      "5937 [D loss: 1.003082] [G loss: 1.000735]\n",
      "5938 [D loss: 1.003254] [G loss: 1.000439]\n",
      "5939 [D loss: 1.003318] [G loss: 1.000284]\n",
      "5940 [D loss: 1.003116] [G loss: 1.000522]\n",
      "5941 [D loss: 1.003212] [G loss: 1.000637]\n",
      "5942 [D loss: 1.003183] [G loss: 1.000380]\n",
      "5943 [D loss: 1.003088] [G loss: 1.000704]\n",
      "5944 [D loss: 1.002956] [G loss: 1.000420]\n",
      "5945 [D loss: 1.003211] [G loss: 1.000514]\n",
      "5946 [D loss: 1.003123] [G loss: 1.000416]\n",
      "5947 [D loss: 1.003068] [G loss: 1.000571]\n",
      "5948 [D loss: 1.003112] [G loss: 1.000474]\n",
      "5949 [D loss: 1.003046] [G loss: 1.000703]\n",
      "5950 [D loss: 1.003165] [G loss: 1.000000]\n",
      "5951 [D loss: 1.003068] [G loss: 1.000485]\n",
      "5952 [D loss: 1.002989] [G loss: 1.000374]\n",
      "5953 [D loss: 1.002994] [G loss: 1.000425]\n",
      "5954 [D loss: 1.003340] [G loss: 1.000554]\n",
      "5955 [D loss: 1.003113] [G loss: 1.000411]\n",
      "5956 [D loss: 1.003059] [G loss: 1.000663]\n",
      "5957 [D loss: 1.003106] [G loss: 1.000634]\n",
      "5958 [D loss: 1.002954] [G loss: 1.000146]\n",
      "5959 [D loss: 1.003209] [G loss: 1.000412]\n",
      "5960 [D loss: 1.003068] [G loss: 1.000356]\n",
      "5961 [D loss: 1.003118] [G loss: 1.000347]\n",
      "5962 [D loss: 1.003179] [G loss: 1.000522]\n",
      "5963 [D loss: 1.003067] [G loss: 1.000478]\n",
      "5964 [D loss: 1.003054] [G loss: 1.000369]\n",
      "5965 [D loss: 1.003406] [G loss: 1.000576]\n",
      "5966 [D loss: 1.003300] [G loss: 1.000787]\n",
      "5967 [D loss: 1.003078] [G loss: 1.000594]\n",
      "5968 [D loss: 1.003319] [G loss: 1.000661]\n",
      "5969 [D loss: 1.003285] [G loss: 1.000546]\n",
      "5970 [D loss: 1.002961] [G loss: 1.000446]\n",
      "5971 [D loss: 1.003209] [G loss: 1.000648]\n",
      "5972 [D loss: 1.003133] [G loss: 1.000547]\n",
      "5973 [D loss: 1.003128] [G loss: 1.000551]\n",
      "5974 [D loss: 1.003066] [G loss: 1.000535]\n",
      "5975 [D loss: 1.003163] [G loss: 1.000778]\n",
      "5976 [D loss: 1.003062] [G loss: 1.000536]\n",
      "5977 [D loss: 1.003053] [G loss: 1.000343]\n",
      "5978 [D loss: 1.002833] [G loss: 1.000401]\n",
      "5979 [D loss: 1.003328] [G loss: 1.000458]\n",
      "5980 [D loss: 1.003229] [G loss: 1.000577]\n",
      "5981 [D loss: 1.003202] [G loss: 1.000462]\n",
      "5982 [D loss: 1.003300] [G loss: 1.000310]\n",
      "5983 [D loss: 1.002929] [G loss: 1.000733]\n",
      "5984 [D loss: 1.003386] [G loss: 1.000402]\n",
      "5985 [D loss: 1.002936] [G loss: 1.000676]\n",
      "5986 [D loss: 1.003100] [G loss: 1.000411]\n",
      "5987 [D loss: 1.003080] [G loss: 1.000620]\n",
      "5988 [D loss: 1.003080] [G loss: 1.000342]\n",
      "5989 [D loss: 1.003039] [G loss: 1.000520]\n",
      "5990 [D loss: 1.003004] [G loss: 1.000557]\n",
      "5991 [D loss: 1.003351] [G loss: 1.000214]\n",
      "5992 [D loss: 1.003183] [G loss: 1.000391]\n",
      "5993 [D loss: 1.003117] [G loss: 1.000162]\n",
      "5994 [D loss: 1.003133] [G loss: 1.000583]\n",
      "5995 [D loss: 1.003212] [G loss: 1.000482]\n",
      "5996 [D loss: 1.003057] [G loss: 1.000520]\n",
      "5997 [D loss: 1.003085] [G loss: 1.000577]\n",
      "5998 [D loss: 1.003043] [G loss: 1.000479]\n",
      "5999 [D loss: 1.003239] [G loss: 1.000582]\n",
      "6000 [D loss: 1.003076] [G loss: 1.000624]\n",
      "(14461, 768)\n",
      "6001 [D loss: 1.003106] [G loss: 1.000540]\n",
      "6002 [D loss: 1.003030] [G loss: 1.000637]\n",
      "6003 [D loss: 1.002917] [G loss: 1.000535]\n",
      "6004 [D loss: 1.002967] [G loss: 1.000409]\n",
      "6005 [D loss: 1.002981] [G loss: 1.000531]\n",
      "6006 [D loss: 1.003215] [G loss: 1.000412]\n",
      "6007 [D loss: 1.003293] [G loss: 1.000455]\n",
      "6008 [D loss: 1.003111] [G loss: 1.000574]\n",
      "6009 [D loss: 1.003241] [G loss: 1.000817]\n",
      "6010 [D loss: 1.002907] [G loss: 1.000613]\n",
      "6011 [D loss: 1.003259] [G loss: 1.000310]\n",
      "6012 [D loss: 1.002885] [G loss: 1.000311]\n",
      "6013 [D loss: 1.003108] [G loss: 1.000500]\n",
      "6014 [D loss: 1.002994] [G loss: 1.000663]\n",
      "6015 [D loss: 1.003103] [G loss: 1.000527]\n",
      "6016 [D loss: 1.003354] [G loss: 1.000343]\n",
      "6017 [D loss: 1.003375] [G loss: 1.000518]\n",
      "6018 [D loss: 1.003048] [G loss: 1.000285]\n",
      "6019 [D loss: 1.003101] [G loss: 1.000589]\n",
      "6020 [D loss: 1.003095] [G loss: 1.000337]\n",
      "6021 [D loss: 1.003181] [G loss: 1.000653]\n",
      "6022 [D loss: 1.003232] [G loss: 1.000640]\n",
      "6023 [D loss: 1.002894] [G loss: 1.000701]\n",
      "6024 [D loss: 1.003395] [G loss: 1.000789]\n",
      "6025 [D loss: 1.002932] [G loss: 1.000642]\n",
      "6026 [D loss: 1.002916] [G loss: 1.000543]\n",
      "6027 [D loss: 1.003188] [G loss: 1.000332]\n",
      "6028 [D loss: 1.003086] [G loss: 1.000446]\n",
      "6029 [D loss: 1.003133] [G loss: 1.000186]\n",
      "6030 [D loss: 1.003120] [G loss: 1.000419]\n",
      "6031 [D loss: 1.003066] [G loss: 1.000312]\n",
      "6032 [D loss: 1.003003] [G loss: 1.000551]\n",
      "6033 [D loss: 1.003311] [G loss: 1.000715]\n",
      "6034 [D loss: 1.003177] [G loss: 1.000716]\n",
      "6035 [D loss: 1.003123] [G loss: 1.000695]\n",
      "6036 [D loss: 1.003145] [G loss: 1.000554]\n",
      "6037 [D loss: 1.003024] [G loss: 1.000623]\n",
      "6038 [D loss: 1.003011] [G loss: 1.000463]\n",
      "6039 [D loss: 1.003104] [G loss: 1.000759]\n",
      "6040 [D loss: 1.003334] [G loss: 1.000449]\n",
      "6041 [D loss: 1.003135] [G loss: 1.000591]\n",
      "6042 [D loss: 1.003220] [G loss: 1.000582]\n",
      "6043 [D loss: 1.003212] [G loss: 1.000554]\n",
      "6044 [D loss: 1.003133] [G loss: 1.000533]\n",
      "6045 [D loss: 1.003327] [G loss: 1.000430]\n",
      "6046 [D loss: 1.003019] [G loss: 1.000344]\n",
      "6047 [D loss: 1.003023] [G loss: 1.000529]\n",
      "6048 [D loss: 1.002891] [G loss: 1.000585]\n",
      "6049 [D loss: 1.003093] [G loss: 1.000174]\n",
      "6050 [D loss: 1.003148] [G loss: 1.000496]\n",
      "6051 [D loss: 1.003253] [G loss: 1.000571]\n",
      "6052 [D loss: 1.003045] [G loss: 1.000547]\n",
      "6053 [D loss: 1.003331] [G loss: 1.000314]\n",
      "6054 [D loss: 1.003291] [G loss: 1.000506]\n",
      "6055 [D loss: 1.003420] [G loss: 1.000565]\n",
      "6056 [D loss: 1.003224] [G loss: 1.000426]\n",
      "6057 [D loss: 1.003316] [G loss: 1.000463]\n",
      "6058 [D loss: 1.003103] [G loss: 1.000422]\n",
      "6059 [D loss: 1.003182] [G loss: 1.000800]\n",
      "6060 [D loss: 1.003102] [G loss: 1.000589]\n",
      "6061 [D loss: 1.002991] [G loss: 1.000517]\n",
      "6062 [D loss: 1.002986] [G loss: 1.000481]\n",
      "6063 [D loss: 1.003323] [G loss: 1.000503]\n",
      "6064 [D loss: 1.003067] [G loss: 1.000515]\n",
      "6065 [D loss: 1.003093] [G loss: 1.000694]\n",
      "6066 [D loss: 1.003332] [G loss: 1.000472]\n",
      "6067 [D loss: 1.003146] [G loss: 1.000934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6068 [D loss: 1.003243] [G loss: 1.000370]\n",
      "6069 [D loss: 1.003175] [G loss: 1.000827]\n",
      "6070 [D loss: 1.003038] [G loss: 1.000464]\n",
      "6071 [D loss: 1.003123] [G loss: 1.000495]\n",
      "6072 [D loss: 1.003131] [G loss: 1.000793]\n",
      "6073 [D loss: 1.003385] [G loss: 1.000344]\n",
      "6074 [D loss: 1.003199] [G loss: 1.000430]\n",
      "6075 [D loss: 1.003040] [G loss: 1.000733]\n",
      "6076 [D loss: 1.003251] [G loss: 1.000518]\n",
      "6077 [D loss: 1.003170] [G loss: 1.000536]\n",
      "6078 [D loss: 1.003139] [G loss: 1.000396]\n",
      "6079 [D loss: 1.003004] [G loss: 1.000438]\n",
      "6080 [D loss: 1.003152] [G loss: 1.000546]\n",
      "6081 [D loss: 1.003206] [G loss: 1.000585]\n",
      "6082 [D loss: 1.003116] [G loss: 1.000872]\n",
      "6083 [D loss: 1.003130] [G loss: 1.000649]\n",
      "6084 [D loss: 1.003063] [G loss: 1.000726]\n",
      "6085 [D loss: 1.003263] [G loss: 1.000428]\n",
      "6086 [D loss: 1.003062] [G loss: 1.000287]\n",
      "6087 [D loss: 1.003105] [G loss: 1.000799]\n",
      "6088 [D loss: 1.003104] [G loss: 1.000640]\n",
      "6089 [D loss: 1.003097] [G loss: 1.000633]\n",
      "6090 [D loss: 1.003182] [G loss: 1.000530]\n",
      "6091 [D loss: 1.003125] [G loss: 1.000337]\n",
      "6092 [D loss: 1.003239] [G loss: 1.000441]\n",
      "6093 [D loss: 1.003258] [G loss: 1.000438]\n",
      "6094 [D loss: 1.002862] [G loss: 1.000641]\n",
      "6095 [D loss: 1.003068] [G loss: 1.000558]\n",
      "6096 [D loss: 1.003194] [G loss: 1.000554]\n",
      "6097 [D loss: 1.003185] [G loss: 1.000842]\n",
      "6098 [D loss: 1.003001] [G loss: 1.000448]\n",
      "6099 [D loss: 1.003357] [G loss: 1.000408]\n",
      "6100 [D loss: 1.002910] [G loss: 1.000813]\n",
      "6101 [D loss: 1.003272] [G loss: 1.000676]\n",
      "6102 [D loss: 1.002864] [G loss: 1.000620]\n",
      "6103 [D loss: 1.003253] [G loss: 1.000729]\n",
      "6104 [D loss: 1.003099] [G loss: 1.000371]\n",
      "6105 [D loss: 1.003130] [G loss: 1.000781]\n",
      "6106 [D loss: 1.003234] [G loss: 1.000482]\n",
      "6107 [D loss: 1.003215] [G loss: 1.000432]\n",
      "6108 [D loss: 1.003177] [G loss: 1.000551]\n",
      "6109 [D loss: 1.003202] [G loss: 1.000649]\n",
      "6110 [D loss: 1.003153] [G loss: 1.000538]\n",
      "6111 [D loss: 1.003235] [G loss: 1.000513]\n",
      "6112 [D loss: 1.003228] [G loss: 1.000706]\n",
      "6113 [D loss: 1.003051] [G loss: 1.000689]\n",
      "6114 [D loss: 1.003113] [G loss: 1.000326]\n",
      "6115 [D loss: 1.003174] [G loss: 1.000475]\n",
      "6116 [D loss: 1.003007] [G loss: 1.000609]\n",
      "6117 [D loss: 1.003138] [G loss: 1.000924]\n",
      "6118 [D loss: 1.002970] [G loss: 1.000584]\n",
      "6119 [D loss: 1.003332] [G loss: 1.000328]\n",
      "6120 [D loss: 1.003158] [G loss: 1.000651]\n",
      "6121 [D loss: 1.003129] [G loss: 1.000707]\n",
      "6122 [D loss: 1.003195] [G loss: 1.000306]\n",
      "6123 [D loss: 1.003151] [G loss: 1.000499]\n",
      "6124 [D loss: 1.003020] [G loss: 1.000316]\n",
      "6125 [D loss: 1.003027] [G loss: 1.000835]\n",
      "6126 [D loss: 1.003012] [G loss: 1.000530]\n",
      "6127 [D loss: 1.003294] [G loss: 1.000505]\n",
      "6128 [D loss: 1.003178] [G loss: 1.000375]\n",
      "6129 [D loss: 1.003092] [G loss: 1.000254]\n",
      "6130 [D loss: 1.003045] [G loss: 1.000703]\n",
      "6131 [D loss: 1.003193] [G loss: 1.000447]\n",
      "6132 [D loss: 1.003245] [G loss: 1.000603]\n",
      "6133 [D loss: 1.003180] [G loss: 1.000881]\n",
      "6134 [D loss: 1.003238] [G loss: 1.000672]\n",
      "6135 [D loss: 1.002951] [G loss: 1.000944]\n",
      "6136 [D loss: 1.003094] [G loss: 1.000437]\n",
      "6137 [D loss: 1.003082] [G loss: 1.000506]\n",
      "6138 [D loss: 1.003188] [G loss: 1.000641]\n",
      "6139 [D loss: 1.003147] [G loss: 1.000597]\n",
      "6140 [D loss: 1.003251] [G loss: 1.000427]\n",
      "6141 [D loss: 1.003073] [G loss: 1.000638]\n",
      "6142 [D loss: 1.003153] [G loss: 1.000858]\n",
      "6143 [D loss: 1.002957] [G loss: 1.000555]\n",
      "6144 [D loss: 1.003231] [G loss: 1.000620]\n",
      "6145 [D loss: 1.003375] [G loss: 1.000850]\n",
      "6146 [D loss: 1.003131] [G loss: 1.000565]\n",
      "6147 [D loss: 1.003296] [G loss: 1.000580]\n",
      "6148 [D loss: 1.003245] [G loss: 1.000396]\n",
      "6149 [D loss: 1.003199] [G loss: 1.000740]\n",
      "6150 [D loss: 1.003401] [G loss: 1.000615]\n",
      "6151 [D loss: 1.003148] [G loss: 1.000725]\n",
      "6152 [D loss: 1.003208] [G loss: 1.000630]\n",
      "6153 [D loss: 1.003034] [G loss: 1.000488]\n",
      "6154 [D loss: 1.003215] [G loss: 1.000481]\n",
      "6155 [D loss: 1.002888] [G loss: 1.000432]\n",
      "6156 [D loss: 1.003086] [G loss: 1.000619]\n",
      "6157 [D loss: 1.002956] [G loss: 1.000440]\n",
      "6158 [D loss: 1.003153] [G loss: 1.000530]\n",
      "6159 [D loss: 1.003120] [G loss: 1.000581]\n",
      "6160 [D loss: 1.003096] [G loss: 1.000654]\n",
      "6161 [D loss: 1.003120] [G loss: 1.000509]\n",
      "6162 [D loss: 1.003329] [G loss: 1.000737]\n",
      "6163 [D loss: 1.003142] [G loss: 1.000476]\n",
      "6164 [D loss: 1.003005] [G loss: 1.000504]\n",
      "6165 [D loss: 1.003144] [G loss: 1.000908]\n",
      "6166 [D loss: 1.003091] [G loss: 1.000423]\n",
      "6167 [D loss: 1.003244] [G loss: 1.000581]\n",
      "6168 [D loss: 1.003530] [G loss: 1.000472]\n",
      "6169 [D loss: 1.003321] [G loss: 1.000472]\n",
      "6170 [D loss: 1.002951] [G loss: 1.000259]\n",
      "6171 [D loss: 1.003041] [G loss: 1.000512]\n",
      "6172 [D loss: 1.003072] [G loss: 1.000530]\n",
      "6173 [D loss: 1.003089] [G loss: 1.001031]\n",
      "6174 [D loss: 1.003008] [G loss: 1.000760]\n",
      "6175 [D loss: 1.003042] [G loss: 1.000480]\n",
      "6176 [D loss: 1.003095] [G loss: 1.000794]\n",
      "6177 [D loss: 1.003162] [G loss: 1.000605]\n",
      "6178 [D loss: 1.002977] [G loss: 1.000488]\n",
      "6179 [D loss: 1.003189] [G loss: 1.000865]\n",
      "6180 [D loss: 1.003438] [G loss: 1.000693]\n",
      "6181 [D loss: 1.003240] [G loss: 1.000782]\n",
      "6182 [D loss: 1.003146] [G loss: 1.000774]\n",
      "6183 [D loss: 1.003302] [G loss: 1.000503]\n",
      "6184 [D loss: 1.003058] [G loss: 1.000506]\n",
      "6185 [D loss: 1.003197] [G loss: 1.000783]\n",
      "6186 [D loss: 1.003142] [G loss: 1.000550]\n",
      "6187 [D loss: 1.003321] [G loss: 1.000693]\n",
      "6188 [D loss: 1.002805] [G loss: 1.000694]\n",
      "6189 [D loss: 1.003162] [G loss: 1.000860]\n",
      "6190 [D loss: 1.003107] [G loss: 1.000467]\n",
      "6191 [D loss: 1.003304] [G loss: 1.000665]\n",
      "6192 [D loss: 1.003263] [G loss: 1.000638]\n",
      "6193 [D loss: 1.003247] [G loss: 1.000371]\n",
      "6194 [D loss: 1.003134] [G loss: 1.000410]\n",
      "6195 [D loss: 1.003342] [G loss: 1.000597]\n",
      "6196 [D loss: 1.003096] [G loss: 1.000902]\n",
      "6197 [D loss: 1.003445] [G loss: 1.000469]\n",
      "6198 [D loss: 1.003186] [G loss: 1.000619]\n",
      "6199 [D loss: 1.003091] [G loss: 1.000431]\n",
      "6200 [D loss: 1.003299] [G loss: 1.000631]\n",
      "6201 [D loss: 1.003225] [G loss: 1.000543]\n",
      "6202 [D loss: 1.003231] [G loss: 1.001012]\n",
      "6203 [D loss: 1.003054] [G loss: 1.000496]\n",
      "6204 [D loss: 1.003357] [G loss: 1.000506]\n",
      "6205 [D loss: 1.003192] [G loss: 1.000758]\n",
      "6206 [D loss: 1.003199] [G loss: 1.000465]\n",
      "6207 [D loss: 1.003186] [G loss: 1.000719]\n",
      "6208 [D loss: 1.003302] [G loss: 1.000630]\n",
      "6209 [D loss: 1.003200] [G loss: 1.000622]\n",
      "6210 [D loss: 1.003146] [G loss: 1.000608]\n",
      "6211 [D loss: 1.003082] [G loss: 1.000722]\n",
      "6212 [D loss: 1.003134] [G loss: 1.000377]\n",
      "6213 [D loss: 1.003290] [G loss: 1.000639]\n",
      "6214 [D loss: 1.003225] [G loss: 1.000772]\n",
      "6215 [D loss: 1.002992] [G loss: 1.000668]\n",
      "6216 [D loss: 1.003008] [G loss: 1.000761]\n",
      "6217 [D loss: 1.003097] [G loss: 1.000386]\n",
      "6218 [D loss: 1.003051] [G loss: 1.000768]\n",
      "6219 [D loss: 1.002998] [G loss: 1.000639]\n",
      "6220 [D loss: 1.002962] [G loss: 1.000819]\n",
      "6221 [D loss: 1.003325] [G loss: 1.000521]\n",
      "6222 [D loss: 1.003087] [G loss: 1.000735]\n",
      "6223 [D loss: 1.003008] [G loss: 1.000469]\n",
      "6224 [D loss: 1.003309] [G loss: 1.000851]\n",
      "6225 [D loss: 1.003067] [G loss: 1.000662]\n",
      "6226 [D loss: 1.003160] [G loss: 1.000598]\n",
      "6227 [D loss: 1.003144] [G loss: 1.000765]\n",
      "6228 [D loss: 1.003076] [G loss: 1.000674]\n",
      "6229 [D loss: 1.003280] [G loss: 1.000520]\n",
      "6230 [D loss: 1.003182] [G loss: 1.000570]\n",
      "6231 [D loss: 1.003223] [G loss: 1.000559]\n",
      "6232 [D loss: 1.003097] [G loss: 1.000859]\n",
      "6233 [D loss: 1.002965] [G loss: 1.000699]\n",
      "6234 [D loss: 1.003152] [G loss: 1.000648]\n",
      "6235 [D loss: 1.003012] [G loss: 1.000462]\n",
      "6236 [D loss: 1.003228] [G loss: 1.000785]\n",
      "6237 [D loss: 1.002978] [G loss: 1.000416]\n",
      "6238 [D loss: 1.003403] [G loss: 1.000403]\n",
      "6239 [D loss: 1.003181] [G loss: 1.001055]\n",
      "6240 [D loss: 1.003365] [G loss: 1.000564]\n",
      "6241 [D loss: 1.003108] [G loss: 1.000336]\n",
      "6242 [D loss: 1.003184] [G loss: 1.000542]\n",
      "6243 [D loss: 1.003261] [G loss: 1.000839]\n",
      "6244 [D loss: 1.003228] [G loss: 1.000348]\n",
      "6245 [D loss: 1.003238] [G loss: 1.000536]\n",
      "6246 [D loss: 1.003197] [G loss: 1.000626]\n",
      "6247 [D loss: 1.003062] [G loss: 1.000891]\n",
      "6248 [D loss: 1.002962] [G loss: 1.000709]\n",
      "6249 [D loss: 1.003164] [G loss: 1.000446]\n",
      "6250 [D loss: 1.003188] [G loss: 1.000824]\n",
      "6251 [D loss: 1.003156] [G loss: 1.000565]\n",
      "6252 [D loss: 1.003314] [G loss: 1.000633]\n",
      "6253 [D loss: 1.003162] [G loss: 1.000556]\n",
      "6254 [D loss: 1.003453] [G loss: 1.000658]\n",
      "6255 [D loss: 1.002961] [G loss: 1.000534]\n",
      "6256 [D loss: 1.003077] [G loss: 1.000570]\n",
      "6257 [D loss: 1.003240] [G loss: 1.000516]\n",
      "6258 [D loss: 1.003360] [G loss: 1.000732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6259 [D loss: 1.003155] [G loss: 1.000633]\n",
      "6260 [D loss: 1.003202] [G loss: 1.000782]\n",
      "6261 [D loss: 1.003135] [G loss: 1.000629]\n",
      "6262 [D loss: 1.003067] [G loss: 1.000805]\n",
      "6263 [D loss: 1.003061] [G loss: 1.000617]\n",
      "6264 [D loss: 1.003382] [G loss: 1.000573]\n",
      "6265 [D loss: 1.003054] [G loss: 1.000604]\n",
      "6266 [D loss: 1.003091] [G loss: 1.000824]\n",
      "6267 [D loss: 1.002987] [G loss: 1.000591]\n",
      "6268 [D loss: 1.003105] [G loss: 1.000922]\n",
      "6269 [D loss: 1.003039] [G loss: 1.000754]\n",
      "6270 [D loss: 1.003189] [G loss: 1.000587]\n",
      "6271 [D loss: 1.003067] [G loss: 1.000586]\n",
      "6272 [D loss: 1.003156] [G loss: 1.000579]\n",
      "6273 [D loss: 1.003296] [G loss: 1.000869]\n",
      "6274 [D loss: 1.003138] [G loss: 1.000353]\n",
      "6275 [D loss: 1.002879] [G loss: 1.000610]\n",
      "6276 [D loss: 1.002962] [G loss: 1.000709]\n",
      "6277 [D loss: 1.002974] [G loss: 1.000842]\n",
      "6278 [D loss: 1.003204] [G loss: 1.000765]\n",
      "6279 [D loss: 1.002684] [G loss: 1.000649]\n",
      "6280 [D loss: 1.002879] [G loss: 1.000606]\n",
      "6281 [D loss: 1.003096] [G loss: 1.000855]\n",
      "6282 [D loss: 1.003128] [G loss: 1.000597]\n",
      "6283 [D loss: 1.002971] [G loss: 1.000415]\n",
      "6284 [D loss: 1.003275] [G loss: 1.000924]\n",
      "6285 [D loss: 1.003082] [G loss: 1.000791]\n",
      "6286 [D loss: 1.003082] [G loss: 1.000600]\n",
      "6287 [D loss: 1.003101] [G loss: 1.000623]\n",
      "6288 [D loss: 1.003187] [G loss: 1.000505]\n",
      "6289 [D loss: 1.003181] [G loss: 1.000700]\n",
      "6290 [D loss: 1.003345] [G loss: 1.000972]\n",
      "6291 [D loss: 1.003198] [G loss: 1.000486]\n",
      "6292 [D loss: 1.003112] [G loss: 1.000549]\n",
      "6293 [D loss: 1.002985] [G loss: 1.000672]\n",
      "6294 [D loss: 1.003256] [G loss: 1.000646]\n",
      "6295 [D loss: 1.002894] [G loss: 1.000772]\n",
      "6296 [D loss: 1.003141] [G loss: 1.000872]\n",
      "6297 [D loss: 1.003059] [G loss: 1.000626]\n",
      "6298 [D loss: 1.002878] [G loss: 1.000870]\n",
      "6299 [D loss: 1.003130] [G loss: 1.000830]\n",
      "6300 [D loss: 1.003099] [G loss: 1.000682]\n",
      "6301 [D loss: 1.003223] [G loss: 1.000684]\n",
      "6302 [D loss: 1.003285] [G loss: 1.000643]\n",
      "6303 [D loss: 1.003169] [G loss: 1.000951]\n",
      "6304 [D loss: 1.003213] [G loss: 1.000669]\n",
      "6305 [D loss: 1.003297] [G loss: 1.000753]\n",
      "6306 [D loss: 1.003171] [G loss: 1.000695]\n",
      "6307 [D loss: 1.003238] [G loss: 1.000764]\n",
      "6308 [D loss: 1.003119] [G loss: 1.000575]\n",
      "6309 [D loss: 1.003161] [G loss: 1.000763]\n",
      "6310 [D loss: 1.002934] [G loss: 1.000642]\n",
      "6311 [D loss: 1.002967] [G loss: 1.000881]\n",
      "6312 [D loss: 1.003077] [G loss: 1.000694]\n",
      "6313 [D loss: 1.003119] [G loss: 1.000650]\n",
      "6314 [D loss: 1.003254] [G loss: 1.000825]\n",
      "6315 [D loss: 1.002989] [G loss: 1.000686]\n",
      "6316 [D loss: 1.002876] [G loss: 1.000582]\n",
      "6317 [D loss: 1.003216] [G loss: 1.000742]\n",
      "6318 [D loss: 1.003332] [G loss: 1.000645]\n",
      "6319 [D loss: 1.003191] [G loss: 1.000946]\n",
      "6320 [D loss: 1.003014] [G loss: 1.000635]\n",
      "6321 [D loss: 1.003149] [G loss: 1.000683]\n",
      "6322 [D loss: 1.003266] [G loss: 1.000667]\n",
      "6323 [D loss: 1.003240] [G loss: 1.000790]\n",
      "6324 [D loss: 1.003129] [G loss: 1.000696]\n",
      "6325 [D loss: 1.003195] [G loss: 1.000890]\n",
      "6326 [D loss: 1.003012] [G loss: 1.000208]\n",
      "6327 [D loss: 1.003153] [G loss: 1.000683]\n",
      "6328 [D loss: 1.003084] [G loss: 1.000765]\n",
      "6329 [D loss: 1.003267] [G loss: 1.000647]\n",
      "6330 [D loss: 1.003146] [G loss: 1.000509]\n",
      "6331 [D loss: 1.003065] [G loss: 1.000653]\n",
      "6332 [D loss: 1.003183] [G loss: 1.000779]\n",
      "6333 [D loss: 1.003212] [G loss: 1.000690]\n",
      "6334 [D loss: 1.003176] [G loss: 1.000865]\n",
      "6335 [D loss: 1.003227] [G loss: 1.000462]\n",
      "6336 [D loss: 1.003146] [G loss: 1.000856]\n",
      "6337 [D loss: 1.003099] [G loss: 1.000357]\n",
      "6338 [D loss: 1.003098] [G loss: 1.000869]\n",
      "6339 [D loss: 1.003185] [G loss: 1.000742]\n",
      "6340 [D loss: 1.003148] [G loss: 1.000993]\n",
      "6341 [D loss: 1.003072] [G loss: 1.000544]\n",
      "6342 [D loss: 1.003034] [G loss: 1.000716]\n",
      "6343 [D loss: 1.003205] [G loss: 1.000887]\n",
      "6344 [D loss: 1.003151] [G loss: 1.000845]\n",
      "6345 [D loss: 1.003077] [G loss: 1.000708]\n",
      "6346 [D loss: 1.002957] [G loss: 1.000892]\n",
      "6347 [D loss: 1.002859] [G loss: 1.000721]\n",
      "6348 [D loss: 1.003125] [G loss: 1.000888]\n",
      "6349 [D loss: 1.003197] [G loss: 1.000747]\n",
      "6350 [D loss: 1.003284] [G loss: 1.000633]\n",
      "6351 [D loss: 1.003179] [G loss: 1.000586]\n",
      "6352 [D loss: 1.003107] [G loss: 1.000924]\n",
      "6353 [D loss: 1.003438] [G loss: 1.000627]\n",
      "6354 [D loss: 1.003136] [G loss: 1.000795]\n",
      "6355 [D loss: 1.003069] [G loss: 1.000703]\n",
      "6356 [D loss: 1.003382] [G loss: 1.000800]\n",
      "6357 [D loss: 1.002994] [G loss: 1.000720]\n",
      "6358 [D loss: 1.003202] [G loss: 1.000819]\n",
      "6359 [D loss: 1.003640] [G loss: 1.000704]\n",
      "6360 [D loss: 1.003072] [G loss: 1.000493]\n",
      "6361 [D loss: 1.003112] [G loss: 1.000921]\n",
      "6362 [D loss: 1.002948] [G loss: 1.000472]\n",
      "6363 [D loss: 1.003303] [G loss: 1.000492]\n",
      "6364 [D loss: 1.003149] [G loss: 1.000724]\n",
      "6365 [D loss: 1.003015] [G loss: 1.000880]\n",
      "6366 [D loss: 1.003240] [G loss: 1.000789]\n",
      "6367 [D loss: 1.002996] [G loss: 1.000861]\n",
      "6368 [D loss: 1.003060] [G loss: 1.000779]\n",
      "6369 [D loss: 1.003286] [G loss: 1.000728]\n",
      "6370 [D loss: 1.003231] [G loss: 1.000870]\n",
      "6371 [D loss: 1.003355] [G loss: 1.000647]\n",
      "6372 [D loss: 1.003014] [G loss: 1.000554]\n",
      "6373 [D loss: 1.003427] [G loss: 1.000819]\n",
      "6374 [D loss: 1.003003] [G loss: 1.000990]\n",
      "6375 [D loss: 1.003239] [G loss: 1.000825]\n",
      "6376 [D loss: 1.003057] [G loss: 1.000855]\n",
      "6377 [D loss: 1.002908] [G loss: 1.000704]\n",
      "6378 [D loss: 1.003263] [G loss: 1.001024]\n",
      "6379 [D loss: 1.003211] [G loss: 1.000809]\n",
      "6380 [D loss: 1.002916] [G loss: 1.000857]\n",
      "6381 [D loss: 1.003242] [G loss: 1.000625]\n",
      "6382 [D loss: 1.003159] [G loss: 1.000704]\n",
      "6383 [D loss: 1.002942] [G loss: 1.000465]\n",
      "6384 [D loss: 1.003108] [G loss: 1.000776]\n",
      "6385 [D loss: 1.003320] [G loss: 1.000757]\n",
      "6386 [D loss: 1.003064] [G loss: 1.000580]\n",
      "6387 [D loss: 1.003123] [G loss: 1.000469]\n",
      "6388 [D loss: 1.003226] [G loss: 1.000721]\n",
      "6389 [D loss: 1.003316] [G loss: 1.000734]\n",
      "6390 [D loss: 1.003260] [G loss: 1.000834]\n",
      "6391 [D loss: 1.003305] [G loss: 1.000723]\n",
      "6392 [D loss: 1.002843] [G loss: 1.000755]\n",
      "6393 [D loss: 1.003191] [G loss: 1.000747]\n",
      "6394 [D loss: 1.003051] [G loss: 1.000414]\n",
      "6395 [D loss: 1.003147] [G loss: 1.000682]\n",
      "6396 [D loss: 1.003287] [G loss: 1.000777]\n",
      "6397 [D loss: 1.003189] [G loss: 1.000452]\n",
      "6398 [D loss: 1.003026] [G loss: 1.000620]\n",
      "6399 [D loss: 1.003376] [G loss: 1.001003]\n",
      "6400 [D loss: 1.003219] [G loss: 1.000887]\n",
      "6401 [D loss: 1.003227] [G loss: 1.000418]\n",
      "6402 [D loss: 1.002952] [G loss: 1.000781]\n",
      "6403 [D loss: 1.003082] [G loss: 1.000826]\n",
      "6404 [D loss: 1.002949] [G loss: 1.000473]\n",
      "6405 [D loss: 1.003145] [G loss: 1.000832]\n",
      "6406 [D loss: 1.003363] [G loss: 1.000635]\n",
      "6407 [D loss: 1.003319] [G loss: 1.000826]\n",
      "6408 [D loss: 1.003061] [G loss: 1.000601]\n",
      "6409 [D loss: 1.003148] [G loss: 1.000800]\n",
      "6410 [D loss: 1.003021] [G loss: 1.000787]\n",
      "6411 [D loss: 1.003258] [G loss: 1.000786]\n",
      "6412 [D loss: 1.003409] [G loss: 1.000670]\n",
      "6413 [D loss: 1.003229] [G loss: 1.000508]\n",
      "6414 [D loss: 1.003283] [G loss: 1.000827]\n",
      "6415 [D loss: 1.003012] [G loss: 1.000867]\n",
      "6416 [D loss: 1.003252] [G loss: 1.000730]\n",
      "6417 [D loss: 1.003186] [G loss: 1.000651]\n",
      "6418 [D loss: 1.003213] [G loss: 1.000705]\n",
      "6419 [D loss: 1.003213] [G loss: 1.000721]\n",
      "6420 [D loss: 1.003258] [G loss: 1.000716]\n",
      "6421 [D loss: 1.002995] [G loss: 1.000570]\n",
      "6422 [D loss: 1.003476] [G loss: 1.000859]\n",
      "6423 [D loss: 1.003097] [G loss: 1.001105]\n",
      "6424 [D loss: 1.003152] [G loss: 1.000770]\n",
      "6425 [D loss: 1.003258] [G loss: 1.000753]\n",
      "6426 [D loss: 1.002897] [G loss: 1.000771]\n",
      "6427 [D loss: 1.003267] [G loss: 1.000596]\n",
      "6428 [D loss: 1.003176] [G loss: 1.000607]\n",
      "6429 [D loss: 1.003219] [G loss: 1.000702]\n",
      "6430 [D loss: 1.003245] [G loss: 1.000945]\n",
      "6431 [D loss: 1.003234] [G loss: 1.000709]\n",
      "6432 [D loss: 1.003261] [G loss: 1.000843]\n",
      "6433 [D loss: 1.003059] [G loss: 1.000791]\n",
      "6434 [D loss: 1.003169] [G loss: 1.000563]\n",
      "6435 [D loss: 1.003161] [G loss: 1.000645]\n",
      "6436 [D loss: 1.003069] [G loss: 1.000793]\n",
      "6437 [D loss: 1.003098] [G loss: 1.000684]\n",
      "6438 [D loss: 1.003035] [G loss: 1.000984]\n",
      "6439 [D loss: 1.003223] [G loss: 1.000796]\n",
      "6440 [D loss: 1.003156] [G loss: 1.000614]\n",
      "6441 [D loss: 1.003256] [G loss: 1.000781]\n",
      "6442 [D loss: 1.003010] [G loss: 1.000601]\n",
      "6443 [D loss: 1.003113] [G loss: 1.000701]\n",
      "6444 [D loss: 1.002907] [G loss: 1.000739]\n",
      "6445 [D loss: 1.003053] [G loss: 1.000691]\n",
      "6446 [D loss: 1.002892] [G loss: 1.000904]\n",
      "6447 [D loss: 1.003103] [G loss: 1.000530]\n",
      "6448 [D loss: 1.003244] [G loss: 1.000815]\n",
      "6449 [D loss: 1.003148] [G loss: 1.000452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6450 [D loss: 1.003056] [G loss: 1.000744]\n",
      "6451 [D loss: 1.003098] [G loss: 1.000818]\n",
      "6452 [D loss: 1.002806] [G loss: 1.000469]\n",
      "6453 [D loss: 1.003115] [G loss: 1.000763]\n",
      "6454 [D loss: 1.003131] [G loss: 1.000915]\n",
      "6455 [D loss: 1.003484] [G loss: 1.000653]\n",
      "6456 [D loss: 1.003288] [G loss: 1.000882]\n",
      "6457 [D loss: 1.003469] [G loss: 1.000757]\n",
      "6458 [D loss: 1.002989] [G loss: 1.000831]\n",
      "6459 [D loss: 1.003333] [G loss: 1.000716]\n",
      "6460 [D loss: 1.003227] [G loss: 1.000857]\n",
      "6461 [D loss: 1.003241] [G loss: 1.000690]\n",
      "6462 [D loss: 1.003268] [G loss: 1.000641]\n",
      "6463 [D loss: 1.003338] [G loss: 1.000792]\n",
      "6464 [D loss: 1.003248] [G loss: 1.000612]\n",
      "6465 [D loss: 1.002762] [G loss: 1.000874]\n",
      "6466 [D loss: 1.002880] [G loss: 1.000904]\n",
      "6467 [D loss: 1.002989] [G loss: 1.000846]\n",
      "6468 [D loss: 1.003101] [G loss: 1.000737]\n",
      "6469 [D loss: 1.003172] [G loss: 1.000820]\n",
      "6470 [D loss: 1.003189] [G loss: 1.000836]\n",
      "6471 [D loss: 1.003435] [G loss: 1.000467]\n",
      "6472 [D loss: 1.003007] [G loss: 1.000929]\n",
      "6473 [D loss: 1.003229] [G loss: 1.000843]\n",
      "6474 [D loss: 1.003176] [G loss: 1.000656]\n",
      "6475 [D loss: 1.003242] [G loss: 1.000854]\n",
      "6476 [D loss: 1.002940] [G loss: 1.000786]\n",
      "6477 [D loss: 1.003157] [G loss: 1.000799]\n",
      "6478 [D loss: 1.002857] [G loss: 1.000809]\n",
      "6479 [D loss: 1.002999] [G loss: 1.000569]\n",
      "6480 [D loss: 1.003064] [G loss: 1.000537]\n",
      "6481 [D loss: 1.003314] [G loss: 1.001054]\n",
      "6482 [D loss: 1.003171] [G loss: 1.000977]\n",
      "6483 [D loss: 1.002944] [G loss: 1.000783]\n",
      "6484 [D loss: 1.003216] [G loss: 1.000494]\n",
      "6485 [D loss: 1.003142] [G loss: 1.000965]\n",
      "6486 [D loss: 1.003016] [G loss: 1.000516]\n",
      "6487 [D loss: 1.003416] [G loss: 1.000815]\n",
      "6488 [D loss: 1.003154] [G loss: 1.000937]\n",
      "6489 [D loss: 1.003351] [G loss: 1.000882]\n",
      "6490 [D loss: 1.003178] [G loss: 1.000657]\n",
      "6491 [D loss: 1.003196] [G loss: 1.000769]\n",
      "6492 [D loss: 1.003175] [G loss: 1.000923]\n",
      "6493 [D loss: 1.003207] [G loss: 1.000673]\n",
      "6494 [D loss: 1.003033] [G loss: 1.000762]\n",
      "6495 [D loss: 1.003204] [G loss: 1.000717]\n",
      "6496 [D loss: 1.003131] [G loss: 1.000896]\n",
      "6497 [D loss: 1.003128] [G loss: 1.001067]\n",
      "6498 [D loss: 1.003262] [G loss: 1.000951]\n",
      "6499 [D loss: 1.003082] [G loss: 1.001026]\n",
      "6500 [D loss: 1.003030] [G loss: 1.000660]\n",
      "6501 [D loss: 1.003146] [G loss: 1.001000]\n",
      "6502 [D loss: 1.003170] [G loss: 1.000885]\n",
      "6503 [D loss: 1.003144] [G loss: 1.000788]\n",
      "6504 [D loss: 1.003033] [G loss: 1.000925]\n",
      "6505 [D loss: 1.003108] [G loss: 1.000510]\n",
      "6506 [D loss: 1.003224] [G loss: 1.000844]\n",
      "6507 [D loss: 1.002985] [G loss: 1.000887]\n",
      "6508 [D loss: 1.003079] [G loss: 1.001041]\n",
      "6509 [D loss: 1.003365] [G loss: 1.000987]\n",
      "6510 [D loss: 1.003102] [G loss: 1.000756]\n",
      "6511 [D loss: 1.003176] [G loss: 1.001073]\n",
      "6512 [D loss: 1.002971] [G loss: 1.000864]\n",
      "6513 [D loss: 1.002999] [G loss: 1.000756]\n",
      "6514 [D loss: 1.003238] [G loss: 1.000551]\n",
      "6515 [D loss: 1.003292] [G loss: 1.000848]\n",
      "6516 [D loss: 1.003094] [G loss: 1.000798]\n",
      "6517 [D loss: 1.003066] [G loss: 1.000844]\n",
      "6518 [D loss: 1.003004] [G loss: 1.000691]\n",
      "6519 [D loss: 1.003249] [G loss: 1.000845]\n",
      "6520 [D loss: 1.003128] [G loss: 1.001010]\n",
      "6521 [D loss: 1.003050] [G loss: 1.000715]\n",
      "6522 [D loss: 1.003077] [G loss: 1.000865]\n",
      "6523 [D loss: 1.003268] [G loss: 1.000952]\n",
      "6524 [D loss: 1.003422] [G loss: 1.001081]\n",
      "6525 [D loss: 1.003298] [G loss: 1.000611]\n",
      "6526 [D loss: 1.003234] [G loss: 1.000928]\n",
      "6527 [D loss: 1.002969] [G loss: 1.000919]\n",
      "6528 [D loss: 1.003031] [G loss: 1.000573]\n",
      "6529 [D loss: 1.003322] [G loss: 1.000883]\n",
      "6530 [D loss: 1.003233] [G loss: 1.000715]\n",
      "6531 [D loss: 1.003201] [G loss: 1.000865]\n",
      "6532 [D loss: 1.002989] [G loss: 1.000834]\n",
      "6533 [D loss: 1.003264] [G loss: 1.000963]\n",
      "6534 [D loss: 1.003214] [G loss: 1.000722]\n",
      "6535 [D loss: 1.003133] [G loss: 1.000749]\n",
      "6536 [D loss: 1.003260] [G loss: 1.000767]\n",
      "6537 [D loss: 1.003193] [G loss: 1.000669]\n",
      "6538 [D loss: 1.003162] [G loss: 1.000871]\n",
      "6539 [D loss: 1.003237] [G loss: 1.000763]\n",
      "6540 [D loss: 1.003148] [G loss: 1.001051]\n",
      "6541 [D loss: 1.003306] [G loss: 1.000812]\n",
      "6542 [D loss: 1.003168] [G loss: 1.000977]\n",
      "6543 [D loss: 1.003270] [G loss: 1.001102]\n",
      "6544 [D loss: 1.002960] [G loss: 1.000670]\n",
      "6545 [D loss: 1.003299] [G loss: 1.000752]\n",
      "6546 [D loss: 1.003157] [G loss: 1.000751]\n",
      "6547 [D loss: 1.003213] [G loss: 1.000939]\n",
      "6548 [D loss: 1.003385] [G loss: 1.000856]\n",
      "6549 [D loss: 1.003187] [G loss: 1.000892]\n",
      "6550 [D loss: 1.003046] [G loss: 1.000806]\n",
      "6551 [D loss: 1.003037] [G loss: 1.000860]\n",
      "6552 [D loss: 1.003330] [G loss: 1.000759]\n",
      "6553 [D loss: 1.003007] [G loss: 1.000754]\n",
      "6554 [D loss: 1.003067] [G loss: 1.000768]\n",
      "6555 [D loss: 1.003175] [G loss: 1.000708]\n",
      "6556 [D loss: 1.003225] [G loss: 1.000745]\n",
      "6557 [D loss: 1.003300] [G loss: 1.000921]\n",
      "6558 [D loss: 1.003343] [G loss: 1.000621]\n",
      "6559 [D loss: 1.003111] [G loss: 1.000901]\n",
      "6560 [D loss: 1.003071] [G loss: 1.000792]\n",
      "6561 [D loss: 1.003357] [G loss: 1.000803]\n",
      "6562 [D loss: 1.003052] [G loss: 1.000937]\n",
      "6563 [D loss: 1.003161] [G loss: 1.000833]\n",
      "6564 [D loss: 1.003244] [G loss: 1.000610]\n",
      "6565 [D loss: 1.003253] [G loss: 1.000645]\n",
      "6566 [D loss: 1.003083] [G loss: 1.000965]\n",
      "6567 [D loss: 1.002983] [G loss: 1.000730]\n",
      "6568 [D loss: 1.003452] [G loss: 1.001036]\n",
      "6569 [D loss: 1.003212] [G loss: 1.000748]\n",
      "6570 [D loss: 1.003205] [G loss: 1.000918]\n",
      "6571 [D loss: 1.003021] [G loss: 1.000534]\n",
      "6572 [D loss: 1.003254] [G loss: 1.000663]\n",
      "6573 [D loss: 1.003291] [G loss: 1.000998]\n",
      "6574 [D loss: 1.003037] [G loss: 1.000742]\n",
      "6575 [D loss: 1.003157] [G loss: 1.001130]\n",
      "6576 [D loss: 1.002875] [G loss: 1.001139]\n",
      "6577 [D loss: 1.003181] [G loss: 1.000848]\n",
      "6578 [D loss: 1.003222] [G loss: 1.000838]\n",
      "6579 [D loss: 1.003164] [G loss: 1.000790]\n",
      "6580 [D loss: 1.002950] [G loss: 1.000818]\n",
      "6581 [D loss: 1.003154] [G loss: 1.000882]\n",
      "6582 [D loss: 1.003006] [G loss: 1.000667]\n",
      "6583 [D loss: 1.003193] [G loss: 1.000687]\n",
      "6584 [D loss: 1.003258] [G loss: 1.000845]\n",
      "6585 [D loss: 1.003072] [G loss: 1.000606]\n",
      "6586 [D loss: 1.003185] [G loss: 1.000657]\n",
      "6587 [D loss: 1.003321] [G loss: 1.000694]\n",
      "6588 [D loss: 1.003270] [G loss: 1.000815]\n",
      "6589 [D loss: 1.003248] [G loss: 1.000579]\n",
      "6590 [D loss: 1.003280] [G loss: 1.000646]\n",
      "6591 [D loss: 1.002927] [G loss: 1.000617]\n",
      "6592 [D loss: 1.003331] [G loss: 1.001132]\n",
      "6593 [D loss: 1.003258] [G loss: 1.000849]\n",
      "6594 [D loss: 1.003265] [G loss: 1.000865]\n",
      "6595 [D loss: 1.003142] [G loss: 1.000536]\n",
      "6596 [D loss: 1.002990] [G loss: 1.000776]\n",
      "6597 [D loss: 1.003479] [G loss: 1.000732]\n",
      "6598 [D loss: 1.003315] [G loss: 1.000666]\n",
      "6599 [D loss: 1.003195] [G loss: 1.000729]\n",
      "6600 [D loss: 1.003274] [G loss: 1.000861]\n",
      "6601 [D loss: 1.003101] [G loss: 1.000644]\n",
      "6602 [D loss: 1.003497] [G loss: 1.000749]\n",
      "6603 [D loss: 1.003175] [G loss: 1.000647]\n",
      "6604 [D loss: 1.002901] [G loss: 1.000921]\n",
      "6605 [D loss: 1.003213] [G loss: 1.000977]\n",
      "6606 [D loss: 1.003110] [G loss: 1.000780]\n",
      "6607 [D loss: 1.003448] [G loss: 1.000615]\n",
      "6608 [D loss: 1.003065] [G loss: 1.000855]\n",
      "6609 [D loss: 1.003020] [G loss: 1.001101]\n",
      "6610 [D loss: 1.002976] [G loss: 1.000936]\n",
      "6611 [D loss: 1.003205] [G loss: 1.000782]\n",
      "6612 [D loss: 1.003115] [G loss: 1.000804]\n",
      "6613 [D loss: 1.003191] [G loss: 1.000832]\n",
      "6614 [D loss: 1.003146] [G loss: 1.000823]\n",
      "6615 [D loss: 1.003180] [G loss: 1.000756]\n",
      "6616 [D loss: 1.003175] [G loss: 1.000995]\n",
      "6617 [D loss: 1.002999] [G loss: 1.000603]\n",
      "6618 [D loss: 1.003111] [G loss: 1.000571]\n",
      "6619 [D loss: 1.003001] [G loss: 1.001004]\n",
      "6620 [D loss: 1.003151] [G loss: 1.000979]\n",
      "6621 [D loss: 1.003191] [G loss: 1.000990]\n",
      "6622 [D loss: 1.003215] [G loss: 1.001052]\n",
      "6623 [D loss: 1.002913] [G loss: 1.001010]\n",
      "6624 [D loss: 1.003163] [G loss: 1.000765]\n",
      "6625 [D loss: 1.003029] [G loss: 1.000800]\n",
      "6626 [D loss: 1.003332] [G loss: 1.000590]\n",
      "6627 [D loss: 1.003358] [G loss: 1.000751]\n",
      "6628 [D loss: 1.003292] [G loss: 1.000625]\n",
      "6629 [D loss: 1.003120] [G loss: 1.000870]\n",
      "6630 [D loss: 1.003157] [G loss: 1.000809]\n",
      "6631 [D loss: 1.003163] [G loss: 1.000587]\n",
      "6632 [D loss: 1.003172] [G loss: 1.001139]\n",
      "6633 [D loss: 1.002950] [G loss: 1.000927]\n",
      "6634 [D loss: 1.003174] [G loss: 1.000828]\n",
      "6635 [D loss: 1.003086] [G loss: 1.001111]\n",
      "6636 [D loss: 1.003186] [G loss: 1.000916]\n",
      "6637 [D loss: 1.003170] [G loss: 1.000587]\n",
      "6638 [D loss: 1.003318] [G loss: 1.000932]\n",
      "6639 [D loss: 1.002993] [G loss: 1.000960]\n",
      "6640 [D loss: 1.003040] [G loss: 1.000700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6641 [D loss: 1.003288] [G loss: 1.001082]\n",
      "6642 [D loss: 1.003157] [G loss: 1.000852]\n",
      "6643 [D loss: 1.003179] [G loss: 1.000896]\n",
      "6644 [D loss: 1.003248] [G loss: 1.000783]\n",
      "6645 [D loss: 1.003299] [G loss: 1.000874]\n",
      "6646 [D loss: 1.003343] [G loss: 1.001133]\n",
      "6647 [D loss: 1.003146] [G loss: 1.001009]\n",
      "6648 [D loss: 1.003505] [G loss: 1.000982]\n",
      "6649 [D loss: 1.003158] [G loss: 1.000981]\n",
      "6650 [D loss: 1.003298] [G loss: 1.000848]\n",
      "6651 [D loss: 1.003109] [G loss: 1.000926]\n",
      "6652 [D loss: 1.003255] [G loss: 1.001072]\n",
      "6653 [D loss: 1.003154] [G loss: 1.000668]\n",
      "6654 [D loss: 1.003104] [G loss: 1.000902]\n",
      "6655 [D loss: 1.003085] [G loss: 1.000646]\n",
      "6656 [D loss: 1.003297] [G loss: 1.000586]\n",
      "6657 [D loss: 1.003152] [G loss: 1.000878]\n",
      "6658 [D loss: 1.002990] [G loss: 1.000941]\n",
      "6659 [D loss: 1.003155] [G loss: 1.000789]\n",
      "6660 [D loss: 1.003380] [G loss: 1.000850]\n",
      "6661 [D loss: 1.003337] [G loss: 1.000431]\n",
      "6662 [D loss: 1.002975] [G loss: 1.000779]\n",
      "6663 [D loss: 1.002873] [G loss: 1.001000]\n",
      "6664 [D loss: 1.003155] [G loss: 1.001033]\n",
      "6665 [D loss: 1.003080] [G loss: 1.000679]\n",
      "6666 [D loss: 1.003202] [G loss: 1.000817]\n",
      "6667 [D loss: 1.002988] [G loss: 1.000778]\n",
      "6668 [D loss: 1.003355] [G loss: 1.000998]\n",
      "6669 [D loss: 1.003263] [G loss: 1.000782]\n",
      "6670 [D loss: 1.003047] [G loss: 1.000815]\n",
      "6671 [D loss: 1.003251] [G loss: 1.000655]\n",
      "6672 [D loss: 1.003556] [G loss: 1.000821]\n",
      "6673 [D loss: 1.003360] [G loss: 1.000936]\n",
      "6674 [D loss: 1.003179] [G loss: 1.000677]\n",
      "6675 [D loss: 1.003313] [G loss: 1.001034]\n",
      "6676 [D loss: 1.003236] [G loss: 1.000666]\n",
      "6677 [D loss: 1.003199] [G loss: 1.000859]\n",
      "6678 [D loss: 1.003125] [G loss: 1.000833]\n",
      "6679 [D loss: 1.003140] [G loss: 1.000705]\n",
      "6680 [D loss: 1.003211] [G loss: 1.000986]\n",
      "6681 [D loss: 1.003233] [G loss: 1.000864]\n",
      "6682 [D loss: 1.003360] [G loss: 1.001048]\n",
      "6683 [D loss: 1.003223] [G loss: 1.000704]\n",
      "6684 [D loss: 1.003383] [G loss: 1.001121]\n",
      "6685 [D loss: 1.003146] [G loss: 1.001093]\n",
      "6686 [D loss: 1.003019] [G loss: 1.000828]\n",
      "6687 [D loss: 1.003312] [G loss: 1.000910]\n",
      "6688 [D loss: 1.003193] [G loss: 1.000599]\n",
      "6689 [D loss: 1.003073] [G loss: 1.000895]\n",
      "6690 [D loss: 1.003252] [G loss: 1.000545]\n",
      "6691 [D loss: 1.002976] [G loss: 1.000905]\n",
      "6692 [D loss: 1.003357] [G loss: 1.000937]\n",
      "6693 [D loss: 1.003115] [G loss: 1.000931]\n",
      "6694 [D loss: 1.003045] [G loss: 1.000976]\n",
      "6695 [D loss: 1.003040] [G loss: 1.000524]\n",
      "6696 [D loss: 1.003073] [G loss: 1.000702]\n",
      "6697 [D loss: 1.003214] [G loss: 1.000654]\n",
      "6698 [D loss: 1.003100] [G loss: 1.000928]\n",
      "6699 [D loss: 1.003274] [G loss: 1.000934]\n",
      "6700 [D loss: 1.002920] [G loss: 1.000778]\n",
      "6701 [D loss: 1.003223] [G loss: 1.001067]\n",
      "6702 [D loss: 1.003007] [G loss: 1.000930]\n",
      "6703 [D loss: 1.003360] [G loss: 1.000619]\n",
      "6704 [D loss: 1.003363] [G loss: 1.000883]\n",
      "6705 [D loss: 1.002999] [G loss: 1.000691]\n",
      "6706 [D loss: 1.003312] [G loss: 1.000781]\n",
      "6707 [D loss: 1.003146] [G loss: 1.000841]\n",
      "6708 [D loss: 1.003347] [G loss: 1.001109]\n",
      "6709 [D loss: 1.003090] [G loss: 1.000923]\n",
      "6710 [D loss: 1.003172] [G loss: 1.001225]\n",
      "6711 [D loss: 1.003343] [G loss: 1.001067]\n",
      "6712 [D loss: 1.003115] [G loss: 1.001118]\n",
      "6713 [D loss: 1.003066] [G loss: 1.000719]\n",
      "6714 [D loss: 1.003185] [G loss: 1.000955]\n",
      "6715 [D loss: 1.003276] [G loss: 1.000988]\n",
      "6716 [D loss: 1.003076] [G loss: 1.000575]\n",
      "6717 [D loss: 1.003215] [G loss: 1.000734]\n",
      "6718 [D loss: 1.003304] [G loss: 1.000896]\n",
      "6719 [D loss: 1.003362] [G loss: 1.000971]\n",
      "6720 [D loss: 1.003078] [G loss: 1.001198]\n",
      "6721 [D loss: 1.003064] [G loss: 1.000684]\n",
      "6722 [D loss: 1.003393] [G loss: 1.000670]\n",
      "6723 [D loss: 1.002807] [G loss: 1.001136]\n",
      "6724 [D loss: 1.002827] [G loss: 1.000711]\n",
      "6725 [D loss: 1.003085] [G loss: 1.000871]\n",
      "6726 [D loss: 1.003190] [G loss: 1.000827]\n",
      "6727 [D loss: 1.003166] [G loss: 1.000749]\n",
      "6728 [D loss: 1.002829] [G loss: 1.000833]\n",
      "6729 [D loss: 1.003133] [G loss: 1.000971]\n",
      "6730 [D loss: 1.003105] [G loss: 1.000788]\n",
      "6731 [D loss: 1.003220] [G loss: 1.000678]\n",
      "6732 [D loss: 1.003190] [G loss: 1.001004]\n",
      "6733 [D loss: 1.003264] [G loss: 1.000889]\n",
      "6734 [D loss: 1.003158] [G loss: 1.001104]\n",
      "6735 [D loss: 1.003112] [G loss: 1.001109]\n",
      "6736 [D loss: 1.003076] [G loss: 1.001050]\n",
      "6737 [D loss: 1.003314] [G loss: 1.000713]\n",
      "6738 [D loss: 1.003333] [G loss: 1.000725]\n",
      "6739 [D loss: 1.002787] [G loss: 1.000936]\n",
      "6740 [D loss: 1.003244] [G loss: 1.000823]\n",
      "6741 [D loss: 1.003423] [G loss: 1.001081]\n",
      "6742 [D loss: 1.003145] [G loss: 1.000727]\n",
      "6743 [D loss: 1.003146] [G loss: 1.000960]\n",
      "6744 [D loss: 1.003080] [G loss: 1.001002]\n",
      "6745 [D loss: 1.003128] [G loss: 1.001264]\n",
      "6746 [D loss: 1.003464] [G loss: 1.001054]\n",
      "6747 [D loss: 1.002965] [G loss: 1.000806]\n",
      "6748 [D loss: 1.003090] [G loss: 1.000686]\n",
      "6749 [D loss: 1.003074] [G loss: 1.000886]\n",
      "6750 [D loss: 1.003095] [G loss: 1.000862]\n",
      "6751 [D loss: 1.003110] [G loss: 1.000687]\n",
      "6752 [D loss: 1.002914] [G loss: 1.000840]\n",
      "6753 [D loss: 1.003178] [G loss: 1.001019]\n",
      "6754 [D loss: 1.003242] [G loss: 1.000851]\n",
      "6755 [D loss: 1.003112] [G loss: 1.000764]\n",
      "6756 [D loss: 1.002983] [G loss: 1.000768]\n",
      "6757 [D loss: 1.003035] [G loss: 1.001052]\n",
      "6758 [D loss: 1.003030] [G loss: 1.000783]\n",
      "6759 [D loss: 1.003116] [G loss: 1.000997]\n",
      "6760 [D loss: 1.003322] [G loss: 1.000572]\n",
      "6761 [D loss: 1.003167] [G loss: 1.000682]\n",
      "6762 [D loss: 1.003044] [G loss: 1.000849]\n",
      "6763 [D loss: 1.003230] [G loss: 1.000952]\n",
      "6764 [D loss: 1.003262] [G loss: 1.000954]\n",
      "6765 [D loss: 1.003178] [G loss: 1.000969]\n",
      "6766 [D loss: 1.003255] [G loss: 1.000834]\n",
      "6767 [D loss: 1.003267] [G loss: 1.000906]\n",
      "6768 [D loss: 1.003040] [G loss: 1.000761]\n",
      "6769 [D loss: 1.003126] [G loss: 1.001204]\n",
      "6770 [D loss: 1.003027] [G loss: 1.001036]\n",
      "6771 [D loss: 1.003147] [G loss: 1.000731]\n",
      "6772 [D loss: 1.003237] [G loss: 1.000860]\n",
      "6773 [D loss: 1.003039] [G loss: 1.000982]\n",
      "6774 [D loss: 1.002826] [G loss: 1.001090]\n",
      "6775 [D loss: 1.003378] [G loss: 1.000772]\n",
      "6776 [D loss: 1.002984] [G loss: 1.000709]\n",
      "6777 [D loss: 1.003083] [G loss: 1.000513]\n",
      "6778 [D loss: 1.002963] [G loss: 1.000733]\n",
      "6779 [D loss: 1.002917] [G loss: 1.000809]\n",
      "6780 [D loss: 1.003184] [G loss: 1.000883]\n",
      "6781 [D loss: 1.003207] [G loss: 1.001021]\n",
      "6782 [D loss: 1.003248] [G loss: 1.000979]\n",
      "6783 [D loss: 1.003070] [G loss: 1.000781]\n",
      "6784 [D loss: 1.003014] [G loss: 1.000758]\n",
      "6785 [D loss: 1.003157] [G loss: 1.000753]\n",
      "6786 [D loss: 1.003011] [G loss: 1.000868]\n",
      "6787 [D loss: 1.003457] [G loss: 1.000761]\n",
      "6788 [D loss: 1.003352] [G loss: 1.001005]\n",
      "6789 [D loss: 1.003307] [G loss: 1.000936]\n",
      "6790 [D loss: 1.003139] [G loss: 1.000909]\n",
      "6791 [D loss: 1.002962] [G loss: 1.001024]\n",
      "6792 [D loss: 1.003284] [G loss: 1.000916]\n",
      "6793 [D loss: 1.003226] [G loss: 1.001002]\n",
      "6794 [D loss: 1.002961] [G loss: 1.000987]\n",
      "6795 [D loss: 1.003388] [G loss: 1.000620]\n",
      "6796 [D loss: 1.003207] [G loss: 1.001004]\n",
      "6797 [D loss: 1.002902] [G loss: 1.001035]\n",
      "6798 [D loss: 1.003038] [G loss: 1.000699]\n",
      "6799 [D loss: 1.003142] [G loss: 1.001099]\n",
      "6800 [D loss: 1.003008] [G loss: 1.000891]\n",
      "6801 [D loss: 1.003129] [G loss: 1.001052]\n",
      "6802 [D loss: 1.003004] [G loss: 1.000965]\n",
      "6803 [D loss: 1.003134] [G loss: 1.000946]\n",
      "6804 [D loss: 1.003083] [G loss: 1.001035]\n",
      "6805 [D loss: 1.003445] [G loss: 1.001040]\n",
      "6806 [D loss: 1.003058] [G loss: 1.000937]\n",
      "6807 [D loss: 1.002990] [G loss: 1.001059]\n",
      "6808 [D loss: 1.003340] [G loss: 1.000766]\n",
      "6809 [D loss: 1.003181] [G loss: 1.000742]\n",
      "6810 [D loss: 1.003046] [G loss: 1.000853]\n",
      "6811 [D loss: 1.003106] [G loss: 1.000842]\n",
      "6812 [D loss: 1.003110] [G loss: 1.000909]\n",
      "6813 [D loss: 1.003082] [G loss: 1.001028]\n",
      "6814 [D loss: 1.003392] [G loss: 1.001018]\n",
      "6815 [D loss: 1.002914] [G loss: 1.001009]\n",
      "6816 [D loss: 1.003128] [G loss: 1.001276]\n",
      "6817 [D loss: 1.002975] [G loss: 1.000986]\n",
      "6818 [D loss: 1.003102] [G loss: 1.000944]\n",
      "6819 [D loss: 1.003109] [G loss: 1.000949]\n",
      "6820 [D loss: 1.003576] [G loss: 1.000677]\n",
      "6821 [D loss: 1.002884] [G loss: 1.000695]\n",
      "6822 [D loss: 1.003226] [G loss: 1.000759]\n",
      "6823 [D loss: 1.003047] [G loss: 1.000777]\n",
      "6824 [D loss: 1.003051] [G loss: 1.000921]\n",
      "6825 [D loss: 1.003060] [G loss: 1.000873]\n",
      "6826 [D loss: 1.003116] [G loss: 1.000931]\n",
      "6827 [D loss: 1.003076] [G loss: 1.001008]\n",
      "6828 [D loss: 1.003182] [G loss: 1.001062]\n",
      "6829 [D loss: 1.003181] [G loss: 1.001097]\n",
      "6830 [D loss: 1.003444] [G loss: 1.001006]\n",
      "6831 [D loss: 1.003091] [G loss: 1.000948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6832 [D loss: 1.003110] [G loss: 1.000860]\n",
      "6833 [D loss: 1.003299] [G loss: 1.001167]\n",
      "6834 [D loss: 1.003138] [G loss: 1.001104]\n",
      "6835 [D loss: 1.002959] [G loss: 1.000951]\n",
      "6836 [D loss: 1.002920] [G loss: 1.000895]\n",
      "6837 [D loss: 1.003247] [G loss: 1.000850]\n",
      "6838 [D loss: 1.003011] [G loss: 1.000904]\n",
      "6839 [D loss: 1.003123] [G loss: 1.000770]\n",
      "6840 [D loss: 1.003061] [G loss: 1.000729]\n",
      "6841 [D loss: 1.003223] [G loss: 1.000639]\n",
      "6842 [D loss: 1.003115] [G loss: 1.000434]\n",
      "6843 [D loss: 1.003298] [G loss: 1.000689]\n",
      "6844 [D loss: 1.003378] [G loss: 1.001057]\n",
      "6845 [D loss: 1.003416] [G loss: 1.001190]\n",
      "6846 [D loss: 1.003208] [G loss: 1.001057]\n",
      "6847 [D loss: 1.003284] [G loss: 1.000814]\n",
      "6848 [D loss: 1.003319] [G loss: 1.000752]\n",
      "6849 [D loss: 1.003284] [G loss: 1.000874]\n",
      "6850 [D loss: 1.003202] [G loss: 1.000729]\n",
      "6851 [D loss: 1.003196] [G loss: 1.000981]\n",
      "6852 [D loss: 1.003042] [G loss: 1.000847]\n",
      "6853 [D loss: 1.003253] [G loss: 1.001106]\n",
      "6854 [D loss: 1.003208] [G loss: 1.000848]\n",
      "6855 [D loss: 1.003096] [G loss: 1.001072]\n",
      "6856 [D loss: 1.003419] [G loss: 1.000515]\n",
      "6857 [D loss: 1.003248] [G loss: 1.000761]\n",
      "6858 [D loss: 1.003371] [G loss: 1.001163]\n",
      "6859 [D loss: 1.003259] [G loss: 1.000847]\n",
      "6860 [D loss: 1.003349] [G loss: 1.000832]\n",
      "6861 [D loss: 1.002983] [G loss: 1.000897]\n",
      "6862 [D loss: 1.003010] [G loss: 1.000933]\n",
      "6863 [D loss: 1.003258] [G loss: 1.000716]\n",
      "6864 [D loss: 1.003004] [G loss: 1.000808]\n",
      "6865 [D loss: 1.003185] [G loss: 1.000805]\n",
      "6866 [D loss: 1.003270] [G loss: 1.000899]\n",
      "6867 [D loss: 1.003146] [G loss: 1.000930]\n",
      "6868 [D loss: 1.003166] [G loss: 1.000857]\n",
      "6869 [D loss: 1.003033] [G loss: 1.000556]\n",
      "6870 [D loss: 1.003190] [G loss: 1.000809]\n",
      "6871 [D loss: 1.003121] [G loss: 1.000653]\n",
      "6872 [D loss: 1.003104] [G loss: 1.000882]\n",
      "6873 [D loss: 1.003164] [G loss: 1.000824]\n",
      "6874 [D loss: 1.003108] [G loss: 1.000944]\n",
      "6875 [D loss: 1.003196] [G loss: 1.001048]\n",
      "6876 [D loss: 1.003021] [G loss: 1.000802]\n",
      "6877 [D loss: 1.003100] [G loss: 1.000757]\n",
      "6878 [D loss: 1.002951] [G loss: 1.000780]\n",
      "6879 [D loss: 1.003257] [G loss: 1.000870]\n",
      "6880 [D loss: 1.003194] [G loss: 1.000766]\n",
      "6881 [D loss: 1.003148] [G loss: 1.000703]\n",
      "6882 [D loss: 1.003055] [G loss: 1.000766]\n",
      "6883 [D loss: 1.003034] [G loss: 1.001206]\n",
      "6884 [D loss: 1.003021] [G loss: 1.000985]\n",
      "6885 [D loss: 1.003116] [G loss: 1.000787]\n",
      "6886 [D loss: 1.003138] [G loss: 1.001092]\n",
      "6887 [D loss: 1.003135] [G loss: 1.000853]\n",
      "6888 [D loss: 1.003281] [G loss: 1.000972]\n",
      "6889 [D loss: 1.003032] [G loss: 1.000598]\n",
      "6890 [D loss: 1.002979] [G loss: 1.000973]\n",
      "6891 [D loss: 1.003145] [G loss: 1.000847]\n",
      "6892 [D loss: 1.003247] [G loss: 1.001089]\n",
      "6893 [D loss: 1.003282] [G loss: 1.000593]\n",
      "6894 [D loss: 1.002962] [G loss: 1.000995]\n",
      "6895 [D loss: 1.003131] [G loss: 1.001002]\n",
      "6896 [D loss: 1.002875] [G loss: 1.000923]\n",
      "6897 [D loss: 1.003014] [G loss: 1.000929]\n",
      "6898 [D loss: 1.003353] [G loss: 1.000726]\n",
      "6899 [D loss: 1.003313] [G loss: 1.001033]\n",
      "6900 [D loss: 1.003032] [G loss: 1.001134]\n",
      "6901 [D loss: 1.003350] [G loss: 1.000939]\n",
      "6902 [D loss: 1.003165] [G loss: 1.000966]\n",
      "6903 [D loss: 1.003191] [G loss: 1.001377]\n",
      "6904 [D loss: 1.003083] [G loss: 1.000782]\n",
      "6905 [D loss: 1.003124] [G loss: 1.001018]\n",
      "6906 [D loss: 1.003265] [G loss: 1.000760]\n",
      "6907 [D loss: 1.003145] [G loss: 1.000885]\n",
      "6908 [D loss: 1.003316] [G loss: 1.000864]\n",
      "6909 [D loss: 1.003068] [G loss: 1.000705]\n",
      "6910 [D loss: 1.003101] [G loss: 1.000857]\n",
      "6911 [D loss: 1.003265] [G loss: 1.000786]\n",
      "6912 [D loss: 1.003376] [G loss: 1.001194]\n",
      "6913 [D loss: 1.003218] [G loss: 1.000551]\n",
      "6914 [D loss: 1.003151] [G loss: 1.000758]\n",
      "6915 [D loss: 1.003375] [G loss: 1.000859]\n",
      "6916 [D loss: 1.003285] [G loss: 1.001080]\n",
      "6917 [D loss: 1.003132] [G loss: 1.000829]\n",
      "6918 [D loss: 1.003286] [G loss: 1.001005]\n",
      "6919 [D loss: 1.003247] [G loss: 1.001132]\n",
      "6920 [D loss: 1.003221] [G loss: 1.000860]\n",
      "6921 [D loss: 1.003351] [G loss: 1.000987]\n",
      "6922 [D loss: 1.002972] [G loss: 1.000796]\n",
      "6923 [D loss: 1.003053] [G loss: 1.001023]\n",
      "6924 [D loss: 1.003151] [G loss: 1.000817]\n",
      "6925 [D loss: 1.003013] [G loss: 1.000914]\n",
      "6926 [D loss: 1.003089] [G loss: 1.000762]\n",
      "6927 [D loss: 1.003027] [G loss: 1.000662]\n",
      "6928 [D loss: 1.003216] [G loss: 1.000748]\n",
      "6929 [D loss: 1.003148] [G loss: 1.001033]\n",
      "6930 [D loss: 1.003146] [G loss: 1.000713]\n",
      "6931 [D loss: 1.003199] [G loss: 1.000706]\n",
      "6932 [D loss: 1.003113] [G loss: 1.000811]\n",
      "6933 [D loss: 1.003251] [G loss: 1.000720]\n",
      "6934 [D loss: 1.003321] [G loss: 1.000710]\n",
      "6935 [D loss: 1.003180] [G loss: 1.000749]\n",
      "6936 [D loss: 1.002948] [G loss: 1.000842]\n",
      "6937 [D loss: 1.003220] [G loss: 1.000914]\n",
      "6938 [D loss: 1.003042] [G loss: 1.000827]\n",
      "6939 [D loss: 1.003171] [G loss: 1.000686]\n",
      "6940 [D loss: 1.003144] [G loss: 1.000752]\n",
      "6941 [D loss: 1.003217] [G loss: 1.000754]\n",
      "6942 [D loss: 1.003275] [G loss: 1.000856]\n",
      "6943 [D loss: 1.003248] [G loss: 1.001030]\n",
      "6944 [D loss: 1.003159] [G loss: 1.000778]\n",
      "6945 [D loss: 1.003037] [G loss: 1.000690]\n",
      "6946 [D loss: 1.003253] [G loss: 1.000887]\n",
      "6947 [D loss: 1.003362] [G loss: 1.000969]\n",
      "6948 [D loss: 1.003348] [G loss: 1.001013]\n",
      "6949 [D loss: 1.003173] [G loss: 1.000852]\n",
      "6950 [D loss: 1.003212] [G loss: 1.000961]\n",
      "6951 [D loss: 1.003140] [G loss: 1.000915]\n",
      "6952 [D loss: 1.003168] [G loss: 1.001062]\n",
      "6953 [D loss: 1.003284] [G loss: 1.000796]\n",
      "6954 [D loss: 1.003070] [G loss: 1.001086]\n",
      "6955 [D loss: 1.002987] [G loss: 1.000731]\n",
      "6956 [D loss: 1.003268] [G loss: 1.001019]\n",
      "6957 [D loss: 1.003145] [G loss: 1.000880]\n",
      "6958 [D loss: 1.003169] [G loss: 1.000737]\n",
      "6959 [D loss: 1.003102] [G loss: 1.000789]\n",
      "6960 [D loss: 1.003092] [G loss: 1.000945]\n",
      "6961 [D loss: 1.003092] [G loss: 1.000668]\n",
      "6962 [D loss: 1.003202] [G loss: 1.000826]\n",
      "6963 [D loss: 1.003231] [G loss: 1.000806]\n",
      "6964 [D loss: 1.003122] [G loss: 1.000933]\n",
      "6965 [D loss: 1.003256] [G loss: 1.000789]\n",
      "6966 [D loss: 1.003326] [G loss: 1.000899]\n",
      "6967 [D loss: 1.003019] [G loss: 1.000865]\n",
      "6968 [D loss: 1.003371] [G loss: 1.000603]\n",
      "6969 [D loss: 1.003292] [G loss: 1.000827]\n",
      "6970 [D loss: 1.002931] [G loss: 1.000852]\n",
      "6971 [D loss: 1.003303] [G loss: 1.001047]\n",
      "6972 [D loss: 1.003110] [G loss: 1.000744]\n",
      "6973 [D loss: 1.003095] [G loss: 1.000919]\n",
      "6974 [D loss: 1.003167] [G loss: 1.000827]\n",
      "6975 [D loss: 1.003158] [G loss: 1.000456]\n",
      "6976 [D loss: 1.003212] [G loss: 1.000883]\n",
      "6977 [D loss: 1.003361] [G loss: 1.000578]\n",
      "6978 [D loss: 1.003354] [G loss: 1.000745]\n",
      "6979 [D loss: 1.003155] [G loss: 1.000906]\n",
      "6980 [D loss: 1.003468] [G loss: 1.001038]\n",
      "6981 [D loss: 1.003194] [G loss: 1.000905]\n",
      "6982 [D loss: 1.003051] [G loss: 1.000867]\n",
      "6983 [D loss: 1.003218] [G loss: 1.000995]\n",
      "6984 [D loss: 1.003191] [G loss: 1.000305]\n",
      "6985 [D loss: 1.003203] [G loss: 1.000927]\n",
      "6986 [D loss: 1.003054] [G loss: 1.000955]\n",
      "6987 [D loss: 1.003144] [G loss: 1.001132]\n",
      "6988 [D loss: 1.002822] [G loss: 1.000875]\n",
      "6989 [D loss: 1.002985] [G loss: 1.000760]\n",
      "6990 [D loss: 1.002925] [G loss: 1.000692]\n",
      "6991 [D loss: 1.003068] [G loss: 1.000753]\n",
      "6992 [D loss: 1.003178] [G loss: 1.000704]\n",
      "6993 [D loss: 1.003124] [G loss: 1.000721]\n",
      "6994 [D loss: 1.003046] [G loss: 1.000806]\n",
      "6995 [D loss: 1.003222] [G loss: 1.001012]\n",
      "6996 [D loss: 1.003024] [G loss: 1.001028]\n",
      "6997 [D loss: 1.002987] [G loss: 1.000874]\n",
      "6998 [D loss: 1.003378] [G loss: 1.001086]\n",
      "6999 [D loss: 1.003009] [G loss: 1.000718]\n",
      "7000 [D loss: 1.003061] [G loss: 1.000954]\n",
      "7001 [D loss: 1.003454] [G loss: 1.000943]\n",
      "7002 [D loss: 1.003271] [G loss: 1.000915]\n",
      "7003 [D loss: 1.003207] [G loss: 1.001003]\n",
      "7004 [D loss: 1.003109] [G loss: 1.000915]\n",
      "7005 [D loss: 1.003313] [G loss: 1.000774]\n",
      "7006 [D loss: 1.003305] [G loss: 1.000579]\n",
      "7007 [D loss: 1.003180] [G loss: 1.000785]\n",
      "7008 [D loss: 1.003291] [G loss: 1.000999]\n",
      "7009 [D loss: 1.003341] [G loss: 1.000912]\n",
      "7010 [D loss: 1.003106] [G loss: 1.000875]\n",
      "7011 [D loss: 1.003392] [G loss: 1.001282]\n",
      "7012 [D loss: 1.003109] [G loss: 1.001157]\n",
      "7013 [D loss: 1.003158] [G loss: 1.000912]\n",
      "7014 [D loss: 1.003315] [G loss: 1.000890]\n",
      "7015 [D loss: 1.002967] [G loss: 1.000680]\n",
      "7016 [D loss: 1.003199] [G loss: 1.000669]\n",
      "7017 [D loss: 1.003093] [G loss: 1.001026]\n",
      "7018 [D loss: 1.003288] [G loss: 1.000766]\n",
      "7019 [D loss: 1.003157] [G loss: 1.000928]\n",
      "7020 [D loss: 1.003330] [G loss: 1.000710]\n",
      "7021 [D loss: 1.003357] [G loss: 1.000955]\n",
      "7022 [D loss: 1.003041] [G loss: 1.000726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7023 [D loss: 1.003185] [G loss: 1.001003]\n",
      "7024 [D loss: 1.003141] [G loss: 1.001252]\n",
      "7025 [D loss: 1.003156] [G loss: 1.000806]\n",
      "7026 [D loss: 1.003115] [G loss: 1.001182]\n",
      "7027 [D loss: 1.003193] [G loss: 1.001008]\n",
      "7028 [D loss: 1.003102] [G loss: 1.001080]\n",
      "7029 [D loss: 1.003107] [G loss: 1.001000]\n",
      "7030 [D loss: 1.003082] [G loss: 1.000517]\n",
      "7031 [D loss: 1.003053] [G loss: 1.001040]\n",
      "7032 [D loss: 1.002908] [G loss: 1.000930]\n",
      "7033 [D loss: 1.003270] [G loss: 1.000643]\n",
      "7034 [D loss: 1.003334] [G loss: 1.000660]\n",
      "7035 [D loss: 1.003285] [G loss: 1.000974]\n",
      "7036 [D loss: 1.003283] [G loss: 1.000879]\n",
      "7037 [D loss: 1.003155] [G loss: 1.000884]\n",
      "7038 [D loss: 1.003080] [G loss: 1.001104]\n",
      "7039 [D loss: 1.003155] [G loss: 1.000823]\n",
      "7040 [D loss: 1.003004] [G loss: 1.000834]\n",
      "7041 [D loss: 1.003269] [G loss: 1.000982]\n",
      "7042 [D loss: 1.002934] [G loss: 1.001099]\n",
      "7043 [D loss: 1.003406] [G loss: 1.000832]\n",
      "7044 [D loss: 1.003132] [G loss: 1.000837]\n",
      "7045 [D loss: 1.003085] [G loss: 1.000798]\n",
      "7046 [D loss: 1.003378] [G loss: 1.000833]\n",
      "7047 [D loss: 1.003242] [G loss: 1.001046]\n",
      "7048 [D loss: 1.003306] [G loss: 1.000642]\n",
      "7049 [D loss: 1.003312] [G loss: 1.000957]\n",
      "7050 [D loss: 1.003144] [G loss: 1.000954]\n",
      "7051 [D loss: 1.003189] [G loss: 1.001100]\n",
      "7052 [D loss: 1.003142] [G loss: 1.000593]\n",
      "7053 [D loss: 1.003268] [G loss: 1.001077]\n",
      "7054 [D loss: 1.002995] [G loss: 1.000771]\n",
      "7055 [D loss: 1.003052] [G loss: 1.000748]\n",
      "7056 [D loss: 1.003240] [G loss: 1.001121]\n",
      "7057 [D loss: 1.003339] [G loss: 1.000730]\n",
      "7058 [D loss: 1.003072] [G loss: 1.000752]\n",
      "7059 [D loss: 1.003205] [G loss: 1.000865]\n",
      "7060 [D loss: 1.003062] [G loss: 1.001106]\n",
      "7061 [D loss: 1.003245] [G loss: 1.001057]\n",
      "7062 [D loss: 1.003122] [G loss: 1.000969]\n",
      "7063 [D loss: 1.003040] [G loss: 1.000996]\n",
      "7064 [D loss: 1.003393] [G loss: 1.000839]\n",
      "7065 [D loss: 1.003281] [G loss: 1.000835]\n",
      "7066 [D loss: 1.003011] [G loss: 1.000756]\n",
      "7067 [D loss: 1.003472] [G loss: 1.000761]\n",
      "7068 [D loss: 1.003032] [G loss: 1.000759]\n",
      "7069 [D loss: 1.003117] [G loss: 1.000925]\n",
      "7070 [D loss: 1.003266] [G loss: 1.000986]\n",
      "7071 [D loss: 1.003429] [G loss: 1.000969]\n",
      "7072 [D loss: 1.002981] [G loss: 1.000951]\n",
      "7073 [D loss: 1.003246] [G loss: 1.000921]\n",
      "7074 [D loss: 1.003066] [G loss: 1.000633]\n",
      "7075 [D loss: 1.003110] [G loss: 1.000451]\n",
      "7076 [D loss: 1.003315] [G loss: 1.000990]\n",
      "7077 [D loss: 1.003083] [G loss: 1.001016]\n",
      "7078 [D loss: 1.002979] [G loss: 1.001254]\n",
      "7079 [D loss: 1.003093] [G loss: 1.001349]\n",
      "7080 [D loss: 1.003442] [G loss: 1.000923]\n",
      "7081 [D loss: 1.003322] [G loss: 1.000550]\n",
      "7082 [D loss: 1.003042] [G loss: 1.000810]\n",
      "7083 [D loss: 1.003274] [G loss: 1.000906]\n",
      "7084 [D loss: 1.003160] [G loss: 1.000869]\n",
      "7085 [D loss: 1.003212] [G loss: 1.000978]\n",
      "7086 [D loss: 1.003016] [G loss: 1.000829]\n",
      "7087 [D loss: 1.003027] [G loss: 1.001013]\n",
      "7088 [D loss: 1.003069] [G loss: 1.000934]\n",
      "7089 [D loss: 1.002968] [G loss: 1.000767]\n",
      "7090 [D loss: 1.003024] [G loss: 1.000901]\n",
      "7091 [D loss: 1.003148] [G loss: 1.000789]\n",
      "7092 [D loss: 1.003023] [G loss: 1.000809]\n",
      "7093 [D loss: 1.002957] [G loss: 1.000860]\n",
      "7094 [D loss: 1.003079] [G loss: 1.000645]\n",
      "7095 [D loss: 1.003288] [G loss: 1.001019]\n",
      "7096 [D loss: 1.003439] [G loss: 1.000979]\n",
      "7097 [D loss: 1.003318] [G loss: 1.000845]\n",
      "7098 [D loss: 1.003100] [G loss: 1.000706]\n",
      "7099 [D loss: 1.003275] [G loss: 1.000907]\n",
      "7100 [D loss: 1.003365] [G loss: 1.000859]\n",
      "7101 [D loss: 1.002965] [G loss: 1.001019]\n",
      "7102 [D loss: 1.003172] [G loss: 1.000656]\n",
      "7103 [D loss: 1.002899] [G loss: 1.000740]\n",
      "7104 [D loss: 1.003100] [G loss: 1.000932]\n",
      "7105 [D loss: 1.003127] [G loss: 1.000832]\n",
      "7106 [D loss: 1.003027] [G loss: 1.000860]\n",
      "7107 [D loss: 1.003237] [G loss: 1.000900]\n",
      "7108 [D loss: 1.002908] [G loss: 1.001001]\n",
      "7109 [D loss: 1.003135] [G loss: 1.000944]\n",
      "7110 [D loss: 1.002997] [G loss: 1.000978]\n",
      "7111 [D loss: 1.003024] [G loss: 1.000871]\n",
      "7112 [D loss: 1.003171] [G loss: 1.000762]\n",
      "7113 [D loss: 1.003192] [G loss: 1.000812]\n",
      "7114 [D loss: 1.003122] [G loss: 1.000734]\n",
      "7115 [D loss: 1.003007] [G loss: 1.001158]\n",
      "7116 [D loss: 1.003162] [G loss: 1.001185]\n",
      "7117 [D loss: 1.003130] [G loss: 1.000956]\n",
      "7118 [D loss: 1.003247] [G loss: 1.000763]\n",
      "7119 [D loss: 1.003086] [G loss: 1.000666]\n",
      "7120 [D loss: 1.003044] [G loss: 1.001123]\n",
      "7121 [D loss: 1.002952] [G loss: 1.001087]\n",
      "7122 [D loss: 1.002950] [G loss: 1.000897]\n",
      "7123 [D loss: 1.003362] [G loss: 1.001120]\n",
      "7124 [D loss: 1.003191] [G loss: 1.000717]\n",
      "7125 [D loss: 1.002917] [G loss: 1.000956]\n",
      "7126 [D loss: 1.003174] [G loss: 1.000892]\n",
      "7127 [D loss: 1.003098] [G loss: 1.000910]\n",
      "7128 [D loss: 1.003078] [G loss: 1.000885]\n",
      "7129 [D loss: 1.003305] [G loss: 1.001034]\n",
      "7130 [D loss: 1.003097] [G loss: 1.001024]\n",
      "7131 [D loss: 1.003125] [G loss: 1.001150]\n",
      "7132 [D loss: 1.002892] [G loss: 1.000719]\n",
      "7133 [D loss: 1.003108] [G loss: 1.001138]\n",
      "7134 [D loss: 1.003095] [G loss: 1.001003]\n",
      "7135 [D loss: 1.003174] [G loss: 1.000903]\n",
      "7136 [D loss: 1.002951] [G loss: 1.000733]\n",
      "7137 [D loss: 1.003036] [G loss: 1.001044]\n",
      "7138 [D loss: 1.003080] [G loss: 1.000799]\n",
      "7139 [D loss: 1.003180] [G loss: 1.000800]\n",
      "7140 [D loss: 1.003085] [G loss: 1.000973]\n",
      "7141 [D loss: 1.002989] [G loss: 1.000832]\n",
      "7142 [D loss: 1.003035] [G loss: 1.001053]\n",
      "7143 [D loss: 1.003140] [G loss: 1.001028]\n",
      "7144 [D loss: 1.002998] [G loss: 1.000804]\n",
      "7145 [D loss: 1.003004] [G loss: 1.000847]\n",
      "7146 [D loss: 1.003160] [G loss: 1.000772]\n",
      "7147 [D loss: 1.002911] [G loss: 1.001100]\n",
      "7148 [D loss: 1.002873] [G loss: 1.001050]\n",
      "7149 [D loss: 1.003225] [G loss: 1.001050]\n",
      "7150 [D loss: 1.003239] [G loss: 1.000962]\n",
      "7151 [D loss: 1.003176] [G loss: 1.000931]\n",
      "7152 [D loss: 1.003146] [G loss: 1.001268]\n",
      "7153 [D loss: 1.002992] [G loss: 1.000690]\n",
      "7154 [D loss: 1.003210] [G loss: 1.000856]\n",
      "7155 [D loss: 1.003213] [G loss: 1.000791]\n",
      "7156 [D loss: 1.003309] [G loss: 1.001223]\n",
      "7157 [D loss: 1.003124] [G loss: 1.001054]\n",
      "7158 [D loss: 1.003432] [G loss: 1.000869]\n",
      "7159 [D loss: 1.003007] [G loss: 1.000759]\n",
      "7160 [D loss: 1.003195] [G loss: 1.000637]\n",
      "7161 [D loss: 1.003404] [G loss: 1.000899]\n",
      "7162 [D loss: 1.003141] [G loss: 1.000647]\n",
      "7163 [D loss: 1.003252] [G loss: 1.000790]\n",
      "7164 [D loss: 1.003246] [G loss: 1.001119]\n",
      "7165 [D loss: 1.003087] [G loss: 1.001159]\n",
      "7166 [D loss: 1.003191] [G loss: 1.000978]\n",
      "7167 [D loss: 1.003054] [G loss: 1.000742]\n",
      "7168 [D loss: 1.003312] [G loss: 1.000735]\n",
      "7169 [D loss: 1.003056] [G loss: 1.000970]\n",
      "7170 [D loss: 1.003366] [G loss: 1.000974]\n",
      "7171 [D loss: 1.003235] [G loss: 1.000916]\n",
      "7172 [D loss: 1.003112] [G loss: 1.000818]\n",
      "7173 [D loss: 1.003294] [G loss: 1.000935]\n",
      "7174 [D loss: 1.003081] [G loss: 1.000873]\n",
      "7175 [D loss: 1.003092] [G loss: 1.001178]\n",
      "7176 [D loss: 1.003210] [G loss: 1.000784]\n",
      "7177 [D loss: 1.003049] [G loss: 1.001020]\n",
      "7178 [D loss: 1.003064] [G loss: 1.000983]\n",
      "7179 [D loss: 1.003193] [G loss: 1.000762]\n",
      "7180 [D loss: 1.003282] [G loss: 1.000746]\n",
      "7181 [D loss: 1.003284] [G loss: 1.000890]\n",
      "7182 [D loss: 1.003165] [G loss: 1.001085]\n",
      "7183 [D loss: 1.003306] [G loss: 1.000753]\n",
      "7184 [D loss: 1.002823] [G loss: 1.000794]\n",
      "7185 [D loss: 1.003247] [G loss: 1.001249]\n",
      "7186 [D loss: 1.002891] [G loss: 1.000869]\n",
      "7187 [D loss: 1.003215] [G loss: 1.000337]\n",
      "7188 [D loss: 1.003192] [G loss: 1.000890]\n",
      "7189 [D loss: 1.003295] [G loss: 1.001058]\n",
      "7190 [D loss: 1.003024] [G loss: 1.000867]\n",
      "7191 [D loss: 1.003265] [G loss: 1.000976]\n",
      "7192 [D loss: 1.003089] [G loss: 1.000761]\n",
      "7193 [D loss: 1.003123] [G loss: 1.000811]\n",
      "7194 [D loss: 1.003253] [G loss: 1.000810]\n",
      "7195 [D loss: 1.002914] [G loss: 1.000941]\n",
      "7196 [D loss: 1.003134] [G loss: 1.000804]\n",
      "7197 [D loss: 1.003306] [G loss: 1.000823]\n",
      "7198 [D loss: 1.003158] [G loss: 1.000849]\n",
      "7199 [D loss: 1.002995] [G loss: 1.000958]\n",
      "7200 [D loss: 1.003031] [G loss: 1.000740]\n",
      "7201 [D loss: 1.003090] [G loss: 1.000984]\n",
      "7202 [D loss: 1.003102] [G loss: 1.001078]\n",
      "7203 [D loss: 1.003228] [G loss: 1.000893]\n",
      "7204 [D loss: 1.003124] [G loss: 1.000870]\n",
      "7205 [D loss: 1.002923] [G loss: 1.000967]\n",
      "7206 [D loss: 1.003289] [G loss: 1.001017]\n",
      "7207 [D loss: 1.003120] [G loss: 1.000908]\n",
      "7208 [D loss: 1.003327] [G loss: 1.000880]\n",
      "7209 [D loss: 1.003396] [G loss: 1.001019]\n",
      "7210 [D loss: 1.003107] [G loss: 1.000970]\n",
      "7211 [D loss: 1.003063] [G loss: 1.000824]\n",
      "7212 [D loss: 1.003271] [G loss: 1.001373]\n",
      "7213 [D loss: 1.003170] [G loss: 1.000965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7214 [D loss: 1.003146] [G loss: 1.001008]\n",
      "7215 [D loss: 1.003122] [G loss: 1.000710]\n",
      "7216 [D loss: 1.003199] [G loss: 1.000554]\n",
      "7217 [D loss: 1.003349] [G loss: 1.000906]\n",
      "7218 [D loss: 1.003072] [G loss: 1.000701]\n",
      "7219 [D loss: 1.003260] [G loss: 1.000667]\n",
      "7220 [D loss: 1.003037] [G loss: 1.001148]\n",
      "7221 [D loss: 1.003119] [G loss: 1.001094]\n",
      "7222 [D loss: 1.003108] [G loss: 1.000885]\n",
      "7223 [D loss: 1.002966] [G loss: 1.001002]\n",
      "7224 [D loss: 1.003180] [G loss: 1.001056]\n",
      "7225 [D loss: 1.003084] [G loss: 1.000773]\n",
      "7226 [D loss: 1.003114] [G loss: 1.000819]\n",
      "7227 [D loss: 1.003415] [G loss: 1.001340]\n",
      "7228 [D loss: 1.003142] [G loss: 1.000802]\n",
      "7229 [D loss: 1.003067] [G loss: 1.001074]\n",
      "7230 [D loss: 1.003209] [G loss: 1.000643]\n",
      "7231 [D loss: 1.003217] [G loss: 1.000752]\n",
      "7232 [D loss: 1.003044] [G loss: 1.000760]\n",
      "7233 [D loss: 1.003054] [G loss: 1.001064]\n",
      "7234 [D loss: 1.002994] [G loss: 1.000770]\n",
      "7235 [D loss: 1.003063] [G loss: 1.000868]\n",
      "7236 [D loss: 1.003214] [G loss: 1.000782]\n",
      "7237 [D loss: 1.003202] [G loss: 1.000899]\n",
      "7238 [D loss: 1.003205] [G loss: 1.000871]\n",
      "7239 [D loss: 1.003193] [G loss: 1.000617]\n",
      "7240 [D loss: 1.003119] [G loss: 1.000737]\n",
      "7241 [D loss: 1.003264] [G loss: 1.000869]\n",
      "7242 [D loss: 1.003110] [G loss: 1.000705]\n",
      "7243 [D loss: 1.003161] [G loss: 1.000616]\n",
      "7244 [D loss: 1.003292] [G loss: 1.001043]\n",
      "7245 [D loss: 1.003275] [G loss: 1.000868]\n",
      "7246 [D loss: 1.003212] [G loss: 1.000967]\n",
      "7247 [D loss: 1.003216] [G loss: 1.000797]\n",
      "7248 [D loss: 1.003226] [G loss: 1.000752]\n",
      "7249 [D loss: 1.003408] [G loss: 1.000577]\n",
      "7250 [D loss: 1.003335] [G loss: 1.000867]\n",
      "7251 [D loss: 1.003339] [G loss: 1.000762]\n",
      "7252 [D loss: 1.003105] [G loss: 1.000753]\n",
      "7253 [D loss: 1.002988] [G loss: 1.001061]\n",
      "7254 [D loss: 1.003053] [G loss: 1.001012]\n",
      "7255 [D loss: 1.003019] [G loss: 1.000971]\n",
      "7256 [D loss: 1.003137] [G loss: 1.000916]\n",
      "7257 [D loss: 1.003359] [G loss: 1.000911]\n",
      "7258 [D loss: 1.003238] [G loss: 1.001005]\n",
      "7259 [D loss: 1.003011] [G loss: 1.000975]\n",
      "7260 [D loss: 1.003098] [G loss: 1.000641]\n",
      "7261 [D loss: 1.003087] [G loss: 1.001073]\n",
      "7262 [D loss: 1.003424] [G loss: 1.000641]\n",
      "7263 [D loss: 1.002879] [G loss: 1.000808]\n",
      "7264 [D loss: 1.002979] [G loss: 1.000794]\n",
      "7265 [D loss: 1.003196] [G loss: 1.000924]\n",
      "7266 [D loss: 1.003107] [G loss: 1.000898]\n",
      "7267 [D loss: 1.003321] [G loss: 1.000770]\n",
      "7268 [D loss: 1.003226] [G loss: 1.000774]\n",
      "7269 [D loss: 1.003110] [G loss: 1.001016]\n",
      "7270 [D loss: 1.003200] [G loss: 1.000801]\n",
      "7271 [D loss: 1.003144] [G loss: 1.001372]\n",
      "7272 [D loss: 1.003197] [G loss: 1.000848]\n",
      "7273 [D loss: 1.003093] [G loss: 1.000648]\n",
      "7274 [D loss: 1.002990] [G loss: 1.000760]\n",
      "7275 [D loss: 1.002892] [G loss: 1.001137]\n",
      "7276 [D loss: 1.003270] [G loss: 1.000981]\n",
      "7277 [D loss: 1.003261] [G loss: 1.001152]\n",
      "7278 [D loss: 1.003366] [G loss: 1.000783]\n",
      "7279 [D loss: 1.003191] [G loss: 1.000754]\n",
      "7280 [D loss: 1.003127] [G loss: 1.000936]\n",
      "7281 [D loss: 1.003142] [G loss: 1.000934]\n",
      "7282 [D loss: 1.003116] [G loss: 1.000796]\n",
      "7283 [D loss: 1.003194] [G loss: 1.000829]\n",
      "7284 [D loss: 1.003221] [G loss: 1.000663]\n",
      "7285 [D loss: 1.003087] [G loss: 1.000653]\n",
      "7286 [D loss: 1.003204] [G loss: 1.000724]\n",
      "7287 [D loss: 1.003215] [G loss: 1.000748]\n",
      "7288 [D loss: 1.003170] [G loss: 1.001178]\n",
      "7289 [D loss: 1.002943] [G loss: 1.000973]\n",
      "7290 [D loss: 1.003125] [G loss: 1.000772]\n",
      "7291 [D loss: 1.003079] [G loss: 1.000826]\n",
      "7292 [D loss: 1.003089] [G loss: 1.000812]\n",
      "7293 [D loss: 1.003330] [G loss: 1.000608]\n",
      "7294 [D loss: 1.003323] [G loss: 1.000598]\n",
      "7295 [D loss: 1.003124] [G loss: 1.001047]\n",
      "7296 [D loss: 1.003005] [G loss: 1.000974]\n",
      "7297 [D loss: 1.003013] [G loss: 1.000628]\n",
      "7298 [D loss: 1.002995] [G loss: 1.000577]\n",
      "7299 [D loss: 1.002986] [G loss: 1.000679]\n",
      "7300 [D loss: 1.003112] [G loss: 1.000783]\n",
      "7301 [D loss: 1.003086] [G loss: 1.000810]\n",
      "7302 [D loss: 1.003419] [G loss: 1.001101]\n",
      "7303 [D loss: 1.003166] [G loss: 1.000941]\n",
      "7304 [D loss: 1.002876] [G loss: 1.001024]\n",
      "7305 [D loss: 1.003225] [G loss: 1.000752]\n",
      "7306 [D loss: 1.003051] [G loss: 1.000938]\n",
      "7307 [D loss: 1.003409] [G loss: 1.000896]\n",
      "7308 [D loss: 1.003095] [G loss: 1.000953]\n",
      "7309 [D loss: 1.003115] [G loss: 1.000668]\n",
      "7310 [D loss: 1.003165] [G loss: 1.000699]\n",
      "7311 [D loss: 1.003336] [G loss: 1.000847]\n",
      "7312 [D loss: 1.003195] [G loss: 1.000968]\n",
      "7313 [D loss: 1.003317] [G loss: 1.000782]\n",
      "7314 [D loss: 1.003180] [G loss: 1.000656]\n",
      "7315 [D loss: 1.003201] [G loss: 1.001000]\n",
      "7316 [D loss: 1.002898] [G loss: 1.000915]\n",
      "7317 [D loss: 1.003231] [G loss: 1.000857]\n",
      "7318 [D loss: 1.003071] [G loss: 1.000884]\n",
      "7319 [D loss: 1.003157] [G loss: 1.000642]\n",
      "7320 [D loss: 1.003303] [G loss: 1.000584]\n",
      "7321 [D loss: 1.003175] [G loss: 1.000888]\n",
      "7322 [D loss: 1.003214] [G loss: 1.000584]\n",
      "7323 [D loss: 1.003076] [G loss: 1.000865]\n",
      "7324 [D loss: 1.002995] [G loss: 1.000520]\n",
      "7325 [D loss: 1.003200] [G loss: 1.000973]\n",
      "7326 [D loss: 1.003031] [G loss: 1.000525]\n",
      "7327 [D loss: 1.003215] [G loss: 1.000666]\n",
      "7328 [D loss: 1.003183] [G loss: 1.000765]\n",
      "7329 [D loss: 1.003141] [G loss: 1.001152]\n",
      "7330 [D loss: 1.003158] [G loss: 1.000688]\n",
      "7331 [D loss: 1.003278] [G loss: 1.000682]\n",
      "7332 [D loss: 1.003052] [G loss: 1.000672]\n",
      "7333 [D loss: 1.003282] [G loss: 1.000644]\n",
      "7334 [D loss: 1.002968] [G loss: 1.000859]\n",
      "7335 [D loss: 1.003139] [G loss: 1.000737]\n",
      "7336 [D loss: 1.003220] [G loss: 1.000934]\n",
      "7337 [D loss: 1.003215] [G loss: 1.000982]\n",
      "7338 [D loss: 1.003009] [G loss: 1.000794]\n",
      "7339 [D loss: 1.003190] [G loss: 1.000951]\n",
      "7340 [D loss: 1.003341] [G loss: 1.000829]\n",
      "7341 [D loss: 1.003071] [G loss: 1.000895]\n",
      "7342 [D loss: 1.003198] [G loss: 1.000727]\n",
      "7343 [D loss: 1.002915] [G loss: 1.000621]\n",
      "7344 [D loss: 1.003264] [G loss: 1.000757]\n",
      "7345 [D loss: 1.003206] [G loss: 1.001072]\n",
      "7346 [D loss: 1.003190] [G loss: 1.000698]\n",
      "7347 [D loss: 1.003135] [G loss: 1.000883]\n",
      "7348 [D loss: 1.003093] [G loss: 1.000742]\n",
      "7349 [D loss: 1.003372] [G loss: 1.000676]\n",
      "7350 [D loss: 1.003106] [G loss: 1.000836]\n",
      "7351 [D loss: 1.003166] [G loss: 1.000941]\n",
      "7352 [D loss: 1.003314] [G loss: 1.000874]\n",
      "7353 [D loss: 1.003288] [G loss: 1.000885]\n",
      "7354 [D loss: 1.003171] [G loss: 1.000889]\n",
      "7355 [D loss: 1.003140] [G loss: 1.000769]\n",
      "7356 [D loss: 1.003122] [G loss: 1.000562]\n",
      "7357 [D loss: 1.003162] [G loss: 1.000643]\n",
      "7358 [D loss: 1.003053] [G loss: 1.000711]\n",
      "7359 [D loss: 1.003251] [G loss: 1.000891]\n",
      "7360 [D loss: 1.002975] [G loss: 1.000879]\n",
      "7361 [D loss: 1.003059] [G loss: 1.000996]\n",
      "7362 [D loss: 1.003170] [G loss: 1.000810]\n",
      "7363 [D loss: 1.003214] [G loss: 1.000885]\n",
      "7364 [D loss: 1.002958] [G loss: 1.000941]\n",
      "7365 [D loss: 1.003225] [G loss: 1.000718]\n",
      "7366 [D loss: 1.003082] [G loss: 1.000752]\n",
      "7367 [D loss: 1.003063] [G loss: 1.001083]\n",
      "7368 [D loss: 1.003305] [G loss: 1.000745]\n",
      "7369 [D loss: 1.003213] [G loss: 1.001039]\n",
      "7370 [D loss: 1.003029] [G loss: 1.000450]\n",
      "7371 [D loss: 1.003314] [G loss: 1.000796]\n",
      "7372 [D loss: 1.003191] [G loss: 1.000548]\n",
      "7373 [D loss: 1.003291] [G loss: 1.000978]\n",
      "7374 [D loss: 1.003103] [G loss: 1.001110]\n",
      "7375 [D loss: 1.003163] [G loss: 1.001010]\n",
      "7376 [D loss: 1.003250] [G loss: 1.000768]\n",
      "7377 [D loss: 1.003279] [G loss: 1.000805]\n",
      "7378 [D loss: 1.002964] [G loss: 1.000639]\n",
      "7379 [D loss: 1.003205] [G loss: 1.000857]\n",
      "7380 [D loss: 1.003388] [G loss: 1.000938]\n",
      "7381 [D loss: 1.003115] [G loss: 1.000629]\n",
      "7382 [D loss: 1.003280] [G loss: 1.000653]\n",
      "7383 [D loss: 1.003001] [G loss: 1.000980]\n",
      "7384 [D loss: 1.003173] [G loss: 1.000942]\n",
      "7385 [D loss: 1.003185] [G loss: 1.000736]\n",
      "7386 [D loss: 1.003167] [G loss: 1.001048]\n",
      "7387 [D loss: 1.003299] [G loss: 1.000863]\n",
      "7388 [D loss: 1.003266] [G loss: 1.000817]\n",
      "7389 [D loss: 1.003132] [G loss: 1.000655]\n",
      "7390 [D loss: 1.003124] [G loss: 1.000612]\n",
      "7391 [D loss: 1.003198] [G loss: 1.000830]\n",
      "7392 [D loss: 1.003195] [G loss: 1.000694]\n",
      "7393 [D loss: 1.003070] [G loss: 1.000904]\n",
      "7394 [D loss: 1.003054] [G loss: 1.000850]\n",
      "7395 [D loss: 1.003315] [G loss: 1.000793]\n",
      "7396 [D loss: 1.002951] [G loss: 1.000803]\n",
      "7397 [D loss: 1.003116] [G loss: 1.001121]\n",
      "7398 [D loss: 1.003234] [G loss: 1.000597]\n",
      "7399 [D loss: 1.002933] [G loss: 1.000894]\n",
      "7400 [D loss: 1.002917] [G loss: 1.000679]\n",
      "7401 [D loss: 1.002977] [G loss: 1.001098]\n",
      "7402 [D loss: 1.003218] [G loss: 1.000851]\n",
      "7403 [D loss: 1.003166] [G loss: 1.000453]\n",
      "7404 [D loss: 1.003257] [G loss: 1.000622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7405 [D loss: 1.003018] [G loss: 1.000786]\n",
      "7406 [D loss: 1.003313] [G loss: 1.000673]\n",
      "7407 [D loss: 1.003079] [G loss: 1.001026]\n",
      "7408 [D loss: 1.002886] [G loss: 1.000792]\n",
      "7409 [D loss: 1.003200] [G loss: 1.000642]\n",
      "7410 [D loss: 1.003262] [G loss: 1.000785]\n",
      "7411 [D loss: 1.003158] [G loss: 1.000847]\n",
      "7412 [D loss: 1.003129] [G loss: 1.000429]\n",
      "7413 [D loss: 1.003274] [G loss: 1.000656]\n",
      "7414 [D loss: 1.002990] [G loss: 1.000769]\n",
      "7415 [D loss: 1.003266] [G loss: 1.000701]\n",
      "7416 [D loss: 1.003058] [G loss: 1.000740]\n",
      "7417 [D loss: 1.003208] [G loss: 1.000737]\n",
      "7418 [D loss: 1.003050] [G loss: 1.000911]\n",
      "7419 [D loss: 1.003318] [G loss: 1.000394]\n",
      "7420 [D loss: 1.003085] [G loss: 1.000879]\n",
      "7421 [D loss: 1.003147] [G loss: 1.000631]\n",
      "7422 [D loss: 1.003173] [G loss: 1.000802]\n",
      "7423 [D loss: 1.003200] [G loss: 1.000603]\n",
      "7424 [D loss: 1.003295] [G loss: 1.000853]\n",
      "7425 [D loss: 1.002946] [G loss: 1.001216]\n",
      "7426 [D loss: 1.003280] [G loss: 1.000979]\n",
      "7427 [D loss: 1.003102] [G loss: 1.000542]\n",
      "7428 [D loss: 1.002947] [G loss: 1.001003]\n",
      "7429 [D loss: 1.003405] [G loss: 1.000722]\n",
      "7430 [D loss: 1.002854] [G loss: 1.000885]\n",
      "7431 [D loss: 1.003081] [G loss: 1.000657]\n",
      "7432 [D loss: 1.003251] [G loss: 1.000882]\n",
      "7433 [D loss: 1.003258] [G loss: 1.000654]\n",
      "7434 [D loss: 1.003295] [G loss: 1.000882]\n",
      "7435 [D loss: 1.003193] [G loss: 1.000728]\n",
      "7436 [D loss: 1.003273] [G loss: 1.000564]\n",
      "7437 [D loss: 1.003217] [G loss: 1.000622]\n",
      "7438 [D loss: 1.003056] [G loss: 1.000695]\n",
      "7439 [D loss: 1.003291] [G loss: 1.000857]\n",
      "7440 [D loss: 1.003185] [G loss: 1.000883]\n",
      "7441 [D loss: 1.003228] [G loss: 1.000744]\n",
      "7442 [D loss: 1.003199] [G loss: 1.000877]\n",
      "7443 [D loss: 1.002975] [G loss: 1.000892]\n",
      "7444 [D loss: 1.003146] [G loss: 1.000760]\n",
      "7445 [D loss: 1.003287] [G loss: 1.000892]\n",
      "7446 [D loss: 1.003273] [G loss: 1.000700]\n",
      "7447 [D loss: 1.003413] [G loss: 1.000892]\n",
      "7448 [D loss: 1.003377] [G loss: 1.001233]\n",
      "7449 [D loss: 1.003003] [G loss: 1.000683]\n",
      "7450 [D loss: 1.003431] [G loss: 1.000640]\n",
      "7451 [D loss: 1.003098] [G loss: 1.000714]\n",
      "7452 [D loss: 1.003062] [G loss: 1.000953]\n",
      "7453 [D loss: 1.003010] [G loss: 1.000938]\n",
      "7454 [D loss: 1.003057] [G loss: 1.001022]\n",
      "7455 [D loss: 1.003278] [G loss: 1.000911]\n",
      "7456 [D loss: 1.003152] [G loss: 1.001010]\n",
      "7457 [D loss: 1.003007] [G loss: 1.000851]\n",
      "7458 [D loss: 1.002997] [G loss: 1.000710]\n",
      "7459 [D loss: 1.003290] [G loss: 1.000582]\n",
      "7460 [D loss: 1.003261] [G loss: 1.000816]\n",
      "7461 [D loss: 1.003220] [G loss: 1.000882]\n",
      "7462 [D loss: 1.003165] [G loss: 1.000740]\n",
      "7463 [D loss: 1.003162] [G loss: 1.000732]\n",
      "7464 [D loss: 1.003037] [G loss: 1.000695]\n",
      "7465 [D loss: 1.003275] [G loss: 1.000787]\n",
      "7466 [D loss: 1.003135] [G loss: 1.001084]\n",
      "7467 [D loss: 1.002930] [G loss: 1.000839]\n",
      "7468 [D loss: 1.003147] [G loss: 1.000956]\n",
      "7469 [D loss: 1.003295] [G loss: 1.001277]\n",
      "7470 [D loss: 1.003196] [G loss: 1.000868]\n",
      "7471 [D loss: 1.003121] [G loss: 1.000816]\n",
      "7472 [D loss: 1.003258] [G loss: 1.000794]\n",
      "7473 [D loss: 1.003206] [G loss: 1.001010]\n",
      "7474 [D loss: 1.003333] [G loss: 1.000984]\n",
      "7475 [D loss: 1.003143] [G loss: 1.000657]\n",
      "7476 [D loss: 1.003039] [G loss: 1.000963]\n",
      "7477 [D loss: 1.002919] [G loss: 1.000579]\n",
      "7478 [D loss: 1.003082] [G loss: 1.000744]\n",
      "7479 [D loss: 1.003156] [G loss: 1.000667]\n",
      "7480 [D loss: 1.003322] [G loss: 1.001013]\n",
      "7481 [D loss: 1.003268] [G loss: 1.000943]\n",
      "7482 [D loss: 1.003162] [G loss: 1.000577]\n",
      "7483 [D loss: 1.003239] [G loss: 1.001006]\n",
      "7484 [D loss: 1.003185] [G loss: 1.000686]\n",
      "7485 [D loss: 1.003088] [G loss: 1.000715]\n",
      "7486 [D loss: 1.003069] [G loss: 1.000817]\n",
      "7487 [D loss: 1.003323] [G loss: 1.001117]\n",
      "7488 [D loss: 1.003276] [G loss: 1.000594]\n",
      "7489 [D loss: 1.002778] [G loss: 1.001044]\n",
      "7490 [D loss: 1.003085] [G loss: 1.000549]\n",
      "7491 [D loss: 1.003433] [G loss: 1.000648]\n",
      "7492 [D loss: 1.003201] [G loss: 1.001033]\n",
      "7493 [D loss: 1.002969] [G loss: 1.000682]\n",
      "7494 [D loss: 1.003348] [G loss: 1.000730]\n",
      "7495 [D loss: 1.003025] [G loss: 1.001049]\n",
      "7496 [D loss: 1.003069] [G loss: 1.000826]\n",
      "7497 [D loss: 1.003238] [G loss: 1.000657]\n",
      "7498 [D loss: 1.003441] [G loss: 1.000929]\n",
      "7499 [D loss: 1.003032] [G loss: 1.000906]\n",
      "7500 [D loss: 1.003135] [G loss: 1.000938]\n",
      "7501 [D loss: 1.003016] [G loss: 1.000767]\n",
      "7502 [D loss: 1.003266] [G loss: 1.000746]\n",
      "7503 [D loss: 1.003253] [G loss: 1.001016]\n",
      "7504 [D loss: 1.003046] [G loss: 1.000796]\n",
      "7505 [D loss: 1.003071] [G loss: 1.000849]\n",
      "7506 [D loss: 1.003054] [G loss: 1.000865]\n",
      "7507 [D loss: 1.003218] [G loss: 1.000641]\n",
      "7508 [D loss: 1.003307] [G loss: 1.000804]\n",
      "7509 [D loss: 1.003218] [G loss: 1.000872]\n",
      "7510 [D loss: 1.003154] [G loss: 1.001195]\n",
      "7511 [D loss: 1.003218] [G loss: 1.000866]\n",
      "7512 [D loss: 1.003172] [G loss: 1.001123]\n",
      "7513 [D loss: 1.003314] [G loss: 1.001104]\n",
      "7514 [D loss: 1.003016] [G loss: 1.000978]\n",
      "7515 [D loss: 1.003232] [G loss: 1.000707]\n",
      "7516 [D loss: 1.003083] [G loss: 1.000850]\n",
      "7517 [D loss: 1.003322] [G loss: 1.000803]\n",
      "7518 [D loss: 1.003245] [G loss: 1.000762]\n",
      "7519 [D loss: 1.003156] [G loss: 1.000858]\n",
      "7520 [D loss: 1.003221] [G loss: 1.000652]\n",
      "7521 [D loss: 1.003192] [G loss: 1.000526]\n",
      "7522 [D loss: 1.003149] [G loss: 1.000850]\n",
      "7523 [D loss: 1.003060] [G loss: 1.000859]\n",
      "7524 [D loss: 1.002907] [G loss: 1.000852]\n",
      "7525 [D loss: 1.003173] [G loss: 1.000987]\n",
      "7526 [D loss: 1.003138] [G loss: 1.000838]\n",
      "7527 [D loss: 1.003198] [G loss: 1.000746]\n",
      "7528 [D loss: 1.003137] [G loss: 1.000469]\n",
      "7529 [D loss: 1.003153] [G loss: 1.000687]\n",
      "7530 [D loss: 1.003294] [G loss: 1.000871]\n",
      "7531 [D loss: 1.003163] [G loss: 1.000793]\n",
      "7532 [D loss: 1.003153] [G loss: 1.001019]\n",
      "7533 [D loss: 1.003176] [G loss: 1.000806]\n",
      "7534 [D loss: 1.003336] [G loss: 1.000794]\n",
      "7535 [D loss: 1.003050] [G loss: 1.000662]\n",
      "7536 [D loss: 1.003234] [G loss: 1.000562]\n",
      "7537 [D loss: 1.003007] [G loss: 1.000925]\n",
      "7538 [D loss: 1.003152] [G loss: 1.000963]\n",
      "7539 [D loss: 1.003122] [G loss: 1.000438]\n",
      "7540 [D loss: 1.003307] [G loss: 1.000611]\n",
      "7541 [D loss: 1.002966] [G loss: 1.000984]\n",
      "7542 [D loss: 1.003326] [G loss: 1.000602]\n",
      "7543 [D loss: 1.002993] [G loss: 1.000838]\n",
      "7544 [D loss: 1.003059] [G loss: 1.000698]\n",
      "7545 [D loss: 1.003277] [G loss: 1.001118]\n",
      "7546 [D loss: 1.003085] [G loss: 1.000872]\n",
      "7547 [D loss: 1.003201] [G loss: 1.000827]\n",
      "7548 [D loss: 1.003107] [G loss: 1.000714]\n",
      "7549 [D loss: 1.003100] [G loss: 1.000941]\n",
      "7550 [D loss: 1.003069] [G loss: 1.001061]\n",
      "7551 [D loss: 1.003252] [G loss: 1.001057]\n",
      "7552 [D loss: 1.002933] [G loss: 1.001058]\n",
      "7553 [D loss: 1.003266] [G loss: 1.000669]\n",
      "7554 [D loss: 1.003213] [G loss: 1.000750]\n",
      "7555 [D loss: 1.003090] [G loss: 1.000890]\n",
      "7556 [D loss: 1.003046] [G loss: 1.001036]\n",
      "7557 [D loss: 1.003197] [G loss: 1.000859]\n",
      "7558 [D loss: 1.003094] [G loss: 1.000886]\n",
      "7559 [D loss: 1.003140] [G loss: 1.000773]\n",
      "7560 [D loss: 1.003061] [G loss: 1.000738]\n",
      "7561 [D loss: 1.003006] [G loss: 1.000709]\n",
      "7562 [D loss: 1.003396] [G loss: 1.000512]\n",
      "7563 [D loss: 1.003223] [G loss: 1.000864]\n",
      "7564 [D loss: 1.003093] [G loss: 1.000600]\n",
      "7565 [D loss: 1.003095] [G loss: 1.000709]\n",
      "7566 [D loss: 1.003000] [G loss: 1.000706]\n",
      "7567 [D loss: 1.002984] [G loss: 1.000871]\n",
      "7568 [D loss: 1.002842] [G loss: 1.000691]\n",
      "7569 [D loss: 1.003192] [G loss: 1.000832]\n",
      "7570 [D loss: 1.003246] [G loss: 1.000534]\n",
      "7571 [D loss: 1.003205] [G loss: 1.000439]\n",
      "7572 [D loss: 1.003137] [G loss: 1.000800]\n",
      "7573 [D loss: 1.003448] [G loss: 1.000812]\n",
      "7574 [D loss: 1.003065] [G loss: 1.000814]\n",
      "7575 [D loss: 1.003271] [G loss: 1.000780]\n",
      "7576 [D loss: 1.003333] [G loss: 1.001098]\n",
      "7577 [D loss: 1.002872] [G loss: 1.000865]\n",
      "7578 [D loss: 1.003238] [G loss: 1.001165]\n",
      "7579 [D loss: 1.003258] [G loss: 1.000760]\n",
      "7580 [D loss: 1.002916] [G loss: 1.000836]\n",
      "7581 [D loss: 1.002907] [G loss: 1.000879]\n",
      "7582 [D loss: 1.003089] [G loss: 1.001029]\n",
      "7583 [D loss: 1.003280] [G loss: 1.000707]\n",
      "7584 [D loss: 1.003234] [G loss: 1.000924]\n",
      "7585 [D loss: 1.003211] [G loss: 1.000833]\n",
      "7586 [D loss: 1.003277] [G loss: 1.001004]\n",
      "7587 [D loss: 1.003253] [G loss: 1.000994]\n",
      "7588 [D loss: 1.003431] [G loss: 1.000626]\n",
      "7589 [D loss: 1.003201] [G loss: 1.000893]\n",
      "7590 [D loss: 1.003038] [G loss: 1.000846]\n",
      "7591 [D loss: 1.003345] [G loss: 1.000880]\n",
      "7592 [D loss: 1.003026] [G loss: 1.000893]\n",
      "7593 [D loss: 1.003361] [G loss: 1.000871]\n",
      "7594 [D loss: 1.003156] [G loss: 1.000850]\n",
      "7595 [D loss: 1.003023] [G loss: 1.000797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7596 [D loss: 1.003173] [G loss: 1.000774]\n",
      "7597 [D loss: 1.003097] [G loss: 1.000969]\n",
      "7598 [D loss: 1.003180] [G loss: 1.000760]\n",
      "7599 [D loss: 1.003269] [G loss: 1.000658]\n",
      "7600 [D loss: 1.003146] [G loss: 1.000583]\n",
      "7601 [D loss: 1.003134] [G loss: 1.000956]\n",
      "7602 [D loss: 1.003282] [G loss: 1.000779]\n",
      "7603 [D loss: 1.002993] [G loss: 1.001168]\n",
      "7604 [D loss: 1.003039] [G loss: 1.001016]\n",
      "7605 [D loss: 1.003252] [G loss: 1.000993]\n",
      "7606 [D loss: 1.003266] [G loss: 1.000798]\n",
      "7607 [D loss: 1.003100] [G loss: 1.000909]\n",
      "7608 [D loss: 1.003329] [G loss: 1.000905]\n",
      "7609 [D loss: 1.003128] [G loss: 1.000946]\n",
      "7610 [D loss: 1.003035] [G loss: 1.000895]\n",
      "7611 [D loss: 1.003311] [G loss: 1.001015]\n",
      "7612 [D loss: 1.003281] [G loss: 1.001111]\n",
      "7613 [D loss: 1.003415] [G loss: 1.001056]\n",
      "7614 [D loss: 1.003195] [G loss: 1.000941]\n",
      "7615 [D loss: 1.003446] [G loss: 1.001236]\n",
      "7616 [D loss: 1.003240] [G loss: 1.000863]\n",
      "7617 [D loss: 1.003273] [G loss: 1.000789]\n",
      "7618 [D loss: 1.003261] [G loss: 1.000837]\n",
      "7619 [D loss: 1.002863] [G loss: 1.000746]\n",
      "7620 [D loss: 1.003382] [G loss: 1.000943]\n",
      "7621 [D loss: 1.003031] [G loss: 1.001079]\n",
      "7622 [D loss: 1.003263] [G loss: 1.001079]\n",
      "7623 [D loss: 1.003337] [G loss: 1.000931]\n",
      "7624 [D loss: 1.003070] [G loss: 1.001130]\n",
      "7625 [D loss: 1.003200] [G loss: 1.000827]\n",
      "7626 [D loss: 1.003223] [G loss: 1.000599]\n",
      "7627 [D loss: 1.002959] [G loss: 1.001163]\n",
      "7628 [D loss: 1.003153] [G loss: 1.000978]\n",
      "7629 [D loss: 1.003394] [G loss: 1.000899]\n",
      "7630 [D loss: 1.002962] [G loss: 1.000639]\n",
      "7631 [D loss: 1.003070] [G loss: 1.000821]\n",
      "7632 [D loss: 1.003193] [G loss: 1.000951]\n",
      "7633 [D loss: 1.003102] [G loss: 1.000678]\n",
      "7634 [D loss: 1.003061] [G loss: 1.000854]\n",
      "7635 [D loss: 1.003124] [G loss: 1.000885]\n",
      "7636 [D loss: 1.003030] [G loss: 1.000859]\n",
      "7637 [D loss: 1.003171] [G loss: 1.000841]\n",
      "7638 [D loss: 1.002883] [G loss: 1.000892]\n",
      "7639 [D loss: 1.003282] [G loss: 1.000888]\n",
      "7640 [D loss: 1.003187] [G loss: 1.000878]\n",
      "7641 [D loss: 1.003244] [G loss: 1.000626]\n",
      "7642 [D loss: 1.003449] [G loss: 1.000988]\n",
      "7643 [D loss: 1.003173] [G loss: 1.000781]\n",
      "7644 [D loss: 1.003074] [G loss: 1.000955]\n",
      "7645 [D loss: 1.003240] [G loss: 1.000657]\n",
      "7646 [D loss: 1.003051] [G loss: 1.000977]\n",
      "7647 [D loss: 1.003003] [G loss: 1.001059]\n",
      "7648 [D loss: 1.003106] [G loss: 1.001024]\n",
      "7649 [D loss: 1.003125] [G loss: 1.000772]\n",
      "7650 [D loss: 1.003183] [G loss: 1.001032]\n",
      "7651 [D loss: 1.003284] [G loss: 1.000793]\n",
      "7652 [D loss: 1.003120] [G loss: 1.000755]\n",
      "7653 [D loss: 1.003386] [G loss: 1.000781]\n",
      "7654 [D loss: 1.003289] [G loss: 1.000948]\n",
      "7655 [D loss: 1.002998] [G loss: 1.000876]\n",
      "7656 [D loss: 1.003204] [G loss: 1.000770]\n",
      "7657 [D loss: 1.002917] [G loss: 1.000642]\n",
      "7658 [D loss: 1.003162] [G loss: 1.001198]\n",
      "7659 [D loss: 1.003218] [G loss: 1.000948]\n",
      "7660 [D loss: 1.003183] [G loss: 1.000726]\n",
      "7661 [D loss: 1.002862] [G loss: 1.000635]\n",
      "7662 [D loss: 1.003184] [G loss: 1.001100]\n",
      "7663 [D loss: 1.003173] [G loss: 1.000726]\n",
      "7664 [D loss: 1.003390] [G loss: 1.000723]\n",
      "7665 [D loss: 1.003245] [G loss: 1.000873]\n",
      "7666 [D loss: 1.003206] [G loss: 1.000993]\n",
      "7667 [D loss: 1.003403] [G loss: 1.000902]\n",
      "7668 [D loss: 1.003383] [G loss: 1.000601]\n",
      "7669 [D loss: 1.003040] [G loss: 1.000538]\n",
      "7670 [D loss: 1.003397] [G loss: 1.000714]\n",
      "7671 [D loss: 1.003069] [G loss: 1.001003]\n",
      "7672 [D loss: 1.003110] [G loss: 1.000814]\n",
      "7673 [D loss: 1.003374] [G loss: 1.000732]\n",
      "7674 [D loss: 1.003373] [G loss: 1.000958]\n",
      "7675 [D loss: 1.002972] [G loss: 1.000666]\n",
      "7676 [D loss: 1.003210] [G loss: 1.000829]\n",
      "7677 [D loss: 1.003303] [G loss: 1.000863]\n",
      "7678 [D loss: 1.003230] [G loss: 1.000921]\n",
      "7679 [D loss: 1.003079] [G loss: 1.001271]\n",
      "7680 [D loss: 1.003039] [G loss: 1.000975]\n",
      "7681 [D loss: 1.003251] [G loss: 1.000845]\n",
      "7682 [D loss: 1.003029] [G loss: 1.000789]\n",
      "7683 [D loss: 1.003094] [G loss: 1.000966]\n",
      "7684 [D loss: 1.003315] [G loss: 1.000886]\n",
      "7685 [D loss: 1.003170] [G loss: 1.000688]\n",
      "7686 [D loss: 1.003225] [G loss: 1.000780]\n",
      "7687 [D loss: 1.003256] [G loss: 1.000897]\n",
      "7688 [D loss: 1.003177] [G loss: 1.000731]\n",
      "7689 [D loss: 1.003252] [G loss: 1.000964]\n",
      "7690 [D loss: 1.003296] [G loss: 1.000781]\n",
      "7691 [D loss: 1.003246] [G loss: 1.000853]\n",
      "7692 [D loss: 1.003144] [G loss: 1.000743]\n",
      "7693 [D loss: 1.003045] [G loss: 1.000845]\n",
      "7694 [D loss: 1.003065] [G loss: 1.000934]\n",
      "7695 [D loss: 1.002935] [G loss: 1.000688]\n",
      "7696 [D loss: 1.002929] [G loss: 1.000804]\n",
      "7697 [D loss: 1.003118] [G loss: 1.000913]\n",
      "7698 [D loss: 1.003379] [G loss: 1.000800]\n",
      "7699 [D loss: 1.003298] [G loss: 1.000938]\n",
      "7700 [D loss: 1.003150] [G loss: 1.000925]\n",
      "7701 [D loss: 1.003098] [G loss: 1.000824]\n",
      "7702 [D loss: 1.003030] [G loss: 1.000999]\n",
      "7703 [D loss: 1.003317] [G loss: 1.001017]\n",
      "7704 [D loss: 1.003232] [G loss: 1.001155]\n",
      "7705 [D loss: 1.003077] [G loss: 1.000904]\n",
      "7706 [D loss: 1.003341] [G loss: 1.001105]\n",
      "7707 [D loss: 1.003010] [G loss: 1.001135]\n",
      "7708 [D loss: 1.003203] [G loss: 1.000748]\n",
      "7709 [D loss: 1.002944] [G loss: 1.000857]\n",
      "7710 [D loss: 1.003264] [G loss: 1.000927]\n",
      "7711 [D loss: 1.003255] [G loss: 1.000864]\n",
      "7712 [D loss: 1.003432] [G loss: 1.001196]\n",
      "7713 [D loss: 1.003112] [G loss: 1.000995]\n",
      "7714 [D loss: 1.003092] [G loss: 1.000864]\n",
      "7715 [D loss: 1.003011] [G loss: 1.000853]\n",
      "7716 [D loss: 1.003135] [G loss: 1.000996]\n",
      "7717 [D loss: 1.002779] [G loss: 1.000805]\n",
      "7718 [D loss: 1.003055] [G loss: 1.000862]\n",
      "7719 [D loss: 1.003006] [G loss: 1.000804]\n",
      "7720 [D loss: 1.002799] [G loss: 1.000999]\n",
      "7721 [D loss: 1.003139] [G loss: 1.000998]\n",
      "7722 [D loss: 1.003264] [G loss: 1.000872]\n",
      "7723 [D loss: 1.003131] [G loss: 1.000887]\n",
      "7724 [D loss: 1.003172] [G loss: 1.001050]\n",
      "7725 [D loss: 1.003263] [G loss: 1.000850]\n",
      "7726 [D loss: 1.003305] [G loss: 1.001041]\n",
      "7727 [D loss: 1.003015] [G loss: 1.001031]\n",
      "7728 [D loss: 1.003247] [G loss: 1.001036]\n",
      "7729 [D loss: 1.003285] [G loss: 1.000809]\n",
      "7730 [D loss: 1.003284] [G loss: 1.001027]\n",
      "7731 [D loss: 1.003056] [G loss: 1.000842]\n",
      "7732 [D loss: 1.003335] [G loss: 1.000930]\n",
      "7733 [D loss: 1.003028] [G loss: 1.000803]\n",
      "7734 [D loss: 1.003123] [G loss: 1.000905]\n",
      "7735 [D loss: 1.003214] [G loss: 1.001064]\n",
      "7736 [D loss: 1.003246] [G loss: 1.001107]\n",
      "7737 [D loss: 1.003416] [G loss: 1.001097]\n",
      "7738 [D loss: 1.003196] [G loss: 1.000785]\n",
      "7739 [D loss: 1.003139] [G loss: 1.001159]\n",
      "7740 [D loss: 1.002966] [G loss: 1.000703]\n",
      "7741 [D loss: 1.002915] [G loss: 1.000870]\n",
      "7742 [D loss: 1.003076] [G loss: 1.000868]\n",
      "7743 [D loss: 1.003158] [G loss: 1.000723]\n",
      "7744 [D loss: 1.003066] [G loss: 1.000627]\n",
      "7745 [D loss: 1.003289] [G loss: 1.000748]\n",
      "7746 [D loss: 1.002801] [G loss: 1.000946]\n",
      "7747 [D loss: 1.003097] [G loss: 1.001170]\n",
      "7748 [D loss: 1.003117] [G loss: 1.001179]\n",
      "7749 [D loss: 1.003288] [G loss: 1.001040]\n",
      "7750 [D loss: 1.003076] [G loss: 1.000891]\n",
      "7751 [D loss: 1.002938] [G loss: 1.001054]\n",
      "7752 [D loss: 1.003232] [G loss: 1.000969]\n",
      "7753 [D loss: 1.003349] [G loss: 1.000807]\n",
      "7754 [D loss: 1.003189] [G loss: 1.001138]\n",
      "7755 [D loss: 1.003217] [G loss: 1.000927]\n",
      "7756 [D loss: 1.003195] [G loss: 1.000780]\n",
      "7757 [D loss: 1.003171] [G loss: 1.001150]\n",
      "7758 [D loss: 1.003070] [G loss: 1.000971]\n",
      "7759 [D loss: 1.003147] [G loss: 1.000665]\n",
      "7760 [D loss: 1.003185] [G loss: 1.000732]\n",
      "7761 [D loss: 1.003028] [G loss: 1.000921]\n",
      "7762 [D loss: 1.003027] [G loss: 1.000975]\n",
      "7763 [D loss: 1.003200] [G loss: 1.000848]\n",
      "7764 [D loss: 1.003043] [G loss: 1.001004]\n",
      "7765 [D loss: 1.003107] [G loss: 1.000795]\n",
      "7766 [D loss: 1.003110] [G loss: 1.000957]\n",
      "7767 [D loss: 1.003089] [G loss: 1.000909]\n",
      "7768 [D loss: 1.002976] [G loss: 1.000691]\n",
      "7769 [D loss: 1.003093] [G loss: 1.000744]\n",
      "7770 [D loss: 1.003170] [G loss: 1.001272]\n",
      "7771 [D loss: 1.003063] [G loss: 1.000739]\n",
      "7772 [D loss: 1.003410] [G loss: 1.000808]\n",
      "7773 [D loss: 1.003387] [G loss: 1.000942]\n",
      "7774 [D loss: 1.003179] [G loss: 1.000727]\n",
      "7775 [D loss: 1.003178] [G loss: 1.000666]\n",
      "7776 [D loss: 1.003005] [G loss: 1.000993]\n",
      "7777 [D loss: 1.003336] [G loss: 1.000684]\n",
      "7778 [D loss: 1.003269] [G loss: 1.001019]\n",
      "7779 [D loss: 1.003169] [G loss: 1.000829]\n",
      "7780 [D loss: 1.003014] [G loss: 1.000690]\n",
      "7781 [D loss: 1.003227] [G loss: 1.000660]\n",
      "7782 [D loss: 1.003122] [G loss: 1.000924]\n",
      "7783 [D loss: 1.003150] [G loss: 1.000616]\n",
      "7784 [D loss: 1.002983] [G loss: 1.000533]\n",
      "7785 [D loss: 1.003064] [G loss: 1.000691]\n",
      "7786 [D loss: 1.003468] [G loss: 1.001021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7787 [D loss: 1.003125] [G loss: 1.000816]\n",
      "7788 [D loss: 1.003149] [G loss: 1.000939]\n",
      "7789 [D loss: 1.003052] [G loss: 1.000675]\n",
      "7790 [D loss: 1.003382] [G loss: 1.001018]\n",
      "7791 [D loss: 1.003068] [G loss: 1.000897]\n",
      "7792 [D loss: 1.003037] [G loss: 1.000891]\n",
      "7793 [D loss: 1.003290] [G loss: 1.001150]\n",
      "7794 [D loss: 1.003089] [G loss: 1.001108]\n",
      "7795 [D loss: 1.003047] [G loss: 1.000745]\n",
      "7796 [D loss: 1.003269] [G loss: 1.000939]\n",
      "7797 [D loss: 1.003278] [G loss: 1.001064]\n",
      "7798 [D loss: 1.003168] [G loss: 1.001341]\n",
      "7799 [D loss: 1.003201] [G loss: 1.000954]\n",
      "7800 [D loss: 1.002995] [G loss: 1.000917]\n",
      "7801 [D loss: 1.003149] [G loss: 1.000811]\n",
      "7802 [D loss: 1.003174] [G loss: 1.000797]\n",
      "7803 [D loss: 1.003308] [G loss: 1.000835]\n",
      "7804 [D loss: 1.003282] [G loss: 1.000732]\n",
      "7805 [D loss: 1.003285] [G loss: 1.000862]\n",
      "7806 [D loss: 1.003182] [G loss: 1.000964]\n",
      "7807 [D loss: 1.003355] [G loss: 1.000992]\n",
      "7808 [D loss: 1.003174] [G loss: 1.000891]\n",
      "7809 [D loss: 1.003112] [G loss: 1.000715]\n",
      "7810 [D loss: 1.003030] [G loss: 1.000834]\n",
      "7811 [D loss: 1.003165] [G loss: 1.000825]\n",
      "7812 [D loss: 1.003283] [G loss: 1.001036]\n",
      "7813 [D loss: 1.003041] [G loss: 1.000669]\n",
      "7814 [D loss: 1.003167] [G loss: 1.000750]\n",
      "7815 [D loss: 1.003123] [G loss: 1.000856]\n",
      "7816 [D loss: 1.003005] [G loss: 1.000919]\n",
      "7817 [D loss: 1.002835] [G loss: 1.000775]\n",
      "7818 [D loss: 1.003119] [G loss: 1.001069]\n",
      "7819 [D loss: 1.002979] [G loss: 1.001013]\n",
      "7820 [D loss: 1.003285] [G loss: 1.000851]\n",
      "7821 [D loss: 1.002971] [G loss: 1.001044]\n",
      "7822 [D loss: 1.003084] [G loss: 1.000734]\n",
      "7823 [D loss: 1.002916] [G loss: 1.000954]\n",
      "7824 [D loss: 1.003219] [G loss: 1.001070]\n",
      "7825 [D loss: 1.003487] [G loss: 1.000771]\n",
      "7826 [D loss: 1.003158] [G loss: 1.000801]\n",
      "7827 [D loss: 1.003040] [G loss: 1.000768]\n",
      "7828 [D loss: 1.003329] [G loss: 1.000716]\n",
      "7829 [D loss: 1.003310] [G loss: 1.000901]\n",
      "7830 [D loss: 1.003236] [G loss: 1.000800]\n",
      "7831 [D loss: 1.002928] [G loss: 1.000838]\n",
      "7832 [D loss: 1.003383] [G loss: 1.000937]\n",
      "7833 [D loss: 1.003342] [G loss: 1.000890]\n",
      "7834 [D loss: 1.003055] [G loss: 1.000979]\n",
      "7835 [D loss: 1.003123] [G loss: 1.001018]\n",
      "7836 [D loss: 1.003106] [G loss: 1.000737]\n",
      "7837 [D loss: 1.003064] [G loss: 1.000847]\n",
      "7838 [D loss: 1.003105] [G loss: 1.001000]\n",
      "7839 [D loss: 1.003184] [G loss: 1.000998]\n",
      "7840 [D loss: 1.003184] [G loss: 1.000889]\n",
      "7841 [D loss: 1.003178] [G loss: 1.000865]\n",
      "7842 [D loss: 1.002965] [G loss: 1.000838]\n",
      "7843 [D loss: 1.002950] [G loss: 1.000941]\n",
      "7844 [D loss: 1.003146] [G loss: 1.001031]\n",
      "7845 [D loss: 1.002982] [G loss: 1.000795]\n",
      "7846 [D loss: 1.003252] [G loss: 1.000946]\n",
      "7847 [D loss: 1.003117] [G loss: 1.000847]\n",
      "7848 [D loss: 1.003014] [G loss: 1.000840]\n",
      "7849 [D loss: 1.003445] [G loss: 1.001178]\n",
      "7850 [D loss: 1.002994] [G loss: 1.000747]\n",
      "7851 [D loss: 1.003118] [G loss: 1.000994]\n",
      "7852 [D loss: 1.003148] [G loss: 1.000690]\n",
      "7853 [D loss: 1.003068] [G loss: 1.001145]\n",
      "7854 [D loss: 1.003324] [G loss: 1.001010]\n",
      "7855 [D loss: 1.003349] [G loss: 1.000883]\n",
      "7856 [D loss: 1.003370] [G loss: 1.000835]\n",
      "7857 [D loss: 1.003137] [G loss: 1.000889]\n",
      "7858 [D loss: 1.003204] [G loss: 1.000498]\n",
      "7859 [D loss: 1.003300] [G loss: 1.001159]\n",
      "7860 [D loss: 1.003096] [G loss: 1.000803]\n",
      "7861 [D loss: 1.003230] [G loss: 1.000788]\n",
      "7862 [D loss: 1.003359] [G loss: 1.001022]\n",
      "7863 [D loss: 1.003144] [G loss: 1.000639]\n",
      "7864 [D loss: 1.003177] [G loss: 1.000982]\n",
      "7865 [D loss: 1.003107] [G loss: 1.000926]\n",
      "7866 [D loss: 1.003115] [G loss: 1.000859]\n",
      "7867 [D loss: 1.003159] [G loss: 1.000911]\n",
      "7868 [D loss: 1.002968] [G loss: 1.000670]\n",
      "7869 [D loss: 1.003219] [G loss: 1.000698]\n",
      "7870 [D loss: 1.003316] [G loss: 1.000726]\n",
      "7871 [D loss: 1.003093] [G loss: 1.000731]\n",
      "7872 [D loss: 1.003213] [G loss: 1.000828]\n",
      "7873 [D loss: 1.003092] [G loss: 1.000827]\n",
      "7874 [D loss: 1.003449] [G loss: 1.000889]\n",
      "7875 [D loss: 1.003282] [G loss: 1.000897]\n",
      "7876 [D loss: 1.003216] [G loss: 1.001124]\n",
      "7877 [D loss: 1.003176] [G loss: 1.001004]\n",
      "7878 [D loss: 1.003076] [G loss: 1.001077]\n",
      "7879 [D loss: 1.003210] [G loss: 1.000754]\n",
      "7880 [D loss: 1.003109] [G loss: 1.000948]\n",
      "7881 [D loss: 1.003191] [G loss: 1.000733]\n",
      "7882 [D loss: 1.003205] [G loss: 1.000755]\n",
      "7883 [D loss: 1.003009] [G loss: 1.001032]\n",
      "7884 [D loss: 1.003046] [G loss: 1.000668]\n",
      "7885 [D loss: 1.003193] [G loss: 1.000752]\n",
      "7886 [D loss: 1.003191] [G loss: 1.000707]\n",
      "7887 [D loss: 1.003189] [G loss: 1.000955]\n",
      "7888 [D loss: 1.003147] [G loss: 1.000895]\n",
      "7889 [D loss: 1.003218] [G loss: 1.000948]\n",
      "7890 [D loss: 1.003036] [G loss: 1.000480]\n",
      "7891 [D loss: 1.003131] [G loss: 1.000877]\n",
      "7892 [D loss: 1.003091] [G loss: 1.000707]\n",
      "7893 [D loss: 1.003113] [G loss: 1.001081]\n",
      "7894 [D loss: 1.002847] [G loss: 1.000721]\n",
      "7895 [D loss: 1.003089] [G loss: 1.000810]\n",
      "7896 [D loss: 1.003248] [G loss: 1.000699]\n",
      "7897 [D loss: 1.003160] [G loss: 1.000851]\n",
      "7898 [D loss: 1.003249] [G loss: 1.000860]\n",
      "7899 [D loss: 1.003179] [G loss: 1.000842]\n",
      "7900 [D loss: 1.003320] [G loss: 1.000534]\n",
      "7901 [D loss: 1.003285] [G loss: 1.001147]\n",
      "7902 [D loss: 1.003245] [G loss: 1.000968]\n",
      "7903 [D loss: 1.003234] [G loss: 1.001073]\n",
      "7904 [D loss: 1.003140] [G loss: 1.000931]\n",
      "7905 [D loss: 1.003140] [G loss: 1.001004]\n",
      "7906 [D loss: 1.003023] [G loss: 1.000864]\n",
      "7907 [D loss: 1.003271] [G loss: 1.000605]\n",
      "7908 [D loss: 1.003050] [G loss: 1.000670]\n",
      "7909 [D loss: 1.002985] [G loss: 1.001006]\n",
      "7910 [D loss: 1.002848] [G loss: 1.001133]\n",
      "7911 [D loss: 1.003244] [G loss: 1.000804]\n",
      "7912 [D loss: 1.003343] [G loss: 1.000547]\n",
      "7913 [D loss: 1.003503] [G loss: 1.000756]\n",
      "7914 [D loss: 1.003074] [G loss: 1.000632]\n",
      "7915 [D loss: 1.003324] [G loss: 1.001041]\n",
      "7916 [D loss: 1.003330] [G loss: 1.000614]\n",
      "7917 [D loss: 1.003046] [G loss: 1.001096]\n",
      "7918 [D loss: 1.003113] [G loss: 1.000616]\n",
      "7919 [D loss: 1.003294] [G loss: 1.000803]\n",
      "7920 [D loss: 1.003475] [G loss: 1.000945]\n",
      "7921 [D loss: 1.003321] [G loss: 1.000799]\n",
      "7922 [D loss: 1.002964] [G loss: 1.000780]\n",
      "7923 [D loss: 1.003226] [G loss: 1.000850]\n",
      "7924 [D loss: 1.003148] [G loss: 1.000927]\n",
      "7925 [D loss: 1.003276] [G loss: 1.000836]\n",
      "7926 [D loss: 1.003270] [G loss: 1.000734]\n",
      "7927 [D loss: 1.003324] [G loss: 1.000771]\n",
      "7928 [D loss: 1.002940] [G loss: 1.000542]\n",
      "7929 [D loss: 1.003355] [G loss: 1.000875]\n",
      "7930 [D loss: 1.003236] [G loss: 1.000615]\n",
      "7931 [D loss: 1.003291] [G loss: 1.000880]\n",
      "7932 [D loss: 1.003392] [G loss: 1.000580]\n",
      "7933 [D loss: 1.003234] [G loss: 1.000896]\n",
      "7934 [D loss: 1.003279] [G loss: 1.000886]\n",
      "7935 [D loss: 1.003136] [G loss: 1.000862]\n",
      "7936 [D loss: 1.003339] [G loss: 1.000849]\n",
      "7937 [D loss: 1.003242] [G loss: 1.000840]\n",
      "7938 [D loss: 1.003138] [G loss: 1.000954]\n",
      "7939 [D loss: 1.003179] [G loss: 1.000876]\n",
      "7940 [D loss: 1.003092] [G loss: 1.000763]\n",
      "7941 [D loss: 1.003331] [G loss: 1.000886]\n",
      "7942 [D loss: 1.003277] [G loss: 1.000551]\n",
      "7943 [D loss: 1.003318] [G loss: 1.000808]\n",
      "7944 [D loss: 1.003192] [G loss: 1.000948]\n",
      "7945 [D loss: 1.003132] [G loss: 1.000747]\n",
      "7946 [D loss: 1.003419] [G loss: 1.000909]\n",
      "7947 [D loss: 1.003252] [G loss: 1.000658]\n",
      "7948 [D loss: 1.003228] [G loss: 1.000959]\n",
      "7949 [D loss: 1.003321] [G loss: 1.000937]\n",
      "7950 [D loss: 1.002623] [G loss: 1.001054]\n",
      "7951 [D loss: 1.003026] [G loss: 1.000792]\n",
      "7952 [D loss: 1.003335] [G loss: 1.000700]\n",
      "7953 [D loss: 1.003176] [G loss: 1.001073]\n",
      "7954 [D loss: 1.002875] [G loss: 1.000627]\n",
      "7955 [D loss: 1.003119] [G loss: 1.000877]\n",
      "7956 [D loss: 1.003239] [G loss: 1.000886]\n",
      "7957 [D loss: 1.002985] [G loss: 1.000860]\n",
      "7958 [D loss: 1.003190] [G loss: 1.000801]\n",
      "7959 [D loss: 1.003323] [G loss: 1.000885]\n",
      "7960 [D loss: 1.003132] [G loss: 1.000961]\n",
      "7961 [D loss: 1.002971] [G loss: 1.000728]\n",
      "7962 [D loss: 1.003133] [G loss: 1.001096]\n",
      "7963 [D loss: 1.003093] [G loss: 1.000505]\n",
      "7964 [D loss: 1.002926] [G loss: 1.000576]\n",
      "7965 [D loss: 1.003428] [G loss: 1.000780]\n",
      "7966 [D loss: 1.003095] [G loss: 1.000581]\n",
      "7967 [D loss: 1.003216] [G loss: 1.001079]\n",
      "7968 [D loss: 1.003144] [G loss: 1.001177]\n",
      "7969 [D loss: 1.002798] [G loss: 1.000972]\n",
      "7970 [D loss: 1.003158] [G loss: 1.000691]\n",
      "7971 [D loss: 1.003141] [G loss: 1.001165]\n",
      "7972 [D loss: 1.003240] [G loss: 1.000891]\n",
      "7973 [D loss: 1.003417] [G loss: 1.000678]\n",
      "7974 [D loss: 1.002856] [G loss: 1.000947]\n",
      "7975 [D loss: 1.003124] [G loss: 1.000842]\n",
      "7976 [D loss: 1.003147] [G loss: 1.000660]\n",
      "7977 [D loss: 1.003250] [G loss: 1.001229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7978 [D loss: 1.003233] [G loss: 1.000779]\n",
      "7979 [D loss: 1.002929] [G loss: 1.000748]\n",
      "7980 [D loss: 1.003073] [G loss: 1.001030]\n",
      "7981 [D loss: 1.003282] [G loss: 1.001108]\n",
      "7982 [D loss: 1.003351] [G loss: 1.000832]\n",
      "7983 [D loss: 1.003173] [G loss: 1.000849]\n",
      "7984 [D loss: 1.003227] [G loss: 1.000986]\n",
      "7985 [D loss: 1.002953] [G loss: 1.000454]\n",
      "7986 [D loss: 1.003208] [G loss: 1.000889]\n",
      "7987 [D loss: 1.003124] [G loss: 1.000682]\n",
      "7988 [D loss: 1.003155] [G loss: 1.000925]\n",
      "7989 [D loss: 1.003080] [G loss: 1.000926]\n",
      "7990 [D loss: 1.003174] [G loss: 1.001252]\n",
      "7991 [D loss: 1.003130] [G loss: 1.001029]\n",
      "7992 [D loss: 1.003131] [G loss: 1.000693]\n",
      "7993 [D loss: 1.003032] [G loss: 1.001133]\n",
      "7994 [D loss: 1.003123] [G loss: 1.000770]\n",
      "7995 [D loss: 1.003477] [G loss: 1.000894]\n",
      "7996 [D loss: 1.003022] [G loss: 1.001103]\n",
      "7997 [D loss: 1.003303] [G loss: 1.001149]\n",
      "7998 [D loss: 1.002955] [G loss: 1.001069]\n",
      "7999 [D loss: 1.003057] [G loss: 1.000879]\n",
      "8000 [D loss: 1.003057] [G loss: 1.001299]\n",
      "(14461, 768)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D,concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cnt=0\n",
    "T = pd.read_csv(r\"C:\\e\\\\02-03\\data\\data_process\\3\\split3new_s3b.csv\", header=None)\n",
    "C = pd.read_csv(r\"C:\\e\\\\02-03\\data\\data_process\\3\\split3news3_benign.csv\",header=None)\n",
    "f3 = pd.read_csv(r\"C:\\e\\\\02-03\\data\\data_process\\benign\\benign3_b.csv\", header=None)\n",
    "\n",
    "\n",
    "T1 = T.append (f3)\n",
    "T1.fillna(0,inplace=True)\n",
    "T1.replace([np.inf,-np.inf],1,inplace=True)\n",
    "\n",
    "testT = np.array(T1)\n",
    "y_test = np.array(C)\n",
    "X_test = np.reshape(testT, (testT.shape[0],32,24))\n",
    "\n",
    "sample1 = pd.read_csv(r\"C:\\e\\\\02-03\\data\\data_process\\3\\sample2.csv\", header=None)\n",
    "\n",
    "sampley = np.array(sample1)\n",
    "\n",
    "class generatorModel (Model):\n",
    "    def __init__(self):\n",
    "        #\n",
    "        super(generatorModel, self).__init__()\n",
    "       \n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 24\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        \n",
    "        self.la1 = Dense(128 * 8 * 6, activation=\"relu\", input_dim=self.latent_dim)\n",
    "        self.la2 = Reshape((8, 6, 128))\n",
    "        self.la3 = UpSampling2D()\n",
    "        self.la4 = Conv2D(104, kernel_size=4, padding=\"same\")\n",
    "        self.la5 = BatchNormalization(momentum=0.8)\n",
    "        self.la6 = Activation(\"relu\")\n",
    "        self.la7 = UpSampling2D()\n",
    "        self.la9 = Conv2D(64, kernel_size=4, padding=\"same\")\n",
    "        self.la10 = BatchNormalization(momentum=0.8)\n",
    "        self.la11 = Activation(\"relu\")\n",
    "        self.la12 = Conv2D(self.channels, kernel_size=4, padding=\"same\")\n",
    "        self.la13 = Activation(\"tanh\")\n",
    "        \n",
    "        self.cla1 = Dense(128 * 8 * 6, activation=\"relu\", input_dim=10)\n",
    "        self.cla2 = Reshape((8, 6, 128))\n",
    "        self.cla3 = UpSampling2D()\n",
    "        self.cla4 = Conv2D(24, kernel_size=4, padding=\"same\")\n",
    "        #self.concat1 = concatenate()\n",
    "\n",
    "    def call(self, x1,x2):\n",
    "        #\n",
    "        #x = self.flatten1(x)\n",
    "        \n",
    "        x1 = self.la1(x1)\n",
    "        x1 = self.la2(x1)\n",
    "        x1 = self.la3(x1)\n",
    "        x1 = self.la4(x1)\n",
    "        \n",
    "        x2 = self.cla1(x2)\n",
    "        x2 = self.cla2(x2)\n",
    "        x2 = self.cla3(x2)\n",
    "        x2 = self.cla4(x2)\n",
    "        \n",
    "        x1 = concatenate([x1,x2])\n",
    "        \n",
    "        \n",
    "        x1 = self.la5(x1)\n",
    "        x1 = self.la6(x1)\n",
    "        x1 = self.la7(x1)\n",
    "       # x1 = self.la8(x1)\n",
    "        x1 = self.la9(x1)\n",
    "        x1 = self.la10(x1)\n",
    "        x1 = self.la11(x1)\n",
    "        x1 = self.la12(x1)\n",
    "        x1 = self.la13(x1)\n",
    "       \n",
    "        return x1\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class WGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 24\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        self.clip_value = 0.01\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build and compile the critic\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z1 =  Input(shape=(self.latent_dim,))\n",
    "        z2 =  Input(shape=(10,))\n",
    "        img = self.generator([z1,z2]) \n",
    "        \n",
    "        z2ex = tf.expand_dims(z2,axis=1)\n",
    "        z2ex = tf.expand_dims(z2ex,axis=-1)\n",
    "        z2ex = tf.tile(z2ex,[1,32,1,1])\n",
    "        imgex = tf.concat ([img,z2ex],axis = 2)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.critic.trainable = False\n",
    "\n",
    "        # The critic takes generated images as input and determines validity\n",
    "        valid = self.critic(imgex)\n",
    "\n",
    "        # The combined model  (stacked generator and critic)\n",
    "        self.combined = Model([z1,z2], valid)\n",
    "        self.combined.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = generatorModel()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        z2 =  Input(shape=(10,))\n",
    "        \n",
    "        #model.build(((100,),(10,)))\n",
    "       # model.summary()\n",
    "        \n",
    "        img = model(noise,z2)\n",
    "        \n",
    "        return Model([noise,z2], img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=(32, 34, self.channels), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=(32, 34, self.channels))\n",
    "        \n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        #(X_train, _), (_, _) = mnist.load_data()\n",
    "        X_train = X_test\n",
    "        global cnt\n",
    "        # Rescale -1 to 1\n",
    "       # X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake = np.ones((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                \n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                input1 = y_test[idx]\n",
    "                \n",
    "                z2ex = tf.expand_dims(input1,axis=1)\n",
    "                z2ex = tf.expand_dims(z2ex,axis=-1)\n",
    "                z2ex = tf.tile(z2ex,[1,32,1,1])\n",
    "                imgsex = tf.concat ([imgs,z2ex],axis = 2)\n",
    "                \n",
    "                \n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict([noise,input1])\n",
    "                \n",
    "                gen_imgsex = tf.concat ([gen_imgs,z2ex],axis = 2)\n",
    "\n",
    "                # Train the critic\n",
    "                d_loss_real = self.critic.train_on_batch(imgsex, valid)\n",
    "                d_loss_fake = self.critic.train_on_batch(gen_imgsex, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "                # Clip critic weights\n",
    "                for l in self.critic.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch([noise, input1], valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                cnt=cnt+1\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 50, 50\n",
    "        idx = np.random.randint(0, sampley.shape[0], 2500)\n",
    "        noise = np.random.normal(0, 1, (14461, self.latent_dim))\n",
    "        input2 = sampley\n",
    "        gen_imgs = self.generator.predict([noise,input2])\n",
    "        \n",
    "        gen_imgs = np.reshape(gen_imgs, (gen_imgs.shape[0],-1))\n",
    "        print ( gen_imgs.shape )\n",
    "        df1 = pd.DataFrame(gen_imgs)\n",
    "        df1.to_csv (r'C:\\Users\\maijieai\\pratice\\p1\\tensorf\\gan_try\\samplest'+'\\\\'+str(cnt)+'.csv')\n",
    "        # Rescale images 0 - 1\n",
    "        #gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        '''\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "        '''\n",
    "       \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wgan = WGAN()\n",
    "    wgan.train(epochs=8001, batch_size=50, sample_interval=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fcca6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
